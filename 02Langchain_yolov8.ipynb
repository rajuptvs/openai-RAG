{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "68819bb2cf3b4fd39fe09940fe75049c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5321d9008d0643f191264361fc221732",
              "IPY_MODEL_417525a8d49d478098292890b572bb42",
              "IPY_MODEL_3b31c24f3a2c44638037d3335c52c55d"
            ],
            "layout": "IPY_MODEL_931b5a6d6f864ac5a9390ff9984f5f6b"
          }
        },
        "5321d9008d0643f191264361fc221732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8920436632045979d5533d80dafc009",
            "placeholder": "​",
            "style": "IPY_MODEL_e14f2615a73947439527eaf1c4bf3eb2",
            "value": "Downloading (…)a8e1d/.gitattributes: 100%"
          }
        },
        "417525a8d49d478098292890b572bb42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e4756e0938a4c5690df9b041b00f972",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_088014ae5f204b57be402f04410b3f54",
            "value": 1175
          }
        },
        "3b31c24f3a2c44638037d3335c52c55d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c2cf8dcae5e42dd9293e2a4a23c542e",
            "placeholder": "​",
            "style": "IPY_MODEL_a3ba6c3d04b6427da8e1b1e8440d573a",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 62.1kB/s]"
          }
        },
        "931b5a6d6f864ac5a9390ff9984f5f6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8920436632045979d5533d80dafc009": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e14f2615a73947439527eaf1c4bf3eb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e4756e0938a4c5690df9b041b00f972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "088014ae5f204b57be402f04410b3f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1c2cf8dcae5e42dd9293e2a4a23c542e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ba6c3d04b6427da8e1b1e8440d573a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20fbcc4f779449cabf711858f49055e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9b04025bfb64bf0b898fda83ef72db9",
              "IPY_MODEL_ed3a34c862244d07a51ae6fb6e381035",
              "IPY_MODEL_b80e3a82d6064b99811f68c68c979ca4"
            ],
            "layout": "IPY_MODEL_e694d8db36fa4f3a882f19c5381f35ab"
          }
        },
        "d9b04025bfb64bf0b898fda83ef72db9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a6614264b0945f8973e028a5c15522a",
            "placeholder": "​",
            "style": "IPY_MODEL_60a804f034d24fd0898d6b7eaa7154f0",
            "value": "Downloading (…)_Pooling/config.json: 100%"
          }
        },
        "ed3a34c862244d07a51ae6fb6e381035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c36c786174fd4d4482abbbc1f5f6aef3",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2638a8754e9468d898af955eda61e7e",
            "value": 190
          }
        },
        "b80e3a82d6064b99811f68c68c979ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36cb7897e78449808203a7c064d031ec",
            "placeholder": "​",
            "style": "IPY_MODEL_d131ff7eb321446c9071d0e9743cda80",
            "value": " 190/190 [00:00&lt;00:00, 8.43kB/s]"
          }
        },
        "e694d8db36fa4f3a882f19c5381f35ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a6614264b0945f8973e028a5c15522a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a804f034d24fd0898d6b7eaa7154f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c36c786174fd4d4482abbbc1f5f6aef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2638a8754e9468d898af955eda61e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36cb7897e78449808203a7c064d031ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d131ff7eb321446c9071d0e9743cda80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9af6b649646a4e1e87458f411b0d48cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94123671246a4579bceaee1e8656bc64",
              "IPY_MODEL_849cdc09b236402bb37ddac54b713d5f",
              "IPY_MODEL_7c2ba4e3e9b4453b9adeaade146f2fbf"
            ],
            "layout": "IPY_MODEL_066b4ac6681e4b318e6004b81f3fe883"
          }
        },
        "94123671246a4579bceaee1e8656bc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee4a6f90bf8e4f2fa7682b3083601429",
            "placeholder": "​",
            "style": "IPY_MODEL_2b0e04ccc68a422e9e0c2b87cd05ed2d",
            "value": "Downloading (…)b20bca8e1d/README.md: 100%"
          }
        },
        "849cdc09b236402bb37ddac54b713d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55b3365ed5fd40ed973e84ead7f54b75",
            "max": 10571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e902b54cc49f462db88d4a1327123014",
            "value": 10571
          }
        },
        "7c2ba4e3e9b4453b9adeaade146f2fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a9b6cfec69d4de58e9290f6543448ce",
            "placeholder": "​",
            "style": "IPY_MODEL_2ee55ced628a46619342a66d3df00d40",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 489kB/s]"
          }
        },
        "066b4ac6681e4b318e6004b81f3fe883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee4a6f90bf8e4f2fa7682b3083601429": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b0e04ccc68a422e9e0c2b87cd05ed2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55b3365ed5fd40ed973e84ead7f54b75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e902b54cc49f462db88d4a1327123014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a9b6cfec69d4de58e9290f6543448ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ee55ced628a46619342a66d3df00d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "008a533621ff4a0ba50e7c14a6231ee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0cf63a0ea8aa4b59815a56a1e98cf13f",
              "IPY_MODEL_6b2b0c55f02e44bbbe5a100752d8767f",
              "IPY_MODEL_cb2ed650c4bf4da5aa83bc9c1d98eaa4"
            ],
            "layout": "IPY_MODEL_d28d6aa863e04dcdb7e6a17413e895a2"
          }
        },
        "0cf63a0ea8aa4b59815a56a1e98cf13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55a2d15518c840f9bf64b66a0b2821a3",
            "placeholder": "​",
            "style": "IPY_MODEL_8c37d831e92f4a3dba2c9ee479160c78",
            "value": "Downloading (…)0bca8e1d/config.json: 100%"
          }
        },
        "6b2b0c55f02e44bbbe5a100752d8767f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_faf504e3e75546a7a10bf2f4cb20eb82",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f05060afab83439b859ef54e53f4b541",
            "value": 571
          }
        },
        "cb2ed650c4bf4da5aa83bc9c1d98eaa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38997c1f17d54dc59c7b4439e2bb52c7",
            "placeholder": "​",
            "style": "IPY_MODEL_2ab725bb471d452d9d0df49174b45090",
            "value": " 571/571 [00:00&lt;00:00, 16.0kB/s]"
          }
        },
        "d28d6aa863e04dcdb7e6a17413e895a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55a2d15518c840f9bf64b66a0b2821a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c37d831e92f4a3dba2c9ee479160c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "faf504e3e75546a7a10bf2f4cb20eb82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05060afab83439b859ef54e53f4b541": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38997c1f17d54dc59c7b4439e2bb52c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab725bb471d452d9d0df49174b45090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3152049236b14468a11bb7379b68f944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a0d38b387804d8e8727e7e72ed77717",
              "IPY_MODEL_580d30538e724e06826d3532c8534855",
              "IPY_MODEL_9bc79ca0a92c47298616876a610f2e1c"
            ],
            "layout": "IPY_MODEL_e6247340bb8648c9a5205bf7da6e3a95"
          }
        },
        "7a0d38b387804d8e8727e7e72ed77717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89cdb56084984bb3a673d949f6d1e178",
            "placeholder": "​",
            "style": "IPY_MODEL_16e0e1b261d5427492f8028203bea6a3",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "580d30538e724e06826d3532c8534855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b676a9e6ca3f41709a01f5643b6ac545",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b9b5e51c73947d0ad03dadb37a48819",
            "value": 116
          }
        },
        "9bc79ca0a92c47298616876a610f2e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9cbd5ad1dff4d8e8cfc7e299c1b56cc",
            "placeholder": "​",
            "style": "IPY_MODEL_7b02ec15c3d4438d8ec768a90489ff0d",
            "value": " 116/116 [00:00&lt;00:00, 2.71kB/s]"
          }
        },
        "e6247340bb8648c9a5205bf7da6e3a95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89cdb56084984bb3a673d949f6d1e178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e0e1b261d5427492f8028203bea6a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b676a9e6ca3f41709a01f5643b6ac545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b9b5e51c73947d0ad03dadb37a48819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9cbd5ad1dff4d8e8cfc7e299c1b56cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b02ec15c3d4438d8ec768a90489ff0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "487a475f99a64da49cb7179a3dfbc2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a638bcdabcfa4b24ab19c7deca6055ae",
              "IPY_MODEL_bafe8939e7954915b369f2eea6e76d9a",
              "IPY_MODEL_37f57707981e45f3a75009efb5f012c3"
            ],
            "layout": "IPY_MODEL_2488cf63cd7642ff9323a93f0c69d8f0"
          }
        },
        "a638bcdabcfa4b24ab19c7deca6055ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43c6ca333e754244868376777a6154f4",
            "placeholder": "​",
            "style": "IPY_MODEL_42056904575a4482b696840e07c6c435",
            "value": "Downloading (…)e1d/data_config.json: 100%"
          }
        },
        "bafe8939e7954915b369f2eea6e76d9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52dd684ad8064860810021c7bb50b55c",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3b4f2b7fd3314d5fbd3a1744eb2eeb96",
            "value": 39265
          }
        },
        "37f57707981e45f3a75009efb5f012c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb3d758b5e804c8c98715ed65a28c175",
            "placeholder": "​",
            "style": "IPY_MODEL_b7d875ecaeef46aebafea05ea94970db",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 848kB/s]"
          }
        },
        "2488cf63cd7642ff9323a93f0c69d8f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43c6ca333e754244868376777a6154f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42056904575a4482b696840e07c6c435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52dd684ad8064860810021c7bb50b55c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b4f2b7fd3314d5fbd3a1744eb2eeb96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb3d758b5e804c8c98715ed65a28c175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d875ecaeef46aebafea05ea94970db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1080e7e815a24886ac2945eb372449ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f5a48704c7a457387bd5f8a37641b33",
              "IPY_MODEL_3730ac6e81e345f8854f300e7d0c1ea7",
              "IPY_MODEL_eb7cff5271bc492bb896f4779ef7c058"
            ],
            "layout": "IPY_MODEL_dd289b200a7948cba9daef3d266923a4"
          }
        },
        "2f5a48704c7a457387bd5f8a37641b33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f3887922ae46d0a514468ec4060381",
            "placeholder": "​",
            "style": "IPY_MODEL_2fe9ffb0d1da410fb3dfd2bfe8f9e47d",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "3730ac6e81e345f8854f300e7d0c1ea7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f62893df22e4c19beab22a77ae51d14",
            "max": 438011953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a858169961cc40f0bbb1e2c58e199037",
            "value": 438011953
          }
        },
        "eb7cff5271bc492bb896f4779ef7c058": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a12a085d7814b7b90ecdbec2232e18c",
            "placeholder": "​",
            "style": "IPY_MODEL_a562a5537d2248e09a10bcce39930f80",
            "value": " 438M/438M [00:02&lt;00:00, 153MB/s]"
          }
        },
        "dd289b200a7948cba9daef3d266923a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f3887922ae46d0a514468ec4060381": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fe9ffb0d1da410fb3dfd2bfe8f9e47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f62893df22e4c19beab22a77ae51d14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a858169961cc40f0bbb1e2c58e199037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a12a085d7814b7b90ecdbec2232e18c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a562a5537d2248e09a10bcce39930f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "967e017d869645b5affed4cc74460ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_354a64b17c2d4a7cb80cb2980e007479",
              "IPY_MODEL_2ba4d6089ffa44cfb4f0bac3871a3628",
              "IPY_MODEL_dc666829a6b04fc0a504eaa655ecad7c"
            ],
            "layout": "IPY_MODEL_a61fe0a293354855b82e73e3b8bfe952"
          }
        },
        "354a64b17c2d4a7cb80cb2980e007479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a87a7315f649ba8a6094c610bac013",
            "placeholder": "​",
            "style": "IPY_MODEL_27ff1577e2e14c9d9302a4219d545ca4",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "2ba4d6089ffa44cfb4f0bac3871a3628": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab6306f366744a4e857c8cc55e828dd9",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa39b6b2ba454cb4ad08be0b7ceac1b2",
            "value": 53
          }
        },
        "dc666829a6b04fc0a504eaa655ecad7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48c0b544ab3546469d6e6e3f19994168",
            "placeholder": "​",
            "style": "IPY_MODEL_2f7df748843241ce8dbada8a7c6a1570",
            "value": " 53.0/53.0 [00:00&lt;00:00, 957B/s]"
          }
        },
        "a61fe0a293354855b82e73e3b8bfe952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a87a7315f649ba8a6094c610bac013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ff1577e2e14c9d9302a4219d545ca4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab6306f366744a4e857c8cc55e828dd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa39b6b2ba454cb4ad08be0b7ceac1b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48c0b544ab3546469d6e6e3f19994168": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f7df748843241ce8dbada8a7c6a1570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed1e6cbbcd2b4ab598f3a3a1f77a7643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e7d51ec78cb40fbafecb58d3870ec71",
              "IPY_MODEL_689175bc3c024d519a86c96ebc69bb11",
              "IPY_MODEL_cd2fd71b7abd4086bfccbcf55a1076ea"
            ],
            "layout": "IPY_MODEL_23144a86a08e4fe7aeaa91e2ed1bf175"
          }
        },
        "3e7d51ec78cb40fbafecb58d3870ec71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48b5289953c747db97e8ddeda89bd270",
            "placeholder": "​",
            "style": "IPY_MODEL_9e3560df021145e0bd72d5ccbdedc775",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "689175bc3c024d519a86c96ebc69bb11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13cd1fd1e6374fac898a2419442d07ea",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_257d2e5eb35e4afe83295fc5f8e27b5d",
            "value": 239
          }
        },
        "cd2fd71b7abd4086bfccbcf55a1076ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_914c90a8fe5849099219ded0ab7825a8",
            "placeholder": "​",
            "style": "IPY_MODEL_e4e40910ddd14b748a7a976a235cd283",
            "value": " 239/239 [00:00&lt;00:00, 3.47kB/s]"
          }
        },
        "23144a86a08e4fe7aeaa91e2ed1bf175": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b5289953c747db97e8ddeda89bd270": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e3560df021145e0bd72d5ccbdedc775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13cd1fd1e6374fac898a2419442d07ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257d2e5eb35e4afe83295fc5f8e27b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "914c90a8fe5849099219ded0ab7825a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e40910ddd14b748a7a976a235cd283": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e97350e624524cf98d367a4b03f6292f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98974814a6df4b7f90736fcce563c664",
              "IPY_MODEL_9be064946b8f4db1897b3c2f7258c72d",
              "IPY_MODEL_68ce432feec34fc286c0971b817001c7"
            ],
            "layout": "IPY_MODEL_e097aa9ed1794fc7ba87da18bf2f8720"
          }
        },
        "98974814a6df4b7f90736fcce563c664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_896d108f42a54ef49d2f8e78ec1c4976",
            "placeholder": "​",
            "style": "IPY_MODEL_37484a3608dd49a9965db2f70eb0c6f5",
            "value": "Downloading (…)a8e1d/tokenizer.json: 100%"
          }
        },
        "9be064946b8f4db1897b3c2f7258c72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa8c1e75c5904eedb533d83c639dead8",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c34fb7a18274f878f08acbbad67e9b7",
            "value": 466021
          }
        },
        "68ce432feec34fc286c0971b817001c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1537d07935c3413197ca949e73871f14",
            "placeholder": "​",
            "style": "IPY_MODEL_3b64b30838a94051890d1853b8da8ec7",
            "value": " 466k/466k [00:00&lt;00:00, 1.10MB/s]"
          }
        },
        "e097aa9ed1794fc7ba87da18bf2f8720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896d108f42a54ef49d2f8e78ec1c4976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37484a3608dd49a9965db2f70eb0c6f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa8c1e75c5904eedb533d83c639dead8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c34fb7a18274f878f08acbbad67e9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1537d07935c3413197ca949e73871f14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b64b30838a94051890d1853b8da8ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "865b5bc1258343da870770a3e1f9289f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4714a012fa141edba79830bb023cb31",
              "IPY_MODEL_4ac42c43abc5494b86da2c4d21806e37",
              "IPY_MODEL_0d1f767b1eb8494fb30f9488858e1295"
            ],
            "layout": "IPY_MODEL_dbc2cdecaffa4d56aa62024a1b17ef80"
          }
        },
        "d4714a012fa141edba79830bb023cb31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74cf7e5c392d49b58d3b16a00a5bd1ba",
            "placeholder": "​",
            "style": "IPY_MODEL_63de72d757be4631b580ae5a28d4c596",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "4ac42c43abc5494b86da2c4d21806e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8eb176a023047239ae927a2b2027e4c",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3241ddf2f7154aafa42b3aca4503e98b",
            "value": 363
          }
        },
        "0d1f767b1eb8494fb30f9488858e1295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_162f7ccb140b4bccb4713838b04c2bd5",
            "placeholder": "​",
            "style": "IPY_MODEL_1c4eef4b7faa4697838189678f4808f0",
            "value": " 363/363 [00:00&lt;00:00, 28.7kB/s]"
          }
        },
        "dbc2cdecaffa4d56aa62024a1b17ef80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74cf7e5c392d49b58d3b16a00a5bd1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63de72d757be4631b580ae5a28d4c596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8eb176a023047239ae927a2b2027e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3241ddf2f7154aafa42b3aca4503e98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "162f7ccb140b4bccb4713838b04c2bd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c4eef4b7faa4697838189678f4808f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abbe315e3e5c49c6b88abf2f54d8a5d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83ff2924d3c94b268215a7bf6e9f8f75",
              "IPY_MODEL_ddb2010532d348d49c771419e1d7bec9",
              "IPY_MODEL_b00ffe5c6a604e53ac80c71d8e98fe05"
            ],
            "layout": "IPY_MODEL_aa25821fc38347c4959123c60cf64c51"
          }
        },
        "83ff2924d3c94b268215a7bf6e9f8f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd633254e20d4814afb75f49e33d97ad",
            "placeholder": "​",
            "style": "IPY_MODEL_e671b2008db04ad3b4b45d43993d2931",
            "value": "Downloading (…)8e1d/train_script.py: 100%"
          }
        },
        "ddb2010532d348d49c771419e1d7bec9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a532f80b1a4426dba63da8aa80ce382",
            "max": 13123,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3e129f2c45140ae834b849cd00aadab",
            "value": 13123
          }
        },
        "b00ffe5c6a604e53ac80c71d8e98fe05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_173605a30c154e74afc46dfba9a8d2cc",
            "placeholder": "​",
            "style": "IPY_MODEL_93522d7f1b5647b3a2d4d25e23718b4a",
            "value": " 13.1k/13.1k [00:00&lt;00:00, 870kB/s]"
          }
        },
        "aa25821fc38347c4959123c60cf64c51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd633254e20d4814afb75f49e33d97ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e671b2008db04ad3b4b45d43993d2931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a532f80b1a4426dba63da8aa80ce382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e129f2c45140ae834b849cd00aadab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "173605a30c154e74afc46dfba9a8d2cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93522d7f1b5647b3a2d4d25e23718b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff4689a1837e4e179789779d02f98a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eeabe8e3d3f64edf8d6e8ca62419c4a0",
              "IPY_MODEL_5b68a00d2fd74a0b84c6141ba1a459cb",
              "IPY_MODEL_75f99cdf6fb84389bf0f7daed5f4c09a"
            ],
            "layout": "IPY_MODEL_1a3e93300d734111adc10e8dc1220404"
          }
        },
        "eeabe8e3d3f64edf8d6e8ca62419c4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93fe4d78ec38472991713fadfffac79a",
            "placeholder": "​",
            "style": "IPY_MODEL_5acea4729dda4bd1970e15486585277d",
            "value": "Downloading (…)b20bca8e1d/vocab.txt: 100%"
          }
        },
        "5b68a00d2fd74a0b84c6141ba1a459cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_348acd9bff494e46a34451a23c2ec301",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_968cbff9100b4db58eb57f5719776aa4",
            "value": 231536
          }
        },
        "75f99cdf6fb84389bf0f7daed5f4c09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72a6d325967042c7925d858350d07b58",
            "placeholder": "​",
            "style": "IPY_MODEL_4dffc4428f8f47589a9f6300072585d9",
            "value": " 232k/232k [00:00&lt;00:00, 552kB/s]"
          }
        },
        "1a3e93300d734111adc10e8dc1220404": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93fe4d78ec38472991713fadfffac79a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5acea4729dda4bd1970e15486585277d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "348acd9bff494e46a34451a23c2ec301": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "968cbff9100b4db58eb57f5719776aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72a6d325967042c7925d858350d07b58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dffc4428f8f47589a9f6300072585d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6766f692045149a3b4522380339fb2ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa6a680c2566475f92acfefef4021e96",
              "IPY_MODEL_f98050587d7a490ca7d8b25791ba93c2",
              "IPY_MODEL_36a16b719f084547b93a553a61b00b8f"
            ],
            "layout": "IPY_MODEL_1b89d9022abb420996d0d3500b4c2dac"
          }
        },
        "fa6a680c2566475f92acfefef4021e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55d2ea44547c4e8d99bf5ddd442c2fc6",
            "placeholder": "​",
            "style": "IPY_MODEL_030933d49b4942b895ac7098fa00ab92",
            "value": "Downloading (…)bca8e1d/modules.json: 100%"
          }
        },
        "f98050587d7a490ca7d8b25791ba93c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5f0834d95f44e9a9518d420a7ca2e1b",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a89fff1cc33d4483b5dd2a131c4e97d7",
            "value": 349
          }
        },
        "36a16b719f084547b93a553a61b00b8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ed61366d2154340bce904d3a1a0c70d",
            "placeholder": "​",
            "style": "IPY_MODEL_1933752b75474525ab6d8d692468bdd7",
            "value": " 349/349 [00:00&lt;00:00, 30.3kB/s]"
          }
        },
        "1b89d9022abb420996d0d3500b4c2dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55d2ea44547c4e8d99bf5ddd442c2fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "030933d49b4942b895ac7098fa00ab92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5f0834d95f44e9a9518d420a7ca2e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a89fff1cc33d4483b5dd2a131c4e97d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3ed61366d2154340bce904d3a1a0c70d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1933752b75474525ab6d8d692468bdd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mMlSosftZ4a",
        "outputId": "a7327c5a-d67b-40ee-ea85-24b70d4d111a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.155-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.0/727.0 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: tqdm>=4.48.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.65.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.3->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.7 frozenlist-1.3.3 langchain-0.0.155 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.8.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "daTl_ubYsN83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q GitPython"
      ],
      "metadata": {
        "id": "3BeDwUsyzlvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caf66e60-25c6-42a0-b972-478960faf1c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import GitLoader"
      ],
      "metadata": {
        "id": "oBEEbHJytolO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf git\n"
      ],
      "metadata": {
        "id": "H_8z_xnkA2Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = GitLoader(\n",
        "    clone_url=\"https://github.com/ultralytics/ultralytics\",\n",
        "    repo_path=\"/content/git\",\n",
        "    branch=\"main\",\n",
        "    file_filter=lambda file_path: file_path.endswith(\".md\")\n",
        ")"
      ],
      "metadata": {
        "id": "YWMkQvRkzb8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = loader.load()"
      ],
      "metadata": {
        "id": "8qSwH1Ujzf3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFhQZKrizieI",
        "outputId": "b87be899-cd71-415c-cf95-8f70dec7ba33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "130"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking out the data"
      ],
      "metadata": {
        "id": "4KJ6WhEuSLAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0].page_content)"
      ],
      "metadata": {
        "id": "RcM9tnNe0BxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef2580c6-c938-42b9-a5cf-377091deb988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Contributing to YOLOv8 🚀\n",
            "\n",
            "We love your input! We want to make contributing to YOLOv8 as easy and transparent as possible, whether it's:\n",
            "\n",
            "- Reporting a bug\n",
            "- Discussing the current state of the code\n",
            "- Submitting a fix\n",
            "- Proposing a new feature\n",
            "- Becoming a maintainer\n",
            "\n",
            "YOLOv8 works so well due to our combined community effort, and for every small improvement you contribute you will be\n",
            "helping push the frontiers of what's possible in AI 😃!\n",
            "\n",
            "## Submitting a Pull Request (PR) 🛠️\n",
            "\n",
            "Submitting a PR is easy! This example shows how to submit a PR for updating `requirements.txt` in 4 steps:\n",
            "\n",
            "### 1. Select File to Update\n",
            "\n",
            "Select `requirements.txt` to update by clicking on it in GitHub.\n",
            "\n",
            "<p align=\"center\"><img width=\"800\" alt=\"PR_step1\" src=\"https://user-images.githubusercontent.com/26833433/122260847-08be2600-ced4-11eb-828b-8287ace4136c.png\"></p>\n",
            "\n",
            "### 2. Click 'Edit this file'\n",
            "\n",
            "Button is in top-right corner.\n",
            "\n",
            "<p align=\"center\"><img width=\"800\" alt=\"PR_step2\" src=\"https://user-images.githubusercontent.com/26833433/122260844-06f46280-ced4-11eb-9eec-b8a24be519ca.png\"></p>\n",
            "\n",
            "### 3. Make Changes\n",
            "\n",
            "Change `matplotlib` version from `3.2.2` to `3.3`.\n",
            "\n",
            "<p align=\"center\"><img width=\"800\" alt=\"PR_step3\" src=\"https://user-images.githubusercontent.com/26833433/122260853-0a87e980-ced4-11eb-9fd2-3650fb6e0842.png\"></p>\n",
            "\n",
            "### 4. Preview Changes and Submit PR\n",
            "\n",
            "Click on the **Preview changes** tab to verify your updates. At the bottom of the screen select 'Create a **new branch**\n",
            "for this commit', assign your branch a descriptive name such as `fix/matplotlib_version` and click the green **Propose\n",
            "changes** button. All done, your PR is now submitted to YOLOv8 for review and approval 😃!\n",
            "\n",
            "<p align=\"center\"><img width=\"800\" alt=\"PR_step4\" src=\"https://user-images.githubusercontent.com/26833433/122260856-0b208000-ced4-11eb-8e8e-77b6151cbcc3.png\"></p>\n",
            "\n",
            "### PR recommendations\n",
            "\n",
            "To allow your work to be integrated as seamlessly as possible, we advise you to:\n",
            "\n",
            "- ✅ Verify your PR is **up-to-date** with `ultralytics/ultralytics` `main` branch. If your PR is behind you can update\n",
            "  your code by clicking the 'Update branch' button or by running `git pull` and `git merge main` locally.\n",
            "\n",
            "<p align=\"center\"><img width=\"751\" alt=\"Screenshot 2022-08-29 at 22 47 15\" src=\"https://user-images.githubusercontent.com/26833433/187295893-50ed9f44-b2c9-4138-a614-de69bd1753d7.png\"></p>\n",
            "\n",
            "- ✅ Verify all YOLOv8 Continuous Integration (CI) **checks are passing**.\n",
            "\n",
            "<p align=\"center\"><img width=\"751\" alt=\"Screenshot 2022-08-29 at 22 47 03\" src=\"https://user-images.githubusercontent.com/26833433/187296922-545c5498-f64a-4d8c-8300-5fa764360da6.png\"></p>\n",
            "\n",
            "- ✅ Reduce changes to the absolute **minimum** required for your bug fix or feature addition. _\"It is not daily increase\n",
            "  but daily decrease, hack away the unessential. The closer to the source, the less wastage there is.\"_  — Bruce Lee\n",
            "\n",
            "### Docstrings\n",
            "\n",
            "Not all functions or classes require docstrings but when they do, we\n",
            "follow [google-style docstrings format](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings).\n",
            "Here is an example:\n",
            "\n",
            "```python\n",
            "\"\"\"\n",
            "   What the function does. Performs NMS on given detection predictions.\n",
            "\n",
            "    Args:\n",
            "        arg1: The description of the 1st argument\n",
            "        arg2: The description of the 2nd argument\n",
            "\n",
            "    Returns:\n",
            "        What the function returns. Empty if nothing is returned.\n",
            "\n",
            "    Raises:\n",
            "        Exception Class: When and why this exception can be raised by the function.\n",
            "\"\"\"\n",
            "```\n",
            "\n",
            "## Submitting a Bug Report 🐛\n",
            "\n",
            "If you spot a problem with YOLOv8 please submit a Bug Report!\n",
            "\n",
            "For us to start investigating a possible problem we need to be able to reproduce it ourselves first. We've created a few\n",
            "short guidelines below to help users provide what we need in order to get started.\n",
            "\n",
            "When asking a question, people will be better able to provide help if you provide **code** that they can easily\n",
            "understand and use to **reproduce** the problem. This is referred to by community members as creating\n",
            "a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/). Your code that reproduces\n",
            "the problem should be:\n",
            "\n",
            "- ✅ **Minimal** – Use as little code as possible that still produces the same problem\n",
            "- ✅ **Complete** – Provide **all** parts someone else needs to reproduce your problem in the question itself\n",
            "- ✅ **Reproducible** – Test the code you're about to provide to make sure it reproduces the problem\n",
            "\n",
            "In addition to the above requirements, for [Ultralytics](https://ultralytics.com/) to provide assistance your code\n",
            "should be:\n",
            "\n",
            "- ✅ **Current** – Verify that your code is up-to-date with current\n",
            "  GitHub [main](https://github.com/ultralytics/ultralytics/tree/main) branch, and if necessary `git pull` or `git clone`\n",
            "  a new copy to ensure your problem has not already been resolved by previous commits.\n",
            "- ✅ **Unmodified** – Your problem must be reproducible without any modifications to the codebase in this\n",
            "  repository. [Ultralytics](https://ultralytics.com/) does not provide support for custom code ⚠️.\n",
            "\n",
            "If you believe your problem meets all of the above criteria, please close this issue and raise a new one using the 🐛\n",
            "**Bug Report** [template](https://github.com/ultralytics/ultralytics/issues/new/choose) and providing\n",
            "a [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us better\n",
            "understand and diagnose your problem.\n",
            "\n",
            "## License\n",
            "\n",
            "By contributing, you agree that your contributions will be licensed under\n",
            "the [AGPL-3.0 license](https://choosealicense.com/licenses/agpl-3.0/)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-udNR6d0F16",
        "outputId": "008804c9-e3d9-42e7-cfd3-f08652ce23d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='## Contributing to YOLOv8 🚀\\n\\nWe love your input! We want to make contributing to YOLOv8 as easy and transparent as possible, whether it\\'s:\\n\\n- Reporting a bug\\n- Discussing the current state of the code\\n- Submitting a fix\\n- Proposing a new feature\\n- Becoming a maintainer\\n\\nYOLOv8 works so well due to our combined community effort, and for every small improvement you contribute you will be\\nhelping push the frontiers of what\\'s possible in AI 😃!\\n\\n## Submitting a Pull Request (PR) 🛠️\\n\\nSubmitting a PR is easy! This example shows how to submit a PR for updating `requirements.txt` in 4 steps:\\n\\n### 1. Select File to Update\\n\\nSelect `requirements.txt` to update by clicking on it in GitHub.\\n\\n<p align=\"center\"><img width=\"800\" alt=\"PR_step1\" src=\"https://user-images.githubusercontent.com/26833433/122260847-08be2600-ced4-11eb-828b-8287ace4136c.png\"></p>\\n\\n### 2. Click \\'Edit this file\\'\\n\\nButton is in top-right corner.\\n\\n<p align=\"center\"><img width=\"800\" alt=\"PR_step2\" src=\"https://user-images.githubusercontent.com/26833433/122260844-06f46280-ced4-11eb-9eec-b8a24be519ca.png\"></p>\\n\\n### 3. Make Changes\\n\\nChange `matplotlib` version from `3.2.2` to `3.3`.\\n\\n<p align=\"center\"><img width=\"800\" alt=\"PR_step3\" src=\"https://user-images.githubusercontent.com/26833433/122260853-0a87e980-ced4-11eb-9fd2-3650fb6e0842.png\"></p>\\n\\n### 4. Preview Changes and Submit PR\\n\\nClick on the **Preview changes** tab to verify your updates. At the bottom of the screen select \\'Create a **new branch**\\nfor this commit\\', assign your branch a descriptive name such as `fix/matplotlib_version` and click the green **Propose\\nchanges** button. All done, your PR is now submitted to YOLOv8 for review and approval 😃!\\n\\n<p align=\"center\"><img width=\"800\" alt=\"PR_step4\" src=\"https://user-images.githubusercontent.com/26833433/122260856-0b208000-ced4-11eb-8e8e-77b6151cbcc3.png\"></p>\\n\\n### PR recommendations\\n\\nTo allow your work to be integrated as seamlessly as possible, we advise you to:\\n\\n- ✅ Verify your PR is **up-to-date** with `ultralytics/ultralytics` `main` branch. If your PR is behind you can update\\n  your code by clicking the \\'Update branch\\' button or by running `git pull` and `git merge main` locally.\\n\\n<p align=\"center\"><img width=\"751\" alt=\"Screenshot 2022-08-29 at 22 47 15\" src=\"https://user-images.githubusercontent.com/26833433/187295893-50ed9f44-b2c9-4138-a614-de69bd1753d7.png\"></p>\\n\\n- ✅ Verify all YOLOv8 Continuous Integration (CI) **checks are passing**.\\n\\n<p align=\"center\"><img width=\"751\" alt=\"Screenshot 2022-08-29 at 22 47 03\" src=\"https://user-images.githubusercontent.com/26833433/187296922-545c5498-f64a-4d8c-8300-5fa764360da6.png\"></p>\\n\\n- ✅ Reduce changes to the absolute **minimum** required for your bug fix or feature addition. _\"It is not daily increase\\n  but daily decrease, hack away the unessential. The closer to the source, the less wastage there is.\"_  — Bruce Lee\\n\\n### Docstrings\\n\\nNot all functions or classes require docstrings but when they do, we\\nfollow [google-style docstrings format](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings).\\nHere is an example:\\n\\n```python\\n\"\"\"\\n   What the function does. Performs NMS on given detection predictions.\\n\\n    Args:\\n        arg1: The description of the 1st argument\\n        arg2: The description of the 2nd argument\\n\\n    Returns:\\n        What the function returns. Empty if nothing is returned.\\n\\n    Raises:\\n        Exception Class: When and why this exception can be raised by the function.\\n\"\"\"\\n```\\n\\n## Submitting a Bug Report 🐛\\n\\nIf you spot a problem with YOLOv8 please submit a Bug Report!\\n\\nFor us to start investigating a possible problem we need to be able to reproduce it ourselves first. We\\'ve created a few\\nshort guidelines below to help users provide what we need in order to get started.\\n\\nWhen asking a question, people will be better able to provide help if you provide **code** that they can easily\\nunderstand and use to **reproduce** the problem. This is referred to by community members as creating\\na [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/). Your code that reproduces\\nthe problem should be:\\n\\n- ✅ **Minimal** – Use as little code as possible that still produces the same problem\\n- ✅ **Complete** – Provide **all** parts someone else needs to reproduce your problem in the question itself\\n- ✅ **Reproducible** – Test the code you\\'re about to provide to make sure it reproduces the problem\\n\\nIn addition to the above requirements, for [Ultralytics](https://ultralytics.com/) to provide assistance your code\\nshould be:\\n\\n- ✅ **Current** – Verify that your code is up-to-date with current\\n  GitHub [main](https://github.com/ultralytics/ultralytics/tree/main) branch, and if necessary `git pull` or `git clone`\\n  a new copy to ensure your problem has not already been resolved by previous commits.\\n- ✅ **Unmodified** – Your problem must be reproducible without any modifications to the codebase in this\\n  repository. [Ultralytics](https://ultralytics.com/) does not provide support for custom code ⚠️.\\n\\nIf you believe your problem meets all of the above criteria, please close this issue and raise a new one using the 🐛\\n**Bug Report** [template](https://github.com/ultralytics/ultralytics/issues/new/choose) and providing\\na [minimum reproducible example](https://docs.ultralytics.com/help/minimum_reproducible_example/) to help us better\\nunderstand and diagnose your problem.\\n\\n## License\\n\\nBy contributing, you agree that your contributions will be licensed under\\nthe [AGPL-3.0 license](https://choosealicense.com/licenses/agpl-3.0/)\\n', metadata={'file_path': 'CONTRIBUTING.md', 'file_name': 'CONTRIBUTING.md', 'file_type': '.md'}), Document(page_content='<div align=\"center\">\\n  <p>\\n    <a href=\"https://ultralytics.com/yolov8\" target=\"_blank\">\\n      <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\\n  </p>\\n\\n[English](README.md) | [简体中文](README.zh-CN.md)\\n<br>\\n\\n<div>\\n    <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\\n    <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"YOLOv8 Citation\"></a>\\n    <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\\n    <br>\\n    <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\\n    <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\\n    <a href=\"https://www.kaggle.com/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n  </div>\\n  <br>\\n\\n[Ultralytics](https://ultralytics.com) [YOLOv8](https://github.com/ultralytics/ultralytics) is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks.\\n\\nWe hope that the resources here will help you get the most out of YOLOv8. Please browse the YOLOv8 <a href=\"https://docs.ultralytics.com/\">Docs</a> for details, raise an issue on <a href=\"https://github.com/ultralytics/ultralytics/issues/new/choose\">GitHub</a> for support, and join our <a href=\"https://discord.gg/n6cFeSPZdD\">Discord</a> community for questions and discussions!\\n\\nTo request an Enterprise License please complete the form at [Ultralytics Licensing](https://ultralytics.com/license).\\n\\n<img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/yolo-comparison-plots.png\"></a>\\n\\n<div align=\"center\">\\n  <a href=\"https://github.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.linkedin.com/company/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://twitter.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://youtube.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.tiktok.com/@ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.instagram.com/ultralytics/\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://discord.gg/n6cFeSPZdD\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/blob/main/social/logo-social-discord.png\" width=\"2%\" alt=\"\" /></a>\\n</div>\\n</div>\\n\\n## <div align=\"center\">Documentation</div>\\n\\nSee below for a quickstart installation and usage example, and see the [YOLOv8 Docs](https://docs.ultralytics.com) for full documentation on training, validation, prediction and deployment.\\n\\n<details open>\\n<summary>Install</summary>\\n\\nPip install the ultralytics package including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) in a [**Python>=3.7**](https://www.python.org/) environment with [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\\n\\n```bash\\npip install ultralytics\\n```\\n\\n</details>\\n\\n<details open>\\n<summary>Usage</summary>\\n\\n#### CLI\\n\\nYOLOv8 may be used directly in the Command Line Interface (CLI) with a `yolo` command:\\n\\n```bash\\nyolo predict model=yolov8n.pt source=\\'https://ultralytics.com/images/bus.jpg\\'\\n```\\n\\n`yolo` can be used for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See the YOLOv8 [CLI Docs](https://docs.ultralytics.com/usage/cli) for examples.\\n\\n#### Python\\n\\nYOLOv8 may also be used directly in a Python environment, and accepts the same [arguments](https://docs.ultralytics.com/usage/cfg/) as in the CLI example above:\\n\\n```python\\nfrom ultralytics import YOLO\\n\\n# Load a model\\nmodel = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\\nmodel = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\\n\\n# Use the model\\nmodel.train(data=\"coco128.yaml\", epochs=3)  # train the model\\nmetrics = model.val()  # evaluate model performance on the validation set\\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\\nsuccess = model.export(format=\"onnx\")  # export the model to ONNX format\\n```\\n\\n[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) download automatically from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases). See YOLOv8 [Python Docs](https://docs.ultralytics.com/usage/python) for more examples.\\n\\n</details>\\n\\n## <div align=\"center\">Models</div>\\n\\nAll YOLOv8 pretrained models are available here. Detect, Segment and Pose models are pretrained on the [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) dataset, while Classify models are pretrained on the [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) dataset.\\n\\n[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) download automatically from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) on first use.\\n\\n<details open><summary>Detection</summary>\\n\\nSee [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples with these models.\\n\\n| Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n| ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\\n| [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640                   | 37.3                 | 80.4                           | 0.99                                | 3.2                | 8.7               |\\n| [YOLOv8s](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt) | 640                   | 44.9                 | 128.4                          | 1.20                                | 11.2               | 28.6              |\\n| [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640                   | 50.2                 | 234.7                          | 1.83                                | 25.9               | 78.9              |\\n| [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640                   | 52.9                 | 375.2                          | 2.39                                | 43.7               | 165.2             |\\n| [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640                   | 53.9                 | 479.1                          | 3.53                                | 68.2               | 257.8             |\\n\\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO val2017](http://cocodataset.org) dataset.\\n  <br>Reproduce by `yolo val detect data=coco.yaml device=0`\\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance.\\n  <br>Reproduce by `yolo val detect data=coco128.yaml batch=1 device=0|cpu`\\n\\n</details>\\n\\n<details><summary>Segmentation</summary>\\n\\nSee [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples with these models.\\n\\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n| -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\\n| [YOLOv8n-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt) | 640                   | 36.7                 | 30.5                  | 96.1                           | 1.21                                | 3.4                | 12.6              |\\n| [YOLOv8s-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-seg.pt) | 640                   | 44.6                 | 36.8                  | 155.7                          | 1.47                                | 11.8               | 42.6              |\\n| [YOLOv8m-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-seg.pt) | 640                   | 49.9                 | 40.8                  | 317.0                          | 2.18                                | 27.3               | 110.2             |\\n| [YOLOv8l-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-seg.pt) | 640                   | 52.3                 | 42.6                  | 572.4                          | 2.79                                | 46.0               | 220.5             |\\n| [YOLOv8x-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt) | 640                   | 53.4                 | 43.4                  | 712.1                          | 4.02                                | 71.8               | 344.1             |\\n\\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO val2017](http://cocodataset.org) dataset.\\n  <br>Reproduce by `yolo val segment data=coco.yaml device=0`\\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance.\\n  <br>Reproduce by `yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu`\\n\\n</details>\\n\\n<details><summary>Classification</summary>\\n\\nSee [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples with these models.\\n\\n| Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 640 |\\n| -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\\n| [YOLOv8n-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-cls.pt) | 224                   | 66.6             | 87.0             | 12.9                           | 0.31                                | 2.7                | 4.3                      |\\n| [YOLOv8s-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-cls.pt) | 224                   | 72.3             | 91.1             | 23.4                           | 0.35                                | 6.4                | 13.5                     |\\n| [YOLOv8m-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-cls.pt) | 224                   | 76.4             | 93.2             | 85.4                           | 0.62                                | 17.0               | 42.7                     |\\n| [YOLOv8l-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-cls.pt) | 224                   | 78.0             | 94.1             | 163.0                          | 0.87                                | 37.5               | 99.7                     |\\n| [YOLOv8x-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt) | 224                   | 78.4             | 94.3             | 232.0                          | 1.01                                | 57.4               | 154.8                    |\\n\\n- **acc** values are model accuracies on the [ImageNet](https://www.image-net.org/) dataset validation set.\\n  <br>Reproduce by `yolo val classify data=path/to/ImageNet device=0`\\n- **Speed** averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance.\\n  <br>Reproduce by `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`\\n\\n</details>\\n\\n<details><summary>Pose</summary>\\n\\nSee [Pose Docs](https://docs.ultralytics.com/tasks/pose) for usage examples with these models.\\n\\n| Model                                                                                                | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n| ---------------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\\n| [YOLOv8n-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt)       | 640                   | 50.4                  | 80.1               | 131.8                          | 1.18                                | 3.3                | 9.2               |\\n| [YOLOv8s-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt)       | 640                   | 60.0                  | 86.2               | 233.2                          | 1.42                                | 11.6               | 30.2              |\\n| [YOLOv8m-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-pose.pt)       | 640                   | 65.0                  | 88.8               | 456.3                          | 2.00                                | 26.4               | 81.0              |\\n| [YOLOv8l-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-pose.pt)       | 640                   | 67.6                  | 90.0               | 784.5                          | 2.59                                | 44.4               | 168.6             |\\n| [YOLOv8x-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose.pt)       | 640                   | 69.2                  | 90.2               | 1607.1                         | 3.73                                | 69.4               | 263.2             |\\n| [YOLOv8x-pose-p6](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose-p6.pt) | 1280                  | 71.6                  | 91.2               | 4088.7                         | 10.04                               | 99.1               | 1066.4            |\\n\\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO Keypoints val2017](http://cocodataset.org)\\n  dataset.\\n  <br>Reproduce by `yolo val pose data=coco-pose.yaml device=0`\\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance.\\n  <br>Reproduce by `yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu`\\n\\n</details>\\n\\n## <div align=\"center\">Integrations</div>\\n\\n<br>\\n<a href=\"https://bit.ly/ultralytics_hub\" target=\"_blank\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"></a>\\n<br>\\n<br>\\n\\n<div align=\"center\">\\n  <a href=\"https://roboflow.com/?ref=ultralytics\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-roboflow.png\" width=\"10%\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"\" />\\n  <a href=\"https://cutt.ly/yolov5-readme-clearml\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-clearml.png\" width=\"10%\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"\" />\\n  <a href=\"https://bit.ly/yolov8-readme-comet\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png\" width=\"10%\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"\" />\\n  <a href=\"https://bit.ly/yolov5-neuralmagic\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png\" width=\"10%\" /></a>\\n</div>\\n\\n|                                                           Roboflow                                                           |                                                            ClearML ⭐ NEW                                                            |                                                                        Comet ⭐ NEW                                                                        |                                           Neural Magic ⭐ NEW                                           |\\n| :--------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------: |\\n| Label and export your custom datasets directly to YOLOv8 for training with [Roboflow](https://roboflow.com/?ref=ultralytics) | Automatically track, visualize and even remotely train YOLOv8 using [ClearML](https://cutt.ly/yolov5-readme-clearml) (open-source!) | Free forever, [Comet](https://bit.ly/yolov8-readme-comet) lets you save YOLOv8 models, resume training, and interactively visualize and debug predictions | Run YOLOv8 inference up to 6x faster with [Neural Magic DeepSparse](https://bit.ly/yolov5-neuralmagic) |\\n\\n## <div align=\"center\">Ultralytics HUB</div>\\n\\nExperience seamless AI with [Ultralytics HUB](https://bit.ly/ultralytics_hub) ⭐, the all-in-one solution for data visualization, YOLOv5 and YOLOv8 🚀 model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendly [Ultralytics App](https://ultralytics.com/app_install). Start your journey for **Free** now!\\n\\n<a href=\"https://bit.ly/ultralytics_hub\" target=\"_blank\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\"></a>\\n\\n## <div align=\"center\">Contribute</div>\\n\\nWe love your input! YOLOv5 and YOLOv8 would not be possible without help from our community. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing) to get started, and fill out our [Survey](https://ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey) to send us feedback on your experience. Thank you 🙏 to all our contributors!\\n\\n<!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=990 -->\\n\\n<a href=\"https://github.com/ultralytics/yolov5/graphs/contributors\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/image-contributors.png\"></a>\\n\\n## <div align=\"center\">License</div>\\n\\nYOLOv8 is available under two different licenses:\\n\\n- **AGPL-3.0 License**: See [LICENSE](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) file for details.\\n- **Enterprise License**: Provides greater flexibility for commercial product development without the open-source requirements of AGPL-3.0. Typical use cases are embedding Ultralytics software and AI models in commercial products and applications. Request an Enterprise License at [Ultralytics Licensing](https://ultralytics.com/license).\\n\\n## <div align=\"center\">Contact</div>\\n\\nFor YOLOv8 bug reports and feature requests please visit [GitHub Issues](https://github.com/ultralytics/ultralytics/issues), and join our [Discord](https://discord.gg/n6cFeSPZdD) community for questions and discussions!\\n\\n<br>\\n<div align=\"center\">\\n  <a href=\"https://github.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.linkedin.com/company/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://twitter.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://youtube.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.tiktok.com/@ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.instagram.com/ultralytics/\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://discord.gg/n6cFeSPZdD\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/blob/main/social/logo-social-discord.png\" width=\"3%\" alt=\"\" /></a>\\n</div>\\n', metadata={'file_path': 'README.md', 'file_name': 'README.md', 'file_type': '.md'}), Document(page_content='<div align=\"center\">\\n  <p>\\n    <a href=\"https://ultralytics.com/yolov8\" target=\"_blank\">\\n      <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\\n  </p>\\n\\n[English](README.md) | [简体中文](README.zh-CN.md)\\n<br>\\n\\n<div>\\n    <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\\n    <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"YOLOv8 Citation\"></a>\\n    <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\\n    <br>\\n    <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\\n    <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\\n    <a href=\"https://www.kaggle.com/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n  </div>\\n  <br>\\n\\n[Ultralytics](https://ultralytics.com) [YOLOv8](https://github.com/ultralytics/ultralytics) 是一款前沿、最先进（SOTA）的模型，基于先前 YOLO 版本的成功，引入了新功能和改进，进一步提升性能和灵活性。YOLOv8 设计快速、准确且易于使用，使其成为各种物体检测与跟踪、实例分割、图像分类和姿态估计任务的绝佳选择。\\n\\n我们希望这里的资源能帮助您充分利用 YOLOv8。请浏览 YOLOv8 <a href=\"https://docs.ultralytics.com/\">文档</a> 了解详细信息，在 <a href=\"https://github.com/ultralytics/ultralytics/issues/new/choose\">GitHub</a> 上提交问题以获得支持，并加入我们的 <a href=\"https://discord.gg/n6cFeSPZdD\">Discord</a> 社区进行问题和讨论！\\n\\n如需申请企业许可，请在 [Ultralytics Licensing](https://ultralytics.com/license) 处填写表格\\n\\n<img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/yolo-comparison-plots.png\"></a>\\n\\n<div align=\"center\">\\n  <a href=\"https://github.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.linkedin.com/company/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://twitter.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://youtube.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.tiktok.com/@ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.instagram.com/ultralytics/\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://discord.gg/n6cFeSPZdD\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/blob/main/social/logo-social-discord.png\" width=\"2%\" alt=\"\" /></a>\\n</div>\\n</div>\\n\\n## <div align=\"center\">文档</div>\\n\\n请参阅下面的快速安装和使用示例，以及 [YOLOv8 文档](https://docs.ultralytics.com) 上有关培训、验证、预测和部署的完整文档。\\n\\n<details open>\\n<summary>安装</summary>\\n\\n在一个 [**Python>=3.7**](https://www.python.org/) 环境中，使用 [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/)，通过 pip 安装 ultralytics 软件包以及所有[依赖项](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt)。\\n\\n```bash\\npip install ultralytics\\n```\\n\\n</details>\\n\\n<details open>\\n<summary>Usage</summary>\\n\\n#### CLI\\n\\nYOLOv8 可以在命令行界面（CLI）中直接使用，只需输入 `yolo` 命令：\\n\\n```bash\\nyolo predict model=yolov8n.pt source=\\'https://ultralytics.com/images/bus.jpg\\'\\n```\\n\\n`yolo` 可用于各种任务和模式，并接受其他参数，例如 `imgsz=640`。查看 YOLOv8 [CLI 文档](https://docs.ultralytics.com/usage/cli)以获取示例。\\n\\n#### Python\\n\\nYOLOv8 也可以在 Python 环境中直接使用，并接受与上述 CLI 示例中相同的[参数](https://docs.ultralytics.com/usage/cfg/)：\\n\\n```python\\nfrom ultralytics import YOLO\\n\\n# 加载模型\\nmodel = YOLO(\"yolov8n.yaml\")  # 从头开始构建新模型\\nmodel = YOLO(\"yolov8n.pt\")  # 加载预训练模型（建议用于训练）\\n\\n# 使用模型\\nmodel.train(data=\"coco128.yaml\", epochs=3)  # 训练模型\\nmetrics = model.val()  # 在验证集上评估模型性能\\nresults = model(\"https://ultralytics.com/images/bus.jpg\")  # 对图像进行预测\\nsuccess = model.export(format=\"onnx\")  # 将模型导出为 ONNX 格式\\n```\\n\\n[模型](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) 会自动从最新的 Ultralytics [发布版本](https://github.com/ultralytics/assets/releases)中下载。查看 YOLOv8 [Python 文档](https://docs.ultralytics.com/usage/python)以获取更多示例。\\n\\n</details>\\n\\n## <div align=\"center\">模型</div>\\n\\n所有的 YOLOv8 预训练模型都可以在此找到。检测、分割和姿态模型在 [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) 数据集上进行预训练，而分类模型在 [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) 数据集上进行预训练。\\n\\n在首次使用时，[模型](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) 会自动从最新的 Ultralytics [发布版本](https://github.com/ultralytics/assets/releases)中下载。\\n\\n<details open><summary>检测</summary>\\n\\n查看 [检测文档](https://docs.ultralytics.com/tasks/detect/) 以获取使用这些模型的示例。\\n\\n| 模型                                                                                   | 尺寸<br><sup>(像素) | mAP<sup>val<br>50-95 | 速度<br><sup>CPU ONNX<br>(ms) | 速度<br><sup>A100 TensorRT<br>(ms) | 参数<br><sup>(M) | FLOPs<br><sup>(B) |\\n| ------------------------------------------------------------------------------------ | --------------- | -------------------- | --------------------------- | -------------------------------- | -------------- | ----------------- |\\n| [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640             | 37.3                 | 80.4                        | 0.99                             | 3.2            | 8.7               |\\n| [YOLOv8s](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt) | 640             | 44.9                 | 128.4                       | 1.20                             | 11.2           | 28.6              |\\n| [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640             | 50.2                 | 234.7                       | 1.83                             | 25.9           | 78.9              |\\n| [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640             | 52.9                 | 375.2                       | 2.39                             | 43.7           | 165.2             |\\n| [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640             | 53.9                 | 479.1                       | 3.53                             | 68.2           | 257.8             |\\n\\n- **mAP<sup>val</sup>** 值是基于单模型单尺度在 [COCO val2017](http://cocodataset.org) 数据集上的结果。\\n  <br>通过 `yolo val detect data=coco.yaml device=0` 复现\\n- **速度** 是使用 [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) 实例对 COCO val 图像进行平均计算的。\\n  <br>通过 `yolo val detect data=coco128.yaml batch=1 device=0|cpu` 复现\\n\\n</details>\\n\\n<details><summary>分割</summary>\\n\\n查看 [分割文档](https://docs.ultralytics.com/tasks/segment/) 以获取使用这些模型的示例。\\n\\n| 模型                                                                                           | 尺寸<br><sup>(像素) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | 速度<br><sup>CPU ONNX<br>(ms) | 速度<br><sup>A100 TensorRT<br>(ms) | 参数<br><sup>(M) | FLOPs<br><sup>(B) |\\n| -------------------------------------------------------------------------------------------- | --------------- | -------------------- | --------------------- | --------------------------- | -------------------------------- | -------------- | ----------------- |\\n| [YOLOv8n-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt) | 640             | 36.7                 | 30.5                  | 96.1                        | 1.21                             | 3.4            | 12.6              |\\n| [YOLOv8s-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-seg.pt) | 640             | 44.6                 | 36.8                  | 155.7                       | 1.47                             | 11.8           | 42.6              |\\n| [YOLOv8m-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-seg.pt) | 640             | 49.9                 | 40.8                  | 317.0                       | 2.18                             | 27.3           | 110.2             |\\n| [YOLOv8l-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-seg.pt) | 640             | 52.3                 | 42.6                  | 572.4                       | 2.79                             | 46.0           | 220.5             |\\n| [YOLOv8x-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt) | 640             | 53.4                 | 43.4                  | 712.1                       | 4.02                             | 71.8           | 344.1             |\\n\\n- **mAP<sup>val</sup>** 值是基于单模型单尺度在 [COCO val2017](http://cocodataset.org) 数据集上的结果。\\n  <br>通过 `yolo val segment data=coco.yaml device=0` 复现\\n- **速度** 是使用 [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) 实例对 COCO val 图像进行平均计算的。\\n  <br>通过 `yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu` 复现\\n\\n</details>\\n\\n<details><summary>分类</summary>\\n\\n查看 [分类文档](https://docs.ultralytics.com/tasks/classify/) 以获取使用这些模型的示例。\\n\\n| 模型                                                                                           | 尺寸<br><sup>(像素) | acc<br><sup>top1 | acc<br><sup>top5 | 速度<br><sup>CPU ONNX<br>(ms) | 速度<br><sup>A100 TensorRT<br>(ms) | 参数<br><sup>(M) | FLOPs<br><sup>(B) at 640 |\\n| -------------------------------------------------------------------------------------------- | --------------- | ---------------- | ---------------- | --------------------------- | -------------------------------- | -------------- | ------------------------ |\\n| [YOLOv8n-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-cls.pt) | 224             | 66.6             | 87.0             | 12.9                        | 0.31                             | 2.7            | 4.3                      |\\n| [YOLOv8s-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-cls.pt) | 224             | 72.3             | 91.1             | 23.4                        | 0.35                             | 6.4            | 13.5                     |\\n| [YOLOv8m-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-cls.pt) | 224             | 76.4             | 93.2             | 85.4                        | 0.62                             | 17.0           | 42.7                     |\\n| [YOLOv8l-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-cls.pt) | 224             | 78.0             | 94.1             | 163.0                       | 0.87                             | 37.5           | 99.7                     |\\n| [YOLOv8x-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt) | 224             | 78.4             | 94.3             | 232.0                       | 1.01                             | 57.4           | 154.8                    |\\n\\n- **acc** 值是模型在 [ImageNet](https://www.image-net.org/) 数据集验证集上的准确率。\\n  <br>通过 `yolo val classify data=path/to/ImageNet device=0` 复现\\n- **速度** 是使用 [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) 实例对 ImageNet val 图像进行平均计算的。\\n  <br>通过 `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu` 复现\\n\\n</details>\\n\\n<details><summary>姿态</summary>\\n\\n查看 [姿态文档](https://docs.ultralytics.com/tasks/) 以获取使用这些模型的示例。\\n\\n| 模型                                                                                                   | 尺寸<br><sup>(像素) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | 速度<br><sup>CPU ONNX<br>(ms) | 速度<br><sup>A100 TensorRT<br>(ms) | 参数<br><sup>(M) | FLOPs<br><sup>(B) |\\n| ---------------------------------------------------------------------------------------------------- | --------------- | --------------------- | ------------------ | --------------------------- | -------------------------------- | -------------- | ----------------- |\\n| [YOLOv8n-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt)       | 640             | 50.4                  | 80.1               | 131.8                       | 1.18                             | 3.3            | 9.2               |\\n| [YOLOv8s-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt)       | 640             | 60.0                  | 86.2               | 233.2                       | 1.42                             | 11.6           | 30.2              |\\n| [YOLOv8m-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-pose.pt)       | 640             | 65.0                  | 88.8               | 456.3                       | 2.00                             | 26.4           | 81.0              |\\n| [YOLOv8l-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-pose.pt)       | 640             | 67.6                  | 90.0               | 784.5                       | 2.59                             | 44.4           | 168.6             |\\n| [YOLOv8x-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose.pt)       | 640             | 69.2                  | 90.2               | 1607.1                      | 3.73                             | 69.4           | 263.2             |\\n| [YOLOv8x-pose-p6](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose-p6.pt) | 1280            | 71.6                  | 91.2               | 4088.7                      | 10.04                            | 99.1           | 1066.4            |\\n\\n- **mAP<sup>val</sup>** 值是基于单模型单尺度在 [COCO Keypoints val2017](http://cocodataset.org) 数据集上的结果。\\n  <br>通过 `yolo val pose data=coco-pose.yaml device=0` 复现\\n- **速度** 是使用 [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) 实例对 COCO val 图像进行平均计算的。\\n  <br>通过 `yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu` 复现\\n\\n</details>\\n\\n## <div align=\"center\">集成</div>\\n\\n<br>\\n<a href=\"https://bit.ly/ultralytics_hub\" target=\"_blank\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"></a>\\n<br>\\n<br>\\n\\n<div align=\"center\">\\n  <a href=\"https://roboflow.com/?ref=ultralytics\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-roboflow.png\" width=\"10%\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"\" />\\n  <a href=\"https://cutt.ly/yolov5-readme-clearml\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-clearml.png\" width=\"10%\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"\" />\\n  <a href=\"https://bit.ly/yolov8-readme-comet\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-comet.png\" width=\"10%\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"15%\" height=\"0\" alt=\"\" />\\n  <a href=\"https://bit.ly/yolov5-neuralmagic\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/partners/logo-neuralmagic.png\" width=\"10%\" /></a>\\n</div>\\n\\n|                                      Roboflow                                      |                                 ClearML ⭐ NEW                                  |                                     Comet ⭐ NEW                                      |                                  Neural Magic ⭐ NEW                                   |\\n| :--------------------------------------------------------------------------------: | :----------------------------------------------------------------------------: | :----------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------: |\\n| 使用 [Roboflow](https://roboflow.com/?ref=ultralytics) 将您的自定义数据集直接标记并导出至 YOLOv8 进行训练 | 使用 [ClearML](https://cutt.ly/yolov5-readme-clearml)（开源！）自动跟踪、可视化，甚至远程训练 YOLOv8 | 免费且永久，[Comet](https://bit.ly/yolov8-readme-comet) 让您保存 YOLOv8 模型、恢复训练，并以交互式方式查看和调试预测 | 使用 [Neural Magic DeepSparse](https://bit.ly/yolov5-neuralmagic) 使 YOLOv8 推理速度提高多达 6 倍 |\\n\\n## <div align=\"center\">Ultralytics HUB</div>\\n\\n体验 [Ultralytics HUB](https://bit.ly/ultralytics_hub) ⭐ 带来的无缝 AI，这是一个一体化解决方案，用于数据可视化、YOLOv5 和即将推出的 YOLOv8 🚀 模型训练和部署，无需任何编码。通过我们先进的平台和用户友好的 [Ultralytics 应用程序](https://ultralytics.com/app_install)，轻松将图像转化为可操作的见解，并实现您的 AI 愿景。现在就开始您的**免费**之旅！\\n\\n<a href=\"https://bit.ly/ultralytics_hub\" target=\"_blank\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\"></a>\\n\\n## <div align=\"center\">贡献</div>\\n\\n我们喜欢您的参与！没有社区的帮助，YOLOv5 和 YOLOv8 将无法实现。请参阅我们的[贡献指南](https://docs.ultralytics.com/help/contributing)以开始使用，并填写我们的[调查问卷](https://ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey)向我们提供您的使用体验反馈。感谢所有贡献者的支持！🙏\\n\\n<!-- SVG image from https://opencollective.com/ultralytics/contributors.svg?width=990 -->\\n\\n<a href=\"https://github.com/ultralytics/yolov5/graphs/contributors\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/image-contributors.png\"></a>\\n\\n## <div align=\"center\">许可证</div>\\n\\nYOLOv8 提供两种不同的许可证：\\n\\n- **AGPL-3.0 许可证**：详细信息请参阅 [LICENSE](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) 文件。\\n- **企业许可证**：为商业产品开发提供更大的灵活性，无需遵循 AGPL-3.0 的开源要求。典型的用例是将 Ultralytics 软件和 AI 模型嵌入商业产品和应用中。在 [Ultralytics 授权](https://ultralytics.com/license) 处申请企业许可证。\\n\\n## <div align=\"center\">联系方式</div>\\n\\n对于 YOLOv8 的错误报告和功能请求，请访问 [GitHub Issues](https://github.com/ultralytics/ultralytics/issues)，并加入我们的 [Discord](https://discord.gg/n6cFeSPZdD) 社区进行问题和讨论！\\n\\n<br>\\n<div align=\"center\">\\n  <a href=\"https://github.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.linkedin.com/company/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://twitter.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://youtube.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.tiktok.com/@ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.instagram.com/ultralytics/\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png\" width=\"3%\" alt=\"\" /></a>\\n  <a href=\"https://discord.gg/n6cFeSPZdD\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/blob/main/social/logo-social-discord.png\" width=\"3%\" alt=\"\" /></a>\\n</div>\\n', metadata={'file_path': 'README.zh-CN.md', 'file_name': 'README.zh-CN.md', 'file_type': '.md'}), Document(page_content='# Ultralytics Docs\\n\\nUltralytics Docs are deployed to [https://docs.ultralytics.com](https://docs.ultralytics.com).\\n\\n### Install Ultralytics package\\n\\nTo install the ultralytics package in developer mode, you will need to have Git and Python 3 installed on your system.\\nThen, follow these steps:\\n\\n1. Clone the ultralytics repository to your local machine using Git:\\n\\n```bash\\ngit clone https://github.com/ultralytics/ultralytics.git\\n```\\n\\n2. Navigate to the root directory of the repository:\\n\\n```bash\\ncd ultralytics\\n```\\n\\n3. Install the package in developer mode using pip:\\n\\n```bash\\npip install -e \\'.[dev]\\'\\n```\\n\\nThis will install the ultralytics package and its dependencies in developer mode, allowing you to make changes to the\\npackage code and have them reflected immediately in your Python environment.\\n\\nNote that you may need to use the pip3 command instead of pip if you have multiple versions of Python installed on your\\nsystem.\\n\\n### Building and Serving Locally\\n\\nThe `mkdocs serve` command is used to build and serve a local version of the MkDocs documentation site. It is typically\\nused during the development and testing phase of a documentation project.\\n\\n```bash\\nmkdocs serve\\n```\\n\\nHere is a breakdown of what this command does:\\n\\n- `mkdocs`: This is the command-line interface (CLI) for the MkDocs static site generator. It is used to build and serve\\n  MkDocs sites.\\n- `serve`: This is a subcommand of the `mkdocs` CLI that tells it to build and serve the documentation site locally.\\n- `-a`: This flag specifies the hostname and port number to bind the server to. The default value is `localhost:8000`.\\n- `-t`: This flag specifies the theme to use for the documentation site. The default value is `mkdocs`.\\n- `-s`: This flag tells the `serve` command to serve the site in silent mode, which means it will not display any log\\n  messages or progress updates.\\n  When you run the `mkdocs serve` command, it will build the documentation site using the files in the `docs/` directory\\n  and serve it at the specified hostname and port number. You can then view the site by going to the URL in your web\\n  browser.\\n\\nWhile the site is being served, you can make changes to the documentation files and see them reflected in the live site\\nimmediately. This is useful for testing and debugging your documentation before deploying it to a live server.\\n\\nTo stop the serve command and terminate the local server, you can use the `CTRL+C` keyboard shortcut.\\n\\n### Deploying Your Documentation Site\\n\\nTo deploy your MkDocs documentation site, you will need to choose a hosting provider and a deployment method. Some\\npopular options include GitHub Pages, GitLab Pages, and Amazon S3.\\n\\nBefore you can deploy your site, you will need to configure your `mkdocs.yml` file to specify the remote host and any\\nother necessary deployment settings.\\n\\nOnce you have configured your `mkdocs.yml` file, you can use the `mkdocs deploy` command to build and deploy your site.\\nThis command will build the documentation site using the files in the `docs/` directory and the specified configuration\\nfile and theme, and then deploy the site to the specified remote host.\\n\\nFor example, to deploy your site to GitHub Pages using the gh-deploy plugin, you can use the following command:\\n\\n```bash\\nmkdocs gh-deploy\\n```\\n\\nIf you are using GitHub Pages, you can set a custom domain for your documentation site by going to the \"Settings\" page\\nfor your repository and updating the \"Custom domain\" field in the \"GitHub Pages\" section.\\n\\n![196814117-fc16e711-d2be-4722-9536-b7c6d78fd167](https://user-images.githubusercontent.com/26833433/210150206-9e86dcd7-10af-43e4-9eb2-9518b3799eac.png)\\n\\nFor more information on deploying your MkDocs documentation site, see\\nthe [MkDocs documentation](https://www.mkdocs.org/user-guide/deploying-your-docs/).\\n', metadata={'file_path': 'docs/README.md', 'file_name': 'README.md', 'file_type': '.md'}), Document(page_content=\"# Security Policy\\n\\nAt [Ultralytics](https://ultralytics.com), the security of our users' data and systems is of utmost importance. To\\nensure the safety and security of our [open-source projects](https://github.com/ultralytics), we have implemented\\nseveral measures to detect and prevent security vulnerabilities.\\n\\n[![ultralytics](https://snyk.io/advisor/python/ultralytics/badge.svg)](https://snyk.io/advisor/python/ultralytics)\\n\\n## Snyk Scanning\\n\\nWe use [Snyk](https://snyk.io/advisor/python/ultralytics) to regularly scan the YOLOv8 repository for vulnerabilities\\nand security issues. Our goal is to identify and remediate any potential threats as soon as possible, to minimize any\\nrisks to our users.\\n\\n## GitHub CodeQL Scanning\\n\\nIn addition to our Snyk scans, we also use\\nGitHub's [CodeQL](https://docs.github.com/en/code-security/code-scanning/automatically-scanning-your-code-for-vulnerabilities-and-errors/about-code-scanning-with-codeql)\\nscans to proactively identify and address security vulnerabilities.\\n\\n## Reporting Security Issues\\n\\nIf you suspect or discover a security vulnerability in the YOLOv8 repository, please let us know immediately. You can\\nreach out to us directly via our [contact form](https://ultralytics.com/contact) or\\nvia [security@ultralytics.com](mailto:security@ultralytics.com). Our security team will investigate and respond as soon\\nas possible.\\n\\nWe appreciate your help in keeping the YOLOv8 repository secure and safe for everyone.\\n\", metadata={'file_path': 'docs/SECURITY.md', 'file_name': 'SECURITY.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Ultralytics HUB App for YOLOv8\\n\\n<a href=\"https://bit.ly/ultralytics_hub\" target=\"_blank\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\"></a>\\n<br>\\n<div align=\"center\">\\n  <a href=\"https://github.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.linkedin.com/company/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://twitter.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://youtube.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.tiktok.com/@ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.instagram.com/ultralytics/\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png\" width=\"2%\" alt=\"\" /></a>\\n  <br>\\n  <br>\\n  <a href=\"https://play.google.com/store/apps/details?id=com.ultralytics.ultralytics_app\" style=\"text-decoration:none;\">\\n    <img src=\"https://raw.githubusercontent.com/ultralytics/assets/master/app/google-play.svg\" width=\"15%\" alt=\"\" /></a>&nbsp;\\n  <a href=\"https://apps.apple.com/xk/app/ultralytics/id1583935240\" style=\"text-decoration:none;\">\\n    <img src=\"https://raw.githubusercontent.com/ultralytics/assets/master/app/app-store.svg\" width=\"15%\" alt=\"\" /></a>\\n</div>\\n\\nWelcome to the Ultralytics HUB app, which is designed to demonstrate the power and capabilities of the YOLOv5 and YOLOv8\\nmodels. This app is available for download on\\nthe [Apple App Store](https://apps.apple.com/xk/app/ultralytics/id1583935240) and\\nthe [Google Play Store](https://play.google.com/store/apps/details?id=com.ultralytics.ultralytics_app).\\n\\n**To install the app, simply scan the QR code provided above**. At the moment, the app features YOLOv5 models, with\\nYOLOv8 models set to be available soon.\\n\\nWith the YOLOv5 model, you can easily detect and classify objects in images and videos with high accuracy and speed. The\\nmodel has been trained on a vast dataset and can recognize a wide range of objects, including pedestrians, traffic\\nsigns, and cars.\\n\\nUsing this app, you can try out YOLOv5 on your images and videos, and observe how the model works in real-time.\\nAdditionally, you can learn more about YOLOv5\\'s functionality and how it can be integrated into real-world applications.\\n\\nWe are confident that you will enjoy using YOLOv5 and be amazed at its capabilities. Thank you for choosing Ultralytics\\nfor your AI solutions.', metadata={'file_path': 'docs/app.md', 'file_name': 'app.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Ultralytics HUB\\n\\n<a href=\"https://bit.ly/ultralytics_hub\" target=\"_blank\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/ultralytics-hub.png\"></a>\\n<br>\\n<div align=\"center\">\\n  <a href=\"https://github.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.linkedin.com/company/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://twitter.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://youtube.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.tiktok.com/@ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"2%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"2%\" alt=\"\" />\\n  <a href=\"https://www.instagram.com/ultralytics/\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png\" width=\"2%\" alt=\"\" /></a>\\n  <br>\\n  <br>\\n  <a href=\"https://github.com/ultralytics/hub/actions/workflows/ci.yaml\">\\n    <img src=\"https://github.com/ultralytics/hub/actions/workflows/ci.yaml/badge.svg\" alt=\"CI CPU\"></a>\\n  <a href=\"https://colab.research.google.com/github/ultralytics/hub/blob/master/hub.ipynb\">\\n    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> \\n</div>\\n\\n\\n[Ultralytics HUB](https://hub.ultralytics.com) is a new no-code online tool developed\\nby [Ultralytics](https://ultralytics.com), the creators of the popular [YOLOv5](https://github.com/ultralytics/yolov5)\\nobject detection and image segmentation models. With Ultralytics HUB, users can easily train and deploy YOLO models\\nwithout any coding or technical expertise.\\n\\nUltralytics HUB is designed to be user-friendly and intuitive, with a drag-and-drop interface that allows users to\\neasily upload their data and select their model configurations. It also offers a range of pre-trained models and\\ntemplates to choose from, making it easy for users to get started with training their own models. Once a model is\\ntrained, it can be easily deployed and used for real-time object detection and image segmentation tasks. Overall,\\nUltralytics HUB is an essential tool for anyone looking to use YOLO for their object detection and image segmentation\\nprojects.\\n\\n**[Get started now](https://hub.ultralytics.com)** and experience the power and simplicity of Ultralytics HUB for\\nyourself. Sign up for a free account and start building, training, and deploying YOLOv5 and YOLOv8 models today.\\n\\n## 1. Upload a Dataset\\n\\nUltralytics HUB datasets are just like YOLOv5 🚀 datasets, they use the same structure and the same label formats to keep\\neverything simple.\\n\\nWhen you upload a dataset to Ultralytics HUB, make sure to **place your dataset YAML inside the dataset root directory**\\nas in the example shown below, and then zip for upload to https://hub.ultralytics.com/. Your **dataset YAML, directory\\nand zip** should all share the same name. For example, if your dataset is called \\'coco6\\' as in our\\nexample [ultralytics/hub/coco6.zip](https://github.com/ultralytics/hub/blob/master/coco6.zip), then you should have a\\ncoco6.yaml inside your coco6/ directory, which should zip to create coco6.zip for upload:\\n\\n```bash\\nzip -r coco6.zip coco6\\n```\\n\\nThe example [coco6.zip](https://github.com/ultralytics/hub/blob/master/coco6.zip) dataset in this repository can be\\ndownloaded and unzipped to see exactly how to structure your custom dataset.\\n\\n<p align=\"center\">\\n<img width=\"80%\" src=\"https://user-images.githubusercontent.com/26833433/201424843-20fa081b-ad4b-4d6c-a095-e810775908d8.png\" title=\"COCO6\" />\\n</p>\\n\\nThe dataset YAML is the same standard YOLOv5 YAML format. See\\nthe [YOLOv5 Train Custom Data tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) for full details.\\n\\n```yaml\\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\\npath:  # dataset root dir (leave empty for HUB)\\ntrain: images/train  # train images (relative to \\'path\\') 8 images\\nval: images/val  # val images (relative to \\'path\\') 8 images\\ntest:  # test images (optional)\\n\\n# Classes\\nnames:\\n  0: person\\n  1: bicycle\\n  2: car\\n  3: motorcycle\\n  ...\\n```\\n\\nAfter zipping your dataset, sign in to [Ultralytics HUB](https://bit.ly/ultralytics_hub) and click the Datasets tab.\\nClick \\'Upload Dataset\\' to upload, scan and visualize your new dataset before training new YOLOv5 models on it!\\n\\n<img width=\"100%\" alt=\"HUB Dataset Upload\" src=\"https://user-images.githubusercontent.com/26833433/216763338-9a8812c8-a4e5-4362-8102-40dad7818396.png\">\\n\\n## 2. Train a Model\\n\\nConnect to the Ultralytics HUB notebook and use your model API key to begin training!\\n\\n<a href=\"https://colab.research.google.com/github/ultralytics/hub/blob/master/hub.ipynb\" target=\"_blank\">\\n<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\\n\\n## 3. Deploy to Real World\\n\\nExport your model to 13 different formats, including TensorFlow, ONNX, OpenVINO, CoreML, Paddle and many others. Run\\nmodels directly on your [iOS](https://apps.apple.com/xk/app/ultralytics/id1583935240) or\\n[Android](https://play.google.com/store/apps/details?id=com.ultralytics.ultralytics_app) mobile device by downloading\\nthe [Ultralytics App](https://ultralytics.com/app_install)!\\n\\n## ❓ Issues\\n\\nIf you are a new [Ultralytics HUB](https://bit.ly/ultralytics_hub) user and have questions or comments, you are in the\\nright place! Please raise a [New Issue](https://github.com/ultralytics/hub/issues/new/choose) and let us know what we\\ncan do to make your life better 😃!\\n', metadata={'file_path': 'docs/hub.md', 'file_name': 'hub.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n<div align=\"center\">\\n  <p>\\n    <a href=\"https://github.com/ultralytics/ultralytics\" target=\"_blank\">\\n    <img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\"></a>\\n  </p>\\n  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yaml/badge.svg\" alt=\"Ultralytics CI\"></a>\\n  <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"YOLOv8 Citation\"></a>\\n  <a href=\"https://hub.docker.com/r/ultralytics/ultralytics\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/ultralytics?logo=docker\" alt=\"Docker Pulls\"></a>\\n  <br>\\n  <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\\n  <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\\n  <a href=\"https://www.kaggle.com/ultralytics/yolov8\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n</div>\\n\\nIntroducing [Ultralytics](https://ultralytics.com) [YOLOv8](https://github.com/ultralytics/ultralytics), the latest version of the acclaimed real-time object detection and image segmentation model. YOLOv8 is built on cutting-edge advancements in deep learning and computer vision, offering unparalleled performance in terms of speed and accuracy. Its streamlined design makes it suitable for various applications and easily adaptable to different hardware platforms, from edge devices to cloud APIs.\\n\\nExplore the YOLOv8 Docs, a comprehensive resource designed to help you understand and utilize its features and capabilities. Whether you are a seasoned machine learning practitioner or new to the field, this hub aims to maximize YOLOv8\\'s potential in your projects\\n\\n## Where to Start\\n\\n- **Install** `ultralytics` with pip and get up and running in minutes &nbsp; [:material-clock-fast: Get Started](quickstart.md){ .md-button }\\n- **Predict** new images and videos with YOLOv8 &nbsp; [:octicons-image-16: Predict on Images](modes/predict.md){ .md-button } \\n- **Train** a new YOLOv8 model on your own custom dataset &nbsp; [:fontawesome-solid-brain: Train a Model](modes/train.md){ .md-button }\\n- **Explore** YOLOv8 tasks like segment, classify, pose and track &nbsp; [:material-magnify-expand: Explore Tasks](tasks/index.md){ .md-button }\\n\\n## YOLO: A Brief History\\n\\n[YOLO](https://arxiv.org/abs/1506.02640) (You Only Look Once), a popular object detection and image segmentation model, was developed by Joseph Redmon and Ali Farhadi at the University of Washington. Launched in 2015, YOLO quickly gained popularity for its high speed and accuracy.\\n\\n- [YOLOv2](https://arxiv.org/abs/1612.08242), released in 2016, improved the original model by incorporating batch normalization, anchor boxes, and dimension clusters.\\n- [YOLOv3](https://pjreddie.com/media/files/papers/YOLOv3.pdf), launched in 2018, further enhanced the model\\'s performance using a more efficient backbone network, multiple anchors and spatial pyramid pooling.\\n- [YOLOv4](https://arxiv.org/abs/2004.10934) was released in 2020, introducing innovations like Mosaic data augmentation, a new anchor-free detection head, and a new loss function.\\n- [YOLOv5](https://github.com/ultralytics/yolov5) further improved the model\\'s performance and added new features such as hyperparameter optimization, integrated experiment tracking and automatic export to popular export formats.\\n- [YOLOv6](https://github.com/meituan/YOLOv6) was open-sourced by [Meituan](https://about.meituan.com/) in 2022 and is in use in many of the company\\'s autonomous delivery robots.\\n- [YOLOv7](https://github.com/WongKinYiu/yolov7) added additional tasks such as pose estimation on the COCO keypoints dataset.\\n- [YOLOv8](https://github.com/ultralytics/ultralytics) is the latest version of YOLO by Ultralytics. As a cutting-edge, state-of-the-art (SOTA) model, YOLOv8 builds on the success of previous versions, introducing new features and improvements for enhanced performance, flexibility, and efficiency. YOLOv8 supports a full range of vision AI tasks, including [detection](tasks/detect.md), [segmentation](tasks/segment.md), [pose estimation](tasks/pose.md), [tracking](modes/track.md), and [classification](tasks/classify.md). This versatility allows users to leverage YOLOv8\\'s capabilities across diverse applications and domains.\\n', metadata={'file_path': 'docs/index.md', 'file_name': 'index.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# YOLO Inference API (UNDER CONSTRUCTION)\\n\\nThe YOLO Inference API allows you to access the YOLOv8 object detection capabilities via a RESTful API. This enables you to run object detection on images without the need to install and set up the YOLOv8 environment locally.\\n\\n## API URL\\n\\nThe API URL is the address used to access the YOLO Inference API. In this case, the base URL is:\\n\\n```\\nhttps://api.ultralytics.com/inference/v1\\n```\\n\\nTo access the API with a specific model and your API key, you can include them as query parameters in the API URL. The `model` parameter refers to the `MODEL_ID` you want to use for inference, and the `key` parameter corresponds to your `API_KEY`.\\n\\nThe complete API URL with the model and API key parameters would be:\\n\\n```\\nhttps://api.ultralytics.com/inference/v1?model=MODEL_ID&key=API_KEY\\n```\\n\\nReplace `MODEL_ID` with the ID of the model you want to use and `API_KEY` with your actual API key from [https://hub.ultralytics.com/settings?tab=api+keys](https://hub.ultralytics.com/settings?tab=api+keys).\\n\\n## Example Usage in Python\\n\\nTo access the YOLO Inference API with the specified model and API key using Python, you can use the following code:\\n\\n```python\\nimport requests\\n\\napi_key = \"API_KEY\"\\nmodel_id = \"MODEL_ID\"\\nurl = f\"https://api.ultralytics.com/inference/v1?model={model_id}&key={api_key}\"\\nimage_path = \"image.jpg\"\\n\\nwith open(image_path, \"rb\") as image_file:\\n    files = {\"image\": image_file}\\n    response = requests.post(url, files=files)\\n\\nprint(response.text)\\n```\\n\\nIn this example, replace `API_KEY` with your actual API key, `MODEL_ID` with the desired model ID, and `image.jpg` with the path to the image you want to analyze.\\n\\n\\n## Example Usage with CLI\\n\\nYou can use the YOLO Inference API with the command-line interface (CLI) by utilizing the `curl` command. Replace `API_KEY` with your actual API key, `MODEL_ID` with the desired model ID, and `image.jpg` with the path to the image you want to analyze:\\n\\n```commandline\\ncurl -X POST -F image=@image.jpg \"https://api.ultralytics.com/inference/v1?model=MODEL_ID&key=API_KEY\"\\n```\\n\\n## Passing Arguments\\n\\nThis command sends a POST request to the YOLO Inference API with the specified `model` and `key` parameters in the URL, along with the image file specified by `@image.jpg`.\\n\\nHere\\'s an example of passing the `model`, `key`, and `normalize` arguments via the API URL using the `requests` library in Python:\\n\\n```python\\nimport requests\\n\\napi_key = \"API_KEY\"\\nmodel_id = \"MODEL_ID\"\\nurl = \"https://api.ultralytics.com/inference/v1\"\\n\\n# Define your query parameters\\nparams = {\\n    \"key\": api_key,\\n    \"model\": model_id,\\n    \"normalize\": \"True\"\\n}\\n\\nimage_path = \"image.jpg\"\\n\\nwith open(image_path, \"rb\") as image_file:\\n    files = {\"image\": image_file}\\n    response = requests.post(url, files=files, params=params)\\n\\nprint(response.text)\\n```\\n\\nIn this example, the `params` dictionary contains the query parameters `key`, `model`, and `normalize`, which tells the API to return all values in normalized image coordinates from 0 to 1. The `normalize` parameter is set to `\"True\"` as a string since query parameters should be passed as strings. These query parameters are then passed to the `requests.post()` function.\\n\\nThis will send the query parameters along with the file in the POST request. Make sure to consult the API documentation for the list of available arguments and their expected values.\\n\\n## Return JSON format\\n\\nThe YOLO Inference API returns a JSON list with the detection results. The format of the JSON list will be the same as the one produced locally by the `results[0].tojson()` command.\\n\\nThe JSON list contains information about the detected objects, their coordinates, classes, and confidence scores.\\n\\n### Detect Model Format\\n\\nYOLO detection models, such as `yolov8n.pt`, can return JSON responses from local inference, CLI API inference, and Python API inference. All of these methods produce the same JSON response format.\\n\\n!!! example \"Detect Model JSON Response\"\\n\\n    === \"Local\"\\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load model\\n        model = YOLO(\\'yolov8n.pt\\')\\n\\n        # Run inference\\n        results = model(\\'image.jpg\\')\\n\\n        # Print image.jpg results in JSON format\\n        print(results[0].tojson())  \\n        ```\\n\\n    === \"CLI API\"\\n        ```commandline\\n        curl -X POST -F image=@image.jpg https://api.ultralytics.com/inference/v1?model=MODEL_ID,key=API_KEY\\n        ```\\n\\n    === \"Python API\"\\n        ```python\\n        import requests\\n        \\n        api_key = \"API_KEY\"\\n        model_id = \"MODEL_ID\"\\n        url = \"https://api.ultralytics.com/inference/v1\"\\n        \\n        # Define your query parameters\\n        params = {\\n            \"key\": api_key,\\n            \"model\": model_id,\\n        }\\n        \\n        image_path = \"image.jpg\"\\n        \\n        with open(image_path, \"rb\") as image_file:\\n            files = {\"image\": image_file}\\n            response = requests.post(url, files=files, params=params)\\n        \\n        print(response.text)\\n        ```\\n\\n    === \"JSON Response\"\\n        ```json\\n        [\\n          {\\n            \"name\": \"person\",\\n            \"class\": 0,\\n            \"confidence\": 0.8359682559967041,\\n            \"box\": {\\n              \"x1\": 0.08974208831787109,\\n              \"y1\": 0.27418340047200523,\\n              \"x2\": 0.8706787109375,\\n              \"y2\": 0.9887352837456598\\n            }\\n          },\\n          {\\n            \"name\": \"person\",\\n            \"class\": 0,\\n            \"confidence\": 0.8189555406570435,\\n            \"box\": {\\n              \"x1\": 0.5847355842590332,\\n              \"y1\": 0.05813225640190972,\\n              \"x2\": 0.8930277824401855,\\n              \"y2\": 0.9903111775716146\\n            }\\n          },\\n          {\\n            \"name\": \"tie\",\\n            \"class\": 27,\\n            \"confidence\": 0.2909725308418274,\\n            \"box\": {\\n              \"x1\": 0.3433395862579346,\\n              \"y1\": 0.6070465511745877,\\n              \"x2\": 0.40964522361755373,\\n              \"y2\": 0.9849439832899306\\n            }\\n          }\\n        ]\\n        ```\\n\\n### Segment Model Format\\n\\nYOLO segmentation models, such as `yolov8n-seg.pt`, can return JSON responses from local inference, CLI API inference, and Python API inference. All of these methods produce the same JSON response format.\\n\\n!!! example \"Segment Model JSON Response\"\\n\\n    === \"Local\"\\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load model\\n        model = YOLO(\\'yolov8n-seg.pt\\')\\n\\n        # Run inference\\n        results = model(\\'image.jpg\\')\\n\\n        # Print image.jpg results in JSON format\\n        print(results[0].tojson())  \\n        ```\\n\\n    === \"CLI API\"\\n        ```commandline\\n        curl -X POST -F image=@image.jpg https://api.ultralytics.com/inference/v1?model=MODEL_ID,key=API_KEY\\n        ```\\n\\n    === \"Python API\"\\n        ```python\\n        import requests\\n        \\n        api_key = \"API_KEY\"\\n        model_id = \"MODEL_ID\"\\n        url = \"https://api.ultralytics.com/inference/v1\"\\n        \\n        # Define your query parameters\\n        params = {\\n            \"key\": api_key,\\n            \"model\": model_id,\\n        }\\n        \\n        image_path = \"image.jpg\"\\n        \\n        with open(image_path, \"rb\") as image_file:\\n            files = {\"image\": image_file}\\n            response = requests.post(url, files=files, params=params)\\n        \\n        print(response.text)\\n        ```\\n\\n    === \"JSON Response\"\\n        Note `segments` `x` and `y` lengths may vary from one object to another. Larger or more complex objects may have more segment points.\\n        ```json\\n        [\\n          {\\n            \"name\": \"person\",\\n            \"class\": 0,\\n            \"confidence\": 0.856913149356842,\\n            \"box\": {\\n              \"x1\": 0.1064866065979004,\\n              \"y1\": 0.2798851860894097,\\n              \"x2\": 0.8738358497619629,\\n              \"y2\": 0.9894873725043403\\n            },\\n            \"segments\": {\\n              \"x\": [\\n                0.421875,\\n                0.4203124940395355,\\n                0.41718751192092896\\n                ...\\n              ],\\n              \"y\": [\\n                0.2888889014720917,\\n                0.2916666567325592,\\n                0.2916666567325592\\n                ...\\n              ]\\n            }\\n          },\\n          {\\n            \"name\": \"person\",\\n            \"class\": 0,\\n            \"confidence\": 0.8512625694274902,\\n            \"box\": {\\n              \"x1\": 0.5757311820983887,\\n              \"y1\": 0.053943040635850696,\\n              \"x2\": 0.8960096359252929,\\n              \"y2\": 0.985154045952691\\n            },\\n            \"segments\": {\\n              \"x\": [\\n                0.7515624761581421,\\n                0.75,\\n                0.7437499761581421\\n                ...\\n              ],\\n              \"y\": [\\n                0.0555555559694767,\\n                0.05833333358168602,\\n                0.05833333358168602\\n                ...\\n              ]\\n            }\\n          },\\n          {\\n            \"name\": \"tie\",\\n            \"class\": 27,\\n            \"confidence\": 0.6485961675643921,\\n            \"box\": {\\n              \"x1\": 0.33911995887756347,\\n              \"y1\": 0.6057066175672743,\\n              \"x2\": 0.4081430912017822,\\n              \"y2\": 0.9916408962673611\\n            },\\n            \"segments\": {\\n              \"x\": [\\n                0.37187498807907104,\\n                0.37031251192092896,\\n                0.3687500059604645\\n                ...\\n              ],\\n              \"y\": [\\n                0.6111111044883728,\\n                0.6138888597488403,\\n                0.6138888597488403\\n                ...\\n              ]\\n            }\\n          }\\n        ]\\n        ```\\n\\n\\n### Pose Model Format\\n\\nYOLO pose models, such as `yolov8n-pose.pt`, can return JSON responses from local inference, CLI API inference, and Python API inference. All of these methods produce the same JSON response format.\\n\\n!!! example \"Pose Model JSON Response\"\\n\\n    === \"Local\"\\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load model\\n        model = YOLO(\\'yolov8n-seg.pt\\')\\n\\n        # Run inference\\n        results = model(\\'image.jpg\\')\\n\\n        # Print image.jpg results in JSON format\\n        print(results[0].tojson())  \\n        ```\\n\\n    === \"CLI API\"\\n        ```commandline\\n        curl -X POST -F image=@image.jpg https://api.ultralytics.com/inference/v1?model=MODEL_ID,key=API_KEY\\n        ```\\n\\n    === \"Python API\"\\n        ```python\\n        import requests\\n        \\n        api_key = \"API_KEY\"\\n        model_id = \"MODEL_ID\"\\n        url = \"https://api.ultralytics.com/inference/v1\"\\n        \\n        # Define your query parameters\\n        params = {\\n            \"key\": api_key,\\n            \"model\": model_id,\\n        }\\n        \\n        image_path = \"image.jpg\"\\n        \\n        with open(image_path, \"rb\") as image_file:\\n            files = {\"image\": image_file}\\n            response = requests.post(url, files=files, params=params)\\n        \\n        print(response.text)\\n        ```\\n\\n    === \"JSON Response\"\\n        Note COCO-keypoints pretrained models will have 17 human keypoints. The `visible` part of the keypoints indicates whether a keypoint is visible or obscured. Obscured keypoints may be outside the image or may not be visible, i.e. a person\\'s eyes facing away from the camera.\\n        ```json\\n        [\\n          {\\n            \"name\": \"person\",\\n            \"class\": 0,\\n            \"confidence\": 0.8439509868621826,\\n            \"box\": {\\n              \"x1\": 0.1125,\\n              \"y1\": 0.28194444444444444,\\n              \"x2\": 0.7953125,\\n              \"y2\": 0.9902777777777778\\n            },\\n            \"keypoints\": {\\n              \"x\": [\\n                0.5058594942092896,\\n                0.5103894472122192,\\n                0.4920862317085266\\n                ...\\n              ],\\n              \"y\": [\\n                0.48964157700538635,\\n                0.4643048942089081,\\n                0.4465252459049225\\n                ...\\n              ],\\n              \"visible\": [\\n                0.8726999163627625,\\n                0.653947651386261,\\n                0.9130823612213135\\n                ...\\n              ]\\n            }\\n          },\\n          {\\n            \"name\": \"person\",\\n            \"class\": 0,\\n            \"confidence\": 0.7474289536476135,\\n            \"box\": {\\n              \"x1\": 0.58125,\\n              \"y1\": 0.0625,\\n              \"x2\": 0.8859375,\\n              \"y2\": 0.9888888888888889\\n            },\\n            \"keypoints\": {\\n              \"x\": [\\n                0.778544008731842,\\n                0.7976160049438477,\\n                0.7530890107154846\\n                ...\\n              ],\\n              \"y\": [\\n                0.27595141530036926,\\n                0.2378823608160019,\\n                0.23644638061523438\\n                ...\\n              ],\\n              \"visible\": [\\n                0.8900790810585022,\\n                0.789978563785553,\\n                0.8974530100822449\\n                ...\\n              ]\\n            }\\n          }\\n        ]\\n        ```', metadata={'file_path': 'docs/inference_api.md', 'file_name': 'inference_api.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n## Install\\n\\nInstall YOLOv8 via the `ultralytics` pip package for the latest stable release or by cloning\\nthe [https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics) repository for the most\\nup-to-date version.\\n\\n!!! example \"Install\"\\n\\n    === \"pip install (recommended)\"\\n        ```bash\\n        pip install ultralytics\\n        ```\\n\\n    === \"git clone (for development)\"\\n        ```bash\\n        git clone https://github.com/ultralytics/ultralytics\\n        cd ultralytics\\n        pip install -e .\\n        ```\\n\\nSee the `ultralytics` [requirements.txt](https://github.com/ultralytics/ultralytics/blob/main/requirements.txt) file for a list of dependencies. Note that `pip` automatically installs all required dependencies.\\n\\n!!! tip \"Tip\"\\n\\n    PyTorch requirements vary by operating system and CUDA requirements, so it\\'s recommended to install PyTorch first following instructions at [https://pytorch.org/get-started/locally](https://pytorch.org/get-started/locally).\\n\\n    <a href=\"https://pytorch.org/get-started/locally/\">\\n        <img width=\"800\" alt=\"PyTorch Installation Instructions\" src=\"https://user-images.githubusercontent.com/26833433/228650108-ab0ec98a-b328-4f40-a40d-95355e8a84e3.png\">\\n    </a>\\n\\n\\n## Use with CLI\\n\\nThe YOLO command line interface (CLI) allows for simple single-line commands without the need for a Python environment.\\nCLI requires no customization or Python code. You can simply run all tasks from the terminal with the `yolo` command. Check out the [CLI Guide](usage/cli.md) to learn more about using YOLOv8 from the command line.\\n\\n\\n!!! example\\n\\n    === \"Syntax\"\\n\\n        Ultralytics `yolo` commands use the following syntax:\\n        ```bash\\n        yolo TASK MODE ARGS\\n\\n        Where   TASK (optional) is one of [detect, segment, classify]\\n                MODE (required) is one of [train, val, predict, export, track]\\n                ARGS (optional) are any number of custom \\'arg=value\\' pairs like \\'imgsz=320\\' that override defaults.\\n        ```\\n        See all ARGS in the full [Configuration Guide](usage/cfg.md) or with `yolo cfg`\\n\\n    === \"Train\"\\n\\n        Train a detection model for 10 epochs with an initial learning_rate of 0.01\\n        ```bash\\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\\n        ```\\n\\n    === \"Predict\"\\n\\n        Predict a YouTube video using a pretrained segmentation model at image size 320:\\n        ```bash\\n        yolo predict model=yolov8n-seg.pt source=\\'https://youtu.be/Zgi9g1ksQHc\\' imgsz=320\\n        ```\\n\\n    === \"Val\"\\n\\n        Val a pretrained detection model at batch-size 1 and image size 640:\\n        ```bash\\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\\n        ```\\n\\n    === \"Export\"\\n\\n        Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\\n        ```bash\\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\\n        ```\\n\\n    === \"Special\"\\n\\n        Run special commands to see version, view settings, run checks and more:\\n        ```bash\\n        yolo help\\n        yolo checks\\n        yolo version\\n        yolo settings\\n        yolo copy-cfg\\n        yolo cfg\\n        ```\\n\\n\\n!!! warning \"Warning\"\\n\\n    Arguments must be passed as `arg=val` pairs, split by an equals `=` sign and delimited by spaces ` ` between pairs. Do not use `--` argument prefixes or commas `,` beteen arguments.\\n\\n    - `yolo predict model=yolov8n.pt imgsz=640 conf=0.25` &nbsp; ✅\\n    - `yolo predict model yolov8n.pt imgsz 640 conf 0.25` &nbsp; ❌\\n    - `yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25` &nbsp; ❌\\n\\n[CLI Guide](usage/cli.md){ .md-button .md-button--primary}\\n\\n## Use with Python\\n\\nYOLOv8\\'s Python interface allows for seamless integration into your Python projects, making it easy to load, run, and process the model\\'s output. Designed with simplicity and ease of use in mind, the Python interface enables users to quickly implement object detection, segmentation, and classification in their projects. This makes YOLOv8\\'s Python interface an invaluable tool for anyone looking to incorporate these functionalities into their Python projects.\\n\\nFor example, users can load a model, train it, evaluate its performance on a validation set, and even export it to ONNX format with just a few lines of code. Check out the [Python Guide](usage/python.md) to learn more about using YOLOv8 within your Python projects.\\n\\n!!! example\\n\\n    ```python\\n    from ultralytics import YOLO\\n    \\n    # Create a new YOLO model from scratch\\n    model = YOLO(\\'yolov8n.yaml\\')\\n    \\n    # Load a pretrained YOLO model (recommended for training)\\n    model = YOLO(\\'yolov8n.pt\\')\\n    \\n    # Train the model using the \\'coco128.yaml\\' dataset for 3 epochs\\n    results = model.train(data=\\'coco128.yaml\\', epochs=3)\\n    \\n    # Evaluate the model\\'s performance on the validation set\\n    results = model.val()\\n    \\n    # Perform object detection on an image using the model\\n    results = model(\\'https://ultralytics.com/images/bus.jpg\\')\\n    \\n    # Export the model to ONNX format\\n    success = model.export(format=\\'onnx\\')\\n    ```\\n\\n[Python Guide](usage/python.md){.md-button .md-button--primary}\\n', metadata={'file_path': 'docs/quickstart.md', 'file_name': 'quickstart.md', 'file_type': '.md'}), Document(page_content='## Ultralytics YOLOv8 Example Applications\\n\\nThis repository features a collection of real-world applications and walkthroughs, provided as either Python files or notebooks. Explore the examples below to see how YOLOv8 can be integrated into various applications.\\n\\n### Ultralytics YOLO Example Applications\\n\\n| Title                                                                                                          | Format             | Contributor                                         |\\n| -------------------------------------------------------------------------------------------------------------- | ------------------ | --------------------------------------------------- |\\n| [YOLO ONNX Detection Inference with C++](./YOLOv8-CPP-Inference)                                               | C++/ONNX           | [Justas Bartnykas](https://github.com/JustasBart)   |\\n| [YOLO OpenCV ONNX Detection Python](./YOLOv8-OpenCV-ONNX-Python)                                               | OpenCV/Python/ONNX | [Farid Inawan](https://github.com/frdteknikelektro) |\\n| [YOLO .Net ONNX Detection C#](https://www.nuget.org/packages/Yolov8.Net)                                       | C# .Net            | [Samuel Stainback](https://github.com/sstainba)     |\\n| [YOLOv8 on NVIDIA Jetson(TensorRT and DeepStream)](https://wiki.seeedstudio.com/YOLOv8-DeepStream-TRT-Jetson/) | Python             | [Lakshantha](https://github.com/lakshanthad)        |\\n\\n### How to Contribute\\n\\nWe welcome contributions from the community in the form of examples, applications, and guides. To contribute, please follow these steps:\\n\\n1. Create a pull request (PR) with the `[Example]` prefix in the title, adding your project folder to the `examples/` directory in the repository.\\n1. Ensure that your project meets the following criteria:\\n   - Utilizes the `ultralytics` package.\\n   - Includes a `README.md` file with instructions on how to run the project.\\n   - Avoids adding large assets or dependencies unless absolutely necessary.\\n   - The contributor is expected to provide support for issues related to their examples.\\n\\nIf you have any questions or concerns about these requirements, please submit a PR, and we will be more than happy to guide you.\\n', metadata={'file_path': 'examples/README.md', 'file_name': 'README.md', 'file_type': '.md'}), Document(page_content='# Ultralytics Individual Contributor License Agreement\\n\\nThank you for your interest in contributing to open source software projects (“Projects”) made available by Ultralytics\\nSE or its affiliates (“Ultralytics”). This Individual Contributor License Agreement (“Agreement”) sets out the terms\\ngoverning any source code, object code, bug fixes, configuration changes, tools, specifications, documentation, data,\\nmaterials, feedback, information or other works of authorship that you submit or have submitted, in any form and in any\\nmanner, to Ultralytics in respect of any of the Projects (collectively “Contributions”). If you have any questions\\nrespecting this Agreement, please contact hello@ultralytics.com.\\n\\nYou agree that the following terms apply to all of your past, present and future Contributions. Except for the licenses\\ngranted in this Agreement, you retain all of your right, title and interest in and to your Contributions.\\n\\n**Copyright License.** You hereby grant, and agree to grant, to Ultralytics a non-exclusive, perpetual, irrevocable,\\nworldwide, fully-paid, royalty-free, transferable copyright license to reproduce, prepare derivative works of, publicly\\ndisplay, publicly perform, and distribute your Contributions and such derivative works, with the right to sublicense the\\nforegoing rights through multiple tiers of sublicensees.\\n\\n**Patent License.** You hereby grant, and agree to grant, to Ultralytics a non-exclusive, perpetual, irrevocable,\\nworldwide, fully-paid, royalty-free, transferable patent license to make, have made, use, offer to sell, sell,\\nimport, and otherwise transfer your Contributions, where such license applies only to those patent claims\\nlicensable by you that are necessarily infringed by your Contributions alone or by combination of your\\nContributions with the Project to which such Contributions were submitted, with the right to sublicense the\\nforegoing rights through multiple tiers of sublicensees.\\n\\n**Moral Rights.** To the fullest extent permitted under applicable law, you hereby waive, and agree not to\\nassert, all of your “moral rights” in or relating to your Contributions for the benefit of Ultralytics, its assigns, and\\ntheir respective direct and indirect sublicensees.\\n\\n**Third Party Content/Rights.** If your Contribution includes or is based on any source code, object code, bug\\nfixes, configuration changes, tools, specifications, documentation, data, materials, feedback, information or\\nother works of authorship that were not authored by you (“Third Party Content”) or if you are aware of any\\nthird party intellectual property or proprietary rights associated with your Contribution (“Third Party Rights”),\\nthen you agree to include with the submission of your Contribution full details respecting such Third Party\\nContent and Third Party Rights, including, without limitation, identification of which aspects of your\\nContribution contain Third Party Content or are associated with Third Party Rights, the owner/author of the\\nThird Party Content and Third Party Rights, where you obtained the Third Party Content, and any applicable\\nthird party license terms or restrictions respecting the Third Party Content and Third Party Rights. For greater\\ncertainty, the foregoing obligations respecting the identification of Third Party Content and Third Party Rights\\ndo not apply to any portion of a Project that is incorporated into your Contribution to that same Project.\\n\\n**Representations.** You represent that, other than the Third Party Content and Third Party Rights identified by\\nyou in accordance with this Agreement, you are the sole author of your Contributions and are legally entitled\\nto grant the foregoing licenses and waivers in respect of your Contributions. If your Contributions were\\ncreated in the course of your employment with your past or present employer(s), you represent that such\\nemployer(s) has authorized you to make your Contributions on behalf of such employer(s) or such employer\\n(s) has waived all of their right, title or interest in or to your Contributions.\\n\\n**Disclaimer.** To the fullest extent permitted under applicable law, your Contributions are provided on an \"asis\"\\nbasis, without any warranties or conditions, express or implied, including, without limitation, any implied\\nwarranties or conditions of non-infringement, merchantability or fitness for a particular purpose. You are not\\nrequired to provide support for your Contributions, except to the extent you desire to provide support.\\n\\n**No Obligation.** You acknowledge that Ultralytics is under no obligation to use or incorporate your Contributions\\ninto any of the Projects. The decision to use or incorporate your Contributions into any of the Projects will be\\nmade at the sole discretion of Ultralytics or its authorized delegates ..\\n\\n**Disputes.** This Agreement shall be governed by and construed in accordance with the laws of the State of\\nNew York, United States of America, without giving effect to its principles or rules regarding conflicts of laws,\\nother than such principles directing application of New York law. The parties hereby submit to venue in, and\\njurisdiction of the courts located in New York, New York for purposes relating to this Agreement. In the event\\nthat any of the provisions of this Agreement shall be held by a court or other tribunal of competent jurisdiction\\nto be unenforceable, the remaining portions hereof shall remain in full force and effect.\\n\\n**Assignment.** You agree that Ultralytics may assign this Agreement, and all of its rights, obligations and licenses\\nhereunder.\\n', metadata={'file_path': 'docs/help/CLA.md', 'file_name': 'CLA.md', 'file_type': '.md'}), Document(page_content=\"---\\ncomments: true\\n---\\n\\n# Ultralytics YOLO Frequently Asked Questions (FAQ)\\n\\nThis FAQ section addresses some common questions and issues users might encounter while working with Ultralytics YOLO repositories.\\n\\n## 1. What are the hardware requirements for running Ultralytics YOLO?\\n\\nUltralytics YOLO can be run on a variety of hardware configurations, including CPUs, GPUs, and even some edge devices. However, for optimal performance and faster training and inference, we recommend using a GPU with a minimum of 8GB of memory. NVIDIA GPUs with CUDA support are ideal for this purpose.\\n\\n## 2. How do I fine-tune a pre-trained YOLO model on my custom dataset?\\n\\nTo fine-tune a pre-trained YOLO model on your custom dataset, you'll need to create a dataset configuration file (YAML) that defines the dataset's properties, such as the path to the images, the number of classes, and class names. Next, you'll need to modify the model configuration file to match the number of classes in your dataset. Finally, use the `train.py` script to start the training process with your custom dataset and the pre-trained model. You can find a detailed guide on fine-tuning YOLO in the Ultralytics documentation.\\n\\n## 3. How do I convert a YOLO model to ONNX or TensorFlow format?\\n\\nUltralytics provides built-in support for converting YOLO models to ONNX format. You can use the `export.py` script to convert a saved model to ONNX format. If you need to convert the model to TensorFlow format, you can use the ONNX model as an intermediary and then use the ONNX-TensorFlow converter to convert the ONNX model to TensorFlow format.\\n\\n## 4. Can I use Ultralytics YOLO for real-time object detection?\\n\\nYes, Ultralytics YOLO is designed to be efficient and fast, making it suitable for real-time object detection tasks. The actual performance will depend on your hardware configuration and the complexity of the model. Using a GPU and optimizing the model for your specific use case can help achieve real-time performance.\\n\\n## 5. How can I improve the accuracy of my YOLO model?\\n\\nImproving the accuracy of a YOLO model may involve several strategies, such as:\\n\\n- Fine-tuning the model on more annotated data\\n- Data augmentation to increase the variety of training samples\\n- Using a larger or more complex model architecture\\n- Adjusting the learning rate, batch size, and other hyperparameters\\n- Using techniques like transfer learning or knowledge distillation\\n\\nRemember that there's often a trade-off between accuracy and inference speed, so finding the right balance is crucial for your specific application.\\n\\nIf you have any more questions or need assistance, don't hesitate to consult the Ultralytics documentation or reach out to the community through GitHub Issues or the official discussion forum.\", metadata={'file_path': 'docs/help/FAQ.md', 'file_name': 'FAQ.md', 'file_type': '.md'}), Document(page_content=\"---\\ncomments: true\\n---\\n\\n# Ultralytics Contributor Covenant Code of Conduct\\n\\n## Our Pledge\\n\\nWe as members, contributors, and leaders pledge to make participation in our\\ncommunity a harassment-free experience for everyone, regardless of age, body\\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\\nidentity and expression, level of experience, education, socio-economic status,\\nnationality, personal appearance, race, religion, or sexual identity\\nand orientation.\\n\\nWe pledge to act and interact in ways that contribute to an open, welcoming,\\ndiverse, inclusive, and healthy community.\\n\\n## Our Standards\\n\\nExamples of behavior that contributes to a positive environment for our\\ncommunity include:\\n\\n- Demonstrating empathy and kindness toward other people\\n- Being respectful of differing opinions, viewpoints, and experiences\\n- Giving and gracefully accepting constructive feedback\\n- Accepting responsibility and apologizing to those affected by our mistakes,\\n  and learning from the experience\\n- Focusing on what is best not just for us as individuals, but for the\\n  overall community\\n\\nExamples of unacceptable behavior include:\\n\\n- The use of sexualized language or imagery, and sexual attention or\\n  advances of any kind\\n- Trolling, insulting or derogatory comments, and personal or political attacks\\n- Public or private harassment\\n- Publishing others' private information, such as a physical or email\\n  address, without their explicit permission\\n- Other conduct which could reasonably be considered inappropriate in a\\n  professional setting\\n\\n## Enforcement Responsibilities\\n\\nCommunity leaders are responsible for clarifying and enforcing our standards of\\nacceptable behavior and will take appropriate and fair corrective action in\\nresponse to any behavior that they deem inappropriate, threatening, offensive,\\nor harmful.\\n\\nCommunity leaders have the right and responsibility to remove, edit, or reject\\ncomments, commits, code, wiki edits, issues, and other contributions that are\\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\\ndecisions when appropriate.\\n\\n## Scope\\n\\nThis Code of Conduct applies within all community spaces, and also applies when\\nan individual is officially representing the community in public spaces.\\nExamples of representing our community include using an official e-mail address,\\nposting via an official social media account, or acting as an appointed\\nrepresentative at an online or offline event.\\n\\n## Enforcement\\n\\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\\nreported to the community leaders responsible for enforcement at\\nhello@ultralytics.com.\\nAll complaints will be reviewed and investigated promptly and fairly.\\n\\nAll community leaders are obligated to respect the privacy and security of the\\nreporter of any incident.\\n\\n## Enforcement Guidelines\\n\\nCommunity leaders will follow these Community Impact Guidelines in determining\\nthe consequences for any action they deem in violation of this Code of Conduct:\\n\\n### 1. Correction\\n\\n**Community Impact**: Use of inappropriate language or other behavior deemed\\nunprofessional or unwelcome in the community.\\n\\n**Consequence**: A private, written warning from community leaders, providing\\nclarity around the nature of the violation and an explanation of why the\\nbehavior was inappropriate. A public apology may be requested.\\n\\n### 2. Warning\\n\\n**Community Impact**: A violation through a single incident or series\\nof actions.\\n\\n**Consequence**: A warning with consequences for continued behavior. No\\ninteraction with the people involved, including unsolicited interaction with\\nthose enforcing the Code of Conduct, for a specified period of time. This\\nincludes avoiding interactions in community spaces as well as external channels\\nlike social media. Violating these terms may lead to a temporary or\\npermanent ban.\\n\\n### 3. Temporary Ban\\n\\n**Community Impact**: A serious violation of community standards, including\\nsustained inappropriate behavior.\\n\\n**Consequence**: A temporary ban from any sort of interaction or public\\ncommunication with the community for a specified period of time. No public or\\nprivate interaction with the people involved, including unsolicited interaction\\nwith those enforcing the Code of Conduct, is allowed during this period.\\nViolating these terms may lead to a permanent ban.\\n\\n### 4. Permanent Ban\\n\\n**Community Impact**: Demonstrating a pattern of violation of community\\nstandards, including sustained inappropriate behavior,  harassment of an\\nindividual, or aggression toward or disparagement of classes of individuals.\\n\\n**Consequence**: A permanent ban from any sort of public interaction within\\nthe community.\\n\\n## Attribution\\n\\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\\nversion 2.0, available at\\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\\n\\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\\nenforcement ladder](https://github.com/mozilla/diversity).\\n\\nFor answers to common questions about this code of conduct, see the FAQ at\\nhttps://www.contributor-covenant.org/faq. Translations are available at\\nhttps://www.contributor-covenant.org/translations.\\n\\n[homepage]: https://www.contributor-covenant.org\\n\", metadata={'file_path': 'docs/help/code_of_conduct.md', 'file_name': 'code_of_conduct.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Contributing to Ultralytics Open-Source YOLO Repositories\\n\\nFirst of all, thank you for your interest in contributing to Ultralytics open-source YOLO repositories! Your contributions will help improve the project and benefit the community. This document provides guidelines and best practices for contributing to Ultralytics YOLO repositories.\\n\\n## Table of Contents\\n\\n- [Code of Conduct](#code-of-conduct)\\n- [Pull Requests](#pull-requests)\\n  - [CLA Signing](#cla-signing)\\n  - [Google-Style Docstrings](#google-style-docstrings)\\n  - [GitHub Actions CI Tests](#github-actions-ci-tests)\\n- [Bug Reports](#bug-reports)\\n  - [Minimum Reproducible Example](#minimum-reproducible-example)\\n- [License and Copyright](#license-and-copyright)\\n\\n## Code of Conduct\\n\\nAll contributors are expected to adhere to the [Code of Conduct](code_of_conduct.md) to ensure a welcoming and inclusive environment for everyone.\\n\\n## Pull Requests\\n\\nWe welcome contributions in the form of pull requests. To make the review process smoother, please follow these guidelines:\\n\\n1. **Fork the repository**: Fork the Ultralytics YOLO repository to your own GitHub account.\\n\\n2. **Create a branch**: Create a new branch in your forked repository with a descriptive name for your changes.\\n\\n3. **Make your changes**: Make the changes you want to contribute. Ensure that your changes follow the coding style of the project and do not introduce new errors or warnings.\\n\\n4. **Test your changes**: Test your changes locally to ensure that they work as expected and do not introduce new issues.\\n\\n5. **Commit your changes**: Commit your changes with a descriptive commit message. Make sure to include any relevant issue numbers in your commit message.\\n\\n6. **Create a pull request**: Create a pull request from your forked repository to the main Ultralytics YOLO repository. In the pull request description, provide a clear explanation of your changes and how they improve the project.\\n\\n### CLA Signing\\n\\nBefore we can accept your pull request, you need to sign a [Contributor License Agreement (CLA)](CLA.md). This is a legal document stating that you agree to the terms of contributing to the Ultralytics YOLO repositories. The CLA ensures that your contributions are properly licensed and that the project can continue to be distributed under the AGPL-3.0 license.\\n\\nTo sign the CLA, follow the instructions provided by the CLA bot after you submit your PR.\\n\\n### Google-Style Docstrings\\n\\nWhen adding new functions or classes, please include a [Google-style docstring](https://google.github.io/styleguide/pyguide.html) to provide clear and concise documentation for other developers. This will help ensure that your contributions are easy to understand and maintain.\\n\\nExample Google-style docstring:\\n\\n```python\\ndef example_function(arg1: int, arg2: str) -> bool:\\n    \"\"\"Example function that demonstrates Google-style docstrings.\\n\\n    Args:\\n        arg1 (int): The first argument.\\n        arg2 (str): The second argument.\\n\\n    Returns:\\n        bool: True if successful, False otherwise.\\n\\n    Raises:\\n        ValueError: If `arg1` is negative or `arg2` is empty.\\n    \"\"\"\\n    if arg1 < 0 or not arg2:\\n        raise ValueError(\"Invalid input values\")\\n    return True\\n```\\n\\n### GitHub Actions CI Tests\\n\\nBefore your pull request can be merged, all GitHub Actions Continuous Integration (CI) tests must pass. These tests include linting, unit tests, and other checks to ensure that your changes meet the quality standards of the project. Make sure to review the output of the GitHub Actions and fix any issues', metadata={'file_path': 'docs/help/contributing.md', 'file_name': 'contributing.md', 'file_type': '.md'}), Document(page_content=\"---\\ncomments: true\\n---\\n\\nWelcome to the Ultralytics Help page! We are committed to providing you with comprehensive resources to make your experience with Ultralytics YOLO repositories as smooth and enjoyable as possible. On this page, you'll find essential links to guides and documents that will help you navigate through common tasks and address any questions you might have while using our repositories.\\n\\n- [Frequently Asked Questions (FAQ)](FAQ.md): Find answers to common questions and issues faced by users and contributors of Ultralytics YOLO repositories.\\n- [Contributing Guide](contributing.md): Learn the best practices for submitting pull requests, reporting bugs, and contributing to the development of our repositories.\\n- [Contributor License Agreement (CLA)](CLA.md): Familiarize yourself with our CLA to understand the terms and conditions for contributing to Ultralytics projects.\\n- [Minimum Reproducible Example (MRE) Guide](minimum_reproducible_example.md): Understand how to create an MRE when submitting bug reports to ensure that our team can quickly and efficiently address the issue.\\n- [Code of Conduct](code_of_conduct.md): Learn about our community guidelines and expectations to ensure a welcoming and inclusive environment for all participants.\\n- [Security Policy](../SECURITY.md): Understand our security practices and how to report security vulnerabilities responsibly.\\n\\nWe highly recommend going through these guides to make the most of your collaboration with the Ultralytics community. Our goal is to maintain a welcoming and supportive environment for all users and contributors. If you need further assistance, don't hesitate to reach out to us through GitHub Issues or the official discussion forum. Happy coding!\", metadata={'file_path': 'docs/help/index.md', 'file_name': 'index.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Creating a Minimum Reproducible Example for Bug Reports in Ultralytics YOLO Repositories\\n\\nWhen submitting a bug report for Ultralytics YOLO repositories, it\\'s essential to provide a [minimum reproducible example](https://stackoverflow.com/help/minimal-reproducible-example) (MRE). An MRE is a small, self-contained piece of code that demonstrates the problem you\\'re experiencing. Providing an MRE helps maintainers and contributors understand the issue and work on a fix more efficiently. This guide explains how to create an MRE when submitting bug reports to Ultralytics YOLO repositories.\\n\\n## 1. Isolate the Problem\\n\\nThe first step in creating an MRE is to isolate the problem. This means removing any unnecessary code or dependencies that are not directly related to the issue. Focus on the specific part of the code that is causing the problem and remove any irrelevant code.\\n\\n## 2. Use Public Models and Datasets\\n\\nWhen creating an MRE, use publicly available models and datasets to reproduce the issue. For example, use the \\'yolov8n.pt\\' model and the \\'coco8.yaml\\' dataset. This ensures that the maintainers and contributors can easily run your example and investigate the problem without needing access to proprietary data or custom models.\\n\\n## 3. Include All Necessary Dependencies\\n\\nMake sure to include all the necessary dependencies in your MRE. If your code relies on external libraries, specify the required packages and their versions. Ideally, provide a `requirements.txt` file or list the dependencies in your bug report.\\n\\n## 4. Write a Clear Description of the Issue\\n\\nProvide a clear and concise description of the issue you\\'re experiencing. Explain the expected behavior and the actual behavior you\\'re encountering. If applicable, include any relevant error messages or logs.\\n\\n## 5. Format Your Code Properly\\n\\nWhen submitting an MRE, format your code properly using code blocks in the issue description. This makes it easier for others to read and understand your code. In GitHub, you can create a code block by wrapping your code with triple backticks (\\\\```) and specifying the language:\\n\\n<pre>\\n```python\\n# Your Python code goes here\\n```\\n</pre>\\n\\n## 6. Test Your MRE\\n\\nBefore submitting your MRE, test it to ensure that it accurately reproduces the issue. Make sure that others can run your example without any issues or modifications.\\n\\n## Example of an MRE\\n\\nHere\\'s an example of an MRE for a hypothetical bug report:\\n\\n**Bug description:**\\n\\nWhen running the `detect.py` script on the sample image from the \\'coco8.yaml\\' dataset, I get an error related to the dimensions of the input tensor.\\n\\n**MRE:**\\n\\n```python\\nimport torch\\nfrom ultralytics import YOLO\\n\\n# Load the model\\nmodel = YOLO(\"yolov8n.pt\")\\n\\n# Load a 0-channel image\\nimage = torch.rand(1, 0, 640, 640)\\n\\n# Run the model\\nresults = model(image)\\n```\\n\\n**Error message:**\\n\\n```\\nRuntimeError: Expected input[1, 0, 640, 640] to have 3 channels, but got 0 channels instead\\n```\\n\\n**Dependencies:**\\n\\n- torch==2.0.0\\n- ultralytics==8.0.90\\n\\nIn this example, the MRE demonstrates the issue with a minimal amount of code, uses a public model (\\'yolov8n.pt\\'), includes all necessary dependencies, and provides a clear description of the problem along with the error message.\\n\\nBy following these guidelines, you\\'ll help the maintainers and contributors of Ultralytics YOLO repositories to understand and resolve your issue more efficiently.', metadata={'file_path': 'docs/help/minimum_reproducible_example.md', 'file_name': 'minimum_reproducible_example.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Models\\n\\nUltralytics supports many models and architectures with more to come in the future. Want to add your model architecture? [Here\\'s](../help/contributing.md) how you can contribute.\\n\\nIn this documentation, we provide information on four major models:\\n\\n1. [YOLOv3](./yolov3.md): The third iteration of the YOLO model family, known for its efficient real-time object detection capabilities.\\n2. [YOLOv5](./yolov5.md): An improved version of the YOLO architecture, offering better performance and speed tradeoffs compared to previous versions.\\n3. [YOLOv8](./yolov8.md): The latest version of the YOLO family, featuring enhanced capabilities such as instance segmentation, pose/keypoints estimation, and classification.\\n4. [Segment Anything Model (SAM)](./sam.md): Meta\\'s Segment Anything Model (SAM).\\n\\nYou can use these models directly in the Command Line Interface (CLI) or in a Python environment. Below are examples of how to use the models with CLI and Python:\\n\\n## CLI Example\\n\\n```bash\\nyolo task=detect mode=train model=yolov8n.yaml data=coco128.yaml epochs=100\\n```\\n\\n## Python Example\\n\\n```python\\nfrom ultralytics import YOLO\\n\\nmodel = YOLO(\"model.yaml\")  # build a YOLOv8n model from scratch\\n# YOLO(\"model.pt\")  use pre-trained model if available\\nmodel.info()  # display model information\\nmodel.train(data=\"coco128.yaml\", epochs=100)  # train the model\\n```\\n\\nFor more details on each model, their supported tasks, modes, and performance, please visit their respective documentation pages linked above.', metadata={'file_path': 'docs/models/index.md', 'file_name': 'index.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Vision Transformers\\n\\nVit models currently support Python environment:\\n\\n```python\\nfrom ultralytics.vit import SAM\\n\\n# from ultralytics.vit import MODEL_TYPe\\n\\nmodel = SAM(\"sam_b.pt\")\\nmodel.info()  # display model information\\nmodel.predict(...)  # train the model\\n```\\n\\n# Segment Anything\\n\\n## About\\n\\n## Supported Tasks\\n\\n| Model Type | Pre-trained Weights | Tasks Supported       |\\n|------------|---------------------|-----------------------|\\n| sam base   | `sam_b.pt`          | Instance Segmentation |\\n| sam large  | `sam_l.pt`          | Instance Segmentation |\\n\\n## Supported Modes\\n\\n| Mode       | Supported          |\\n|------------|--------------------|\\n| Inference  | :heavy_check_mark: |\\n| Validation | :x:                |\\n| Training   | :x:                |\\n', metadata={'file_path': 'docs/models/sam.md', 'file_name': 'sam.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# 🚧Page Under Construction ⚒\\n\\nThis page is currently under construction!️👷Please check back later for updates. 😃🔜\\n', metadata={'file_path': 'docs/models/yolov3.md', 'file_name': 'yolov3.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# YOLOv5u\\n\\n## About\\n\\nAnchor-free YOLOv5 models with improved accuracy-speed tradeoff.\\n\\n## Supported Tasks\\n\\n| Model Type | Pre-trained Weights                                                                                                         | Task      |\\n|------------|-----------------------------------------------------------------------------------------------------------------------------|-----------|\\n| YOLOv5u    | `yolov5nu`, `yolov5su`, `yolov5mu`, `yolov5lu`, `yolov5xu`, `yolov5n6u`, `yolov5s6u`, `yolov5m6u`, `yolov5l6u`, `yolov5x6u` | Detection |\\n\\n## Supported Modes\\n\\n| Mode       | Supported          |\\n|------------|--------------------|\\n| Inference  | :heavy_check_mark: |\\n| Validation | :heavy_check_mark: |\\n| Training   | :heavy_check_mark: |\\n\\n??? Performance\\n\\n    === \"Detection\"\\n\\n        | Model                                                                                    | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n        | ---------------------------------------------------------------------------------------- | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\\n        | [YOLOv5nu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5nu.pt)   | 640                   | 34.3                 | 73.6                           | 1.06                                | 2.6                | 7.7               |\\n        | [YOLOv5su](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5su.pt)   | 640                   | 43.0                 | 120.7                          | 1.27                                | 9.1                | 24.0              |\\n        | [YOLOv5mu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5mu.pt)   | 640                   | 49.0                 | 233.9                          | 1.86                                | 25.1               | 64.2              |\\n        | [YOLOv5lu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5lu.pt)   | 640                   | 52.2                 | 408.4                          | 2.50                                | 53.2               | 135.0             |\\n        | [YOLOv5xu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5xu.pt)   | 640                   | 53.2                 | 763.2                          | 3.81                                | 97.2               | 246.4             |\\n        |                                                                                          |                       |                      |                                |                                     |                    |                   |\\n        | [YOLOv5n6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5n6u.pt) | 1280                  | 42.1                 | -                              | -                                   | 4.3                | 7.8               |\\n        | [YOLOv5s6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5s6u.pt) | 1280                  | 48.6                 | -                              | -                                   | 15.3               | 24.6              |\\n        | [YOLOv5m6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5m6u.pt) | 1280                  | 53.6                 | -                              | -                                   | 41.2               | 65.7              |\\n        | [YOLOv5l6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5l6u.pt) | 1280                  | 55.7                 | -                              | -                                   | 86.1               | 137.4             |\\n        | [YOLOv5x6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5x6u.pt) | 1280                  | 56.8                 | -                              | -                                   | 155.4              | 250.7             |\\n', metadata={'file_path': 'docs/models/yolov5.md', 'file_name': 'yolov5.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# YOLOv8\\n\\n## About\\n\\n## Supported Tasks\\n\\n| Model Type  | Pre-trained Weights                                                                                              | Task                  |\\n|-------------|------------------------------------------------------------------------------------------------------------------|-----------------------|\\n| YOLOv8      | `yolov8n.pt`, `yolov8s.pt`, `yolov8m.pt`, `yolov8l.pt`, `yolov8x.pt`                                             | Detection             |\\n| YOLOv8-seg  | `yolov8n-seg.pt`, `yolov8s-seg.pt`, `yolov8m-seg.pt`, `yolov8l-seg.pt`, `yolov8x-seg.pt`                         | Instance Segmentation |\\n| YOLOv8-pose | `yolov8n-pose.pt`, `yolov8s-pose.pt`, `yolov8m-pose.pt`, `yolov8l-pose.pt`, `yolov8x-pose.pt` ,`yolov8x-pose-p6` | Pose/Keypoints        |\\n| YOLOv8-cls  | `yolov8n-cls.pt`, `yolov8s-cls.pt`, `yolov8m-cls.pt`, `yolov8l-cls.pt`, `yolov8x-cls.pt`                         | Classification        |\\n\\n## Supported Modes\\n\\n| Mode       | Supported          |\\n|------------|--------------------|\\n| Inference  | :heavy_check_mark: |\\n| Validation | :heavy_check_mark: |\\n| Training   | :heavy_check_mark: |\\n\\n??? Performance\\n\\n    === \"Detection\"\\n\\n        | Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n        | ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\\n        | [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640                   | 37.3                 | 80.4                           | 0.99                                | 3.2                | 8.7               |\\n        | [YOLOv8s](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt) | 640                   | 44.9                 | 128.4                          | 1.20                                | 11.2               | 28.6              |\\n        | [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640                   | 50.2                 | 234.7                          | 1.83                                | 25.9               | 78.9              |\\n        | [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640                   | 52.9                 | 375.2                          | 2.39                                | 43.7               | 165.2             |\\n        | [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640                   | 53.9                 | 479.1                          | 3.53                                | 68.2               | 257.8             |\\n\\n    === \"Segmentation\"\\n\\n        | Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n        | -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\\n        | [YOLOv8n-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt) | 640                   | 36.7                 | 30.5                  | 96.1                           | 1.21                                | 3.4                | 12.6              |\\n        | [YOLOv8s-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-seg.pt) | 640                   | 44.6                 | 36.8                  | 155.7                          | 1.47                                | 11.8               | 42.6              |\\n        | [YOLOv8m-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-seg.pt) | 640                   | 49.9                 | 40.8                  | 317.0                          | 2.18                                | 27.3               | 110.2             |\\n        | [YOLOv8l-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-seg.pt) | 640                   | 52.3                 | 42.6                  | 572.4                          | 2.79                                | 46.0               | 220.5             |\\n        | [YOLOv8x-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt) | 640                   | 53.4                 | 43.4                  | 712.1                          | 4.02                                | 71.8               | 344.1             |\\n\\n    === \"Classification\"\\n\\n        | Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 640 |\\n        | -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\\n        | [YOLOv8n-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-cls.pt) | 224                   | 66.6             | 87.0             | 12.9                           | 0.31                                | 2.7                | 4.3                      |\\n        | [YOLOv8s-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-cls.pt) | 224                   | 72.3             | 91.1             | 23.4                           | 0.35                                | 6.4                | 13.5                     |\\n        | [YOLOv8m-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-cls.pt) | 224                   | 76.4             | 93.2             | 85.4                           | 0.62                                | 17.0               | 42.7                     |\\n        | [YOLOv8l-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-cls.pt) | 224                   | 78.0             | 94.1             | 163.0                          | 0.87                                | 37.5               | 99.7                     |\\n        | [YOLOv8x-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt) | 224                   | 78.4             | 94.3             | 232.0                          | 1.01                                | 57.4               | 154.8                    |\\n\\n    === \"Pose\"\\n\\n        | Model                                                                                                | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n        | ---------------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\\n        | [YOLOv8n-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt)       | 640                   | 50.4                  | 80.1               | 131.8                          | 1.18                                | 3.3                | 9.2               |\\n        | [YOLOv8s-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt)       | 640                   | 60.0                  | 86.2               | 233.2                          | 1.42                                | 11.6               | 30.2              |\\n        | [YOLOv8m-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-pose.pt)       | 640                   | 65.0                  | 88.8               | 456.3                          | 2.00                                | 26.4               | 81.0              |\\n        | [YOLOv8l-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-pose.pt)       | 640                   | 67.6                  | 90.0               | 784.5                          | 2.59                                | 44.4               | 168.6             |\\n        | [YOLOv8x-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose.pt)       | 640                   | 69.2                  | 90.2               | 1607.1                         | 3.73                                | 69.4               | 263.2             |\\n        | [YOLOv8x-pose-p6](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose-p6.pt) | 1280                  | 71.6                  | 91.2               | 4088.7                         | 10.04                               | 99.1               | 1066.4            |\\n', metadata={'file_path': 'docs/models/yolov8.md', 'file_name': 'yolov8.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n<img width=\"1024\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\">\\n\\n**Benchmark mode** is used to profile the speed and accuracy of various export formats for YOLOv8. The benchmarks\\nprovide information on the size of the exported format, its `mAP50-95` metrics (for object detection, segmentation and pose)\\nor `accuracy_top5` metrics (for classification), and the inference time in milliseconds per image across various export\\nformats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for\\ntheir specific use case based on their requirements for speed and accuracy.\\n\\n!!! tip \"Tip\"\\n\\n    * Export to ONNX or OpenVINO for up to 3x CPU speedup.\\n    * Export to TensorRT for up to 5x GPU speedup.\\n\\n## Usage Examples\\n\\nRun YOLOv8n benchmarks on all supported export formats including ONNX, TensorRT etc. See Arguments section below for a\\nfull list of export arguments.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics.yolo.utils.benchmarks import benchmark\\n        \\n        # Benchmark on GPU\\n        benchmark(model=\\'yolov8n.pt\\', imgsz=640, half=False, device=0)\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo benchmark model=yolov8n.pt imgsz=640 half=False device=0\\n        ```\\n\\n## Arguments\\n\\nArguments such as `model`, `imgsz`, `half`, `device`, and `hard_fail` provide users with the flexibility to fine-tune\\nthe benchmarks to their specific needs and compare the performance of different export formats with ease.\\n\\n| Key         | Value   | Description                                                          |\\n|-------------|---------|----------------------------------------------------------------------|\\n| `model`     | `None`  | path to model file, i.e. yolov8n.pt, yolov8n.yaml                    |\\n| `imgsz`     | `640`   | image size as scalar or (h, w) list, i.e. (640, 480)                 |\\n| `half`      | `False` | FP16 quantization                                                    |\\n| `int8`      | `False` | INT8 quantization                                                    |\\n| `device`    | `None`  | device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu |\\n| `hard_fail` | `False` | do not continue on error (bool), or val floor threshold (float)      |\\n\\n## Export Formats\\n\\nBenchmarks will attempt to run automatically on all possible export formats below.\\n\\n| Format                                                             | `format` Argument | Model                     | Metadata |\\n|--------------------------------------------------------------------|-------------------|---------------------------|----------|\\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | ✅        |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | ✅        |\\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | ✅        |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | ✅        |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | ✅        |\\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | ✅        |\\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | ✅        |\\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | ❌        |\\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | ✅        |\\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ✅        |\\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | ✅        |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | ✅        |\\n\\nSee full `export` details in the [Export](https://docs.ultralytics.com/modes/export/) page.', metadata={'file_path': 'docs/modes/benchmark.md', 'file_name': 'benchmark.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n<img width=\"1024\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\">\\n\\n**Export mode** is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the\\nmodel is converted to a format that can be used by other software applications or hardware devices. This mode is useful\\nwhen deploying the model to production environments.\\n\\n!!! tip \"Tip\"\\n\\n    * Export to ONNX or OpenVINO for up to 3x CPU speedup.\\n    * Export to TensorRT for up to 5x GPU speedup.\\n\\n## Usage Examples\\n\\nExport a YOLOv8n model to a different format like ONNX or TensorRT. See Arguments section below for a full list of\\nexport arguments.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom trained\\n        \\n        # Export the model\\n        model.export(format=\\'onnx\\')\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo export model=yolov8n.pt format=onnx  # export official model\\n        yolo export model=path/to/best.pt format=onnx  # export custom trained model\\n        ```\\n\\n## Arguments\\n\\nExport settings for YOLO models refer to the various configurations and options used to save or\\nexport the model for use in other environments or platforms. These settings can affect the model\\'s performance, size,\\nand compatibility with different systems. Some common YOLO export settings include the format of the exported model\\nfile (e.g. ONNX, TensorFlow SavedModel), the device on which the model will be run (e.g. CPU, GPU), and the presence of\\nadditional features such as masks or multiple labels per box. Other factors that may affect the export process include\\nthe specific task the model is being used for and the requirements or constraints of the target environment or platform.\\nIt is important to carefully consider and configure these settings to ensure that the exported model is optimized for\\nthe intended use case and can be used effectively in the target environment.\\n\\n| Key         | Value           | Description                                          |\\n|-------------|-----------------|------------------------------------------------------|\\n| `format`    | `\\'torchscript\\'` | format to export to                                  |\\n| `imgsz`     | `640`           | image size as scalar or (h, w) list, i.e. (640, 480) |\\n| `keras`     | `False`         | use Keras for TF SavedModel export                   |\\n| `optimize`  | `False`         | TorchScript: optimize for mobile                     |\\n| `half`      | `False`         | FP16 quantization                                    |\\n| `int8`      | `False`         | INT8 quantization                                    |\\n| `dynamic`   | `False`         | ONNX/TF/TensorRT: dynamic axes                       |\\n| `simplify`  | `False`         | ONNX: simplify model                                 |\\n| `opset`     | `None`          | ONNX: opset version (optional, defaults to latest)   |\\n| `workspace` | `4`             | TensorRT: workspace size (GB)                        |\\n| `nms`       | `False`         | CoreML: add NMS                                      |\\n\\n## Export Formats\\n\\nAvailable YOLOv8 export formats are in the table below. You can export to any format using the `format` argument,\\ni.e. `format=\\'onnx\\'` or `format=\\'engine\\'`.\\n\\n| Format                                                             | `format` Argument | Model                     | Metadata |\\n|--------------------------------------------------------------------|-------------------|---------------------------|----------|\\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | ✅        |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | ✅        |\\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | ✅        |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | ✅        |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | ✅        |\\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | ✅        |\\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | ✅        |\\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | ❌        |\\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | ✅        |\\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ✅        |\\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | ✅        |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | ✅        |\\n', metadata={'file_path': 'docs/modes/export.md', 'file_name': 'export.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Ultralytics YOLOv8 Modes\\n\\n<img width=\"1024\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\">\\n\\nUltralytics YOLOv8 supports several **modes** that can be used to perform different tasks. These modes are:\\n\\n**Train**: For training a YOLOv8 model on a custom dataset.  \\n**Val**: For validating a YOLOv8 model after it has been trained.  \\n**Predict**: For making predictions using a trained YOLOv8 model on new images or videos.  \\n**Export**: For exporting a YOLOv8 model to a format that can be used for deployment.  \\n**Track**: For tracking objects in real-time using a YOLOv8 model.  \\n**Benchmark**: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.\\n\\n## [Train](train.md)\\n\\nTrain mode is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the\\nspecified dataset and hyperparameters. The training process involves optimizing the model\\'s parameters so that it can\\naccurately predict the classes and locations of objects in an image.\\n\\n[Train Examples](train.md){ .md-button .md-button--primary}\\n\\n## [Val](val.md)\\n\\nVal mode is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a\\nvalidation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters\\nof the model to improve its performance.\\n\\n[Val Examples](val.md){ .md-button .md-button--primary}\\n\\n## [Predict](predict.md)\\n\\nPredict mode is used for making predictions using a trained YOLOv8 model on new images or videos. In this mode, the\\nmodel is loaded from a checkpoint file, and the user can provide images or videos to perform inference. The model\\npredicts the classes and locations of objects in the input images or videos.\\n\\n[Predict Examples](predict.md){ .md-button .md-button--primary}\\n\\n## [Export](export.md)\\n\\nExport mode is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the model is\\nconverted to a format that can be used by other software applications or hardware devices. This mode is useful when\\ndeploying the model to production environments.\\n\\n[Export Examples](export.md){ .md-button .md-button--primary}\\n\\n## [Track](track.md)\\n\\nTrack mode is used for tracking objects in real-time using a YOLOv8 model. In this mode, the model is loaded from a\\ncheckpoint file, and the user can provide a live video stream to perform real-time object tracking. This mode is useful\\nfor applications such as surveillance systems or self-driving cars.\\n\\n[Track Examples](track.md){ .md-button .md-button--primary}\\n\\n## [Benchmark](benchmark.md)\\n\\nBenchmark mode is used to profile the speed and accuracy of various export formats for YOLOv8. The benchmarks provide\\ninformation on the size of the exported format, its `mAP50-95` metrics (for object detection, segmentation and pose)\\nor `accuracy_top5` metrics (for classification), and the inference time in milliseconds per image across various export\\nformats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for\\ntheir specific use case based on their requirements for speed and accuracy.\\n\\n[Benchmark Examples](benchmark.md){ .md-button .md-button--primary}\\n', metadata={'file_path': 'docs/modes/index.md', 'file_name': 'index.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n<img width=\"1024\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\">\\n\\nYOLOv8 **predict mode** can generate predictions for various tasks, returning either a list of `Results` objects or a\\nmemory-efficient generator of `Results` objects when using the streaming mode. Enable streaming mode by\\npassing `stream=True` in the predictor\\'s call method.\\n\\n!!! example \"Predict\"\\n\\n    === \"Return a list with `Stream=False`\"\\n        ```python\\n        inputs = [img, img]  # list of numpy arrays\\n        results = model(inputs)  # list of Results objects\\n        \\n        for result in results:\\n            boxes = result.boxes  # Boxes object for bbox outputs\\n            masks = result.masks  # Masks object for segmentation masks outputs\\n            probs = result.probs  # Class probabilities for classification outputs\\n        ```\\n\\n    === \"Return a generator with `Stream=True`\"\\n        ```python\\n        inputs = [img, img]  # list of numpy arrays\\n        results = model(inputs, stream=True)  # generator of Results objects\\n        \\n        for result in results:\\n            boxes = result.boxes  # Boxes object for bbox outputs\\n            masks = result.masks  # Masks object for segmentation masks outputs\\n            probs = result.probs  # Class probabilities for classification outputs\\n        ```\\n\\n!!! tip \"Tip\"\\n\\n    Streaming mode with `stream=True` should be used for long videos or large predict sources, otherwise results will accumuate in memory and will eventually cause out-of-memory errors. \\n\\n## Sources\\n\\nYOLOv8 can accept various input sources, as shown in the table below. This includes images, URLs, PIL images, OpenCV,\\nnumpy arrays, torch tensors, CSV files, videos, directories, globs, YouTube videos, and streams. The table indicates\\nwhether each source can be used in streaming mode with `stream=True` ✅ and an example argument for each source.\\n\\n| source      | model(arg)                                 | type           | notes            |\\n|-------------|--------------------------------------------|----------------|------------------|\\n| image       | `\\'im.jpg\\'`                                 | `str`, `Path`  |                  |\\n| URL         | `\\'https://ultralytics.com/images/bus.jpg\\'` | `str`          |                  |\\n| screenshot  | `\\'screen\\'`                                 | `str`          |                  |\\n| PIL         | `Image.open(\\'im.jpg\\')`                     | `PIL.Image`    | HWC, RGB         |\\n| OpenCV      | `cv2.imread(\\'im.jpg\\')[:,:,::-1]`           | `np.ndarray`   | HWC, BGR to RGB  |\\n| numpy       | `np.zeros((640,1280,3))`                   | `np.ndarray`   | HWC              |\\n| torch       | `torch.zeros(16,3,320,640)`                | `torch.Tensor` | BCHW, RGB        |\\n| CSV         | `\\'sources.csv\\'`                            | `str`, `Path`  | RTSP, RTMP, HTTP |         \\n| video ✅     | `\\'vid.mp4\\'`                                | `str`, `Path`  |                  |\\n| directory ✅ | `\\'path/\\'`                                  | `str`, `Path`  |                  |\\n| glob ✅      | `\\'path/*.jpg\\'`                             | `str`          | Use `*` operator |\\n| YouTube ✅   | `\\'https://youtu.be/Zgi9g1ksQHc\\'`           | `str`          |                  |\\n| stream ✅    | `\\'rtsp://example.com/media.mp4\\'`           | `str`          | RTSP, RTMP, HTTP |\\n\\n\\n## Arguments\\n`model.predict` accepts multiple arguments that control the prediction operation. These arguments can be passed directly to `model.predict`:\\n!!! example\\n    ```\\n    model.predict(source, save=True, imgsz=320, conf=0.5)\\n    ```\\n\\nAll supported arguments:\\n\\n| Key              | Value                  | Description                                              |\\n|------------------|------------------------|----------------------------------------------------------|\\n| `source`         | `\\'ultralytics/assets\\'` | source directory for images or videos                    |\\n| `conf`           | `0.25`                 | object confidence threshold for detection                |\\n| `iou`            | `0.7`                  | intersection over union (IoU) threshold for NMS          |\\n| `half`           | `False`                | use half precision (FP16)                                |\\n| `device`         | `None`                 | device to run on, i.e. cuda device=0/1/2/3 or device=cpu |\\n| `show`           | `False`                | show results if possible                                 |\\n| `save`           | `False`                | save images with results                                 |\\n| `save_txt`       | `False`                | save results as .txt file                                |\\n| `save_conf`      | `False`                | save results with confidence scores                      |\\n| `save_crop`      | `False`                | save cropped images with results                         |\\n| `hide_labels`    | `False`                | hide labels                                              |\\n| `hide_conf`      | `False`                | hide confidence scores                                   |\\n| `max_det`        | `300`                  | maximum number of detections per image                   |\\n| `vid_stride`     | `False`                | video frame-rate stride                                  |\\n| `line_thickness` | `3`                    | bounding box thickness (pixels)                          |\\n| `visualize`      | `False`                | visualize model features                                 |\\n| `augment`        | `False`                | apply image augmentation to prediction sources           |\\n| `agnostic_nms`   | `False`                | class-agnostic NMS                                       |\\n| `retina_masks`   | `False`                | use high-resolution segmentation masks                   |\\n| `classes`        | `None`                 | filter results by class, i.e. class=0, or class=[0,2,3]  |\\n| `boxes`          | `True`                 | Show boxes in segmentation predictions                   |\\n\\n## Image and Video Formats\\n\\nYOLOv8 supports various image and video formats, as specified\\nin [yolo/data/utils.py](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/data/utils.py). See the\\ntables below for the valid suffixes and example predict commands.\\n\\n### Image Suffixes\\n\\n| Image Suffixes | Example Predict Command          | Reference                                                                     |\\n|----------------|----------------------------------|-------------------------------------------------------------------------------|\\n| .bmp           | `yolo predict source=image.bmp`  | [Microsoft BMP File Format](https://en.wikipedia.org/wiki/BMP_file_format)    |\\n| .dng           | `yolo predict source=image.dng`  | [Adobe DNG](https://www.adobe.com/products/photoshop/extend.displayTab2.html) |\\n| .jpeg          | `yolo predict source=image.jpeg` | [JPEG](https://en.wikipedia.org/wiki/JPEG)                                    |\\n| .jpg           | `yolo predict source=image.jpg`  | [JPEG](https://en.wikipedia.org/wiki/JPEG)                                    |\\n| .mpo           | `yolo predict source=image.mpo`  | [Multi Picture Object](https://fileinfo.com/extension/mpo)                    |\\n| .png           | `yolo predict source=image.png`  | [Portable Network Graphics](https://en.wikipedia.org/wiki/PNG)                |\\n| .tif           | `yolo predict source=image.tif`  | [Tag Image File Format](https://en.wikipedia.org/wiki/TIFF)                   |\\n| .tiff          | `yolo predict source=image.tiff` | [Tag Image File Format](https://en.wikipedia.org/wiki/TIFF)                   |\\n| .webp          | `yolo predict source=image.webp` | [WebP](https://en.wikipedia.org/wiki/WebP)                                    |\\n| .pfm           | `yolo predict source=image.pfm`  | [Portable FloatMap](https://en.wikipedia.org/wiki/Netpbm#File_formats)        |\\n\\n### Video Suffixes\\n\\n| Video Suffixes | Example Predict Command          | Reference                                                                        |\\n|----------------|----------------------------------|----------------------------------------------------------------------------------|\\n| .asf           | `yolo predict source=video.asf`  | [Advanced Systems Format](https://en.wikipedia.org/wiki/Advanced_Systems_Format) |\\n| .avi           | `yolo predict source=video.avi`  | [Audio Video Interleave](https://en.wikipedia.org/wiki/Audio_Video_Interleave)   |\\n| .gif           | `yolo predict source=video.gif`  | [Graphics Interchange Format](https://en.wikipedia.org/wiki/GIF)                 |\\n| .m4v           | `yolo predict source=video.m4v`  | [MPEG-4 Part 14](https://en.wikipedia.org/wiki/M4V)                              |\\n| .mkv           | `yolo predict source=video.mkv`  | [Matroska](https://en.wikipedia.org/wiki/Matroska)                               |\\n| .mov           | `yolo predict source=video.mov`  | [QuickTime File Format](https://en.wikipedia.org/wiki/QuickTime_File_Format)     |\\n| .mp4           | `yolo predict source=video.mp4`  | [MPEG-4 Part 14 - Wikipedia](https://en.wikipedia.org/wiki/MPEG-4_Part_14)       |\\n| .mpeg          | `yolo predict source=video.mpeg` | [MPEG-1 Part 2](https://en.wikipedia.org/wiki/MPEG-1)                            |\\n| .mpg           | `yolo predict source=video.mpg`  | [MPEG-1 Part 2](https://en.wikipedia.org/wiki/MPEG-1)                            |\\n| .ts            | `yolo predict source=video.ts`   | [MPEG Transport Stream](https://en.wikipedia.org/wiki/MPEG_transport_stream)     |\\n| .wmv           | `yolo predict source=video.wmv`  | [Windows Media Video](https://en.wikipedia.org/wiki/Windows_Media_Video)         |\\n| .webm          | `yolo predict source=video.webm` | [WebM Project](https://en.wikipedia.org/wiki/WebM)                               |\\n\\n## Working with Results\\n\\nThe `Results` object contains the following components:\\n\\n- `Results.boxes`: `Boxes` object with properties and methods for manipulating bounding boxes\\n- `Results.masks`: `Masks` object for indexing masks or getting segment coordinates\\n- `Results.probs`: `torch.Tensor` containing class probabilities or logits\\n- `Results.orig_img`: Original image loaded in memory\\n- `Results.path`: `Path` containing the path to the input image\\n\\nEach result is composed of a `torch.Tensor` by default, which allows for easy manipulation:\\n\\n!!! example \"Results\"\\n\\n    ```python\\n    results = results.cuda()\\n    results = results.cpu()\\n    results = results.to(\\'cpu\\')\\n    results = results.numpy()\\n    ```\\n\\n### Boxes\\n\\n`Boxes` object can be used to index, manipulate, and convert bounding boxes to different formats. Box format conversion\\noperations are cached, meaning they\\'re only calculated once per object, and those values are reused for future calls.\\n\\n- Indexing a `Boxes` object returns a `Boxes` object:\\n\\n!!! example \"Boxes\"\\n\\n    ```python\\n    results = model(img)\\n    boxes = results[0].boxes\\n    box = boxes[0]  # returns one box\\n    box.xyxy\\n    ```\\n\\n- Properties and conversions\\n\\n!!! example \"Boxes Properties\"\\n\\n    ```python\\n    boxes.xyxy  # box with xyxy format, (N, 4)\\n    boxes.xywh  # box with xywh format, (N, 4)\\n    boxes.xyxyn  # box with xyxy format but normalized, (N, 4)\\n    boxes.xywhn  # box with xywh format but normalized, (N, 4)\\n    boxes.conf  # confidence score, (N, 1)\\n    boxes.cls  # cls, (N, 1)\\n    boxes.data  # raw bboxes tensor, (N, 6) or boxes.boxes\\n    ```\\n\\n### Masks\\n\\n`Masks` object can be used index, manipulate and convert masks to segments. The segment conversion operation is cached.\\n\\n!!! example \"Masks\"\\n\\n    ```python\\n    results = model(inputs)\\n    masks = results[0].masks  # Masks object\\n    masks.xy  # x, y segments (pixels), List[segment] * N\\n    masks.xyn  # x, y segments (normalized), List[segment] * N\\n    masks.data  # raw masks tensor, (N, H, W) or masks.masks \\n    ```\\n\\n### probs\\n\\n`probs` attribute of `Results` class is a `Tensor` containing class probabilities of a classification operation.\\n\\n!!! example \"Probs\"\\n\\n    ```python\\n    results = model(inputs)\\n    results[0].probs  # cls prob, (num_class, )\\n    ```\\n\\nClass reference documentation for `Results` module and its components can be found [here](../reference/yolo/engine/results.md)\\n\\n## Plotting results\\n\\nYou can use `plot()` function of `Result` object to plot results on in image object. It plots all components(boxes,\\nmasks, classification logits, etc.) found in the results object\\n\\n!!! example \"Plotting\"\\n\\n    ```python\\n    res = model(img)\\n    res_plotted = res[0].plot()\\n    cv2.imshow(\"result\", res_plotted)\\n    ```\\n| Argument                       | Description                                                                            |\\n|--------------------------------|----------------------------------------------------------------------------------------|\\n| `conf (bool)`                  | Whether to plot the detection confidence score.                                        |\\n| `line_width (float, optional)` | The line width of the bounding boxes. If None, it is scaled to the image size.         |\\n| `font_size (float, optional)`  | The font size of the text. If None, it is scaled to the image size.                    |\\n| `font (str)`                   | The font to use for the text.                                                          |\\n| `pil (bool)`                   | Whether to use PIL for image plotting.                                                 |\\n| `example (str)`                | An example string to display. Useful for indicating the expected format of the output. |\\n| `img (numpy.ndarray)`          | Plot to another image. if not, plot to original image.                                 |\\n| `labels (bool)`                | Whether to plot the label of bounding boxes.                                           |\\n| `boxes (bool)`                 | Whether to plot the bounding boxes.                                                    |\\n| `masks (bool)`                 | Whether to plot the masks.                                                             |\\n| `probs (bool)`                 | Whether to plot classification probability.                                            |\\n\\n\\n## Streaming Source `for`-loop\\n\\nHere\\'s a Python script using OpenCV (cv2) and YOLOv8 to run inference on video frames. This script assumes you have already installed the necessary packages (opencv-python and ultralytics).\\n\\n!!! example \"Streaming for-loop\"\\n\\n    ```python\\n    import cv2\\n    from ultralytics import YOLO\\n    \\n    # Load the YOLOv8 model\\n    model = YOLO(\\'yolov8n.pt\\')\\n    \\n    # Open the video file\\n    video_path = \"path/to/your/video/file.mp4\"\\n    cap = cv2.VideoCapture(video_path)\\n    \\n    # Loop through the video frames\\n    while cap.isOpened():\\n        # Read a frame from the video\\n        success, frame = cap.read()\\n    \\n        if success:\\n            # Run YOLOv8 inference on the frame\\n            results = model(frame)\\n    \\n            # Visualize the results on the frame\\n            annotated_frame = results[0].plot()\\n    \\n            # Display the annotated frame\\n            cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\\n    \\n            # Break the loop if \\'q\\' is pressed\\n            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\\n                break\\n        else:\\n            # Break the loop if the end of the video is reached\\n            break\\n    \\n    # Release the video capture object and close the display window\\n    cap.release()\\n    cv2.destroyAllWindows()\\n    ```\\n', metadata={'file_path': 'docs/modes/predict.md', 'file_name': 'predict.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n<img width=\"1024\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\">\\n\\nObject tracking is a task that involves identifying the location and class of objects, then assigning a unique ID to\\nthat detection in video streams.\\n\\nThe output of tracker is the same as detection with an added object ID.\\n\\n## Available Trackers\\n\\nThe following tracking algorithms have been implemented and can be enabled by passing `tracker=tracker_type.yaml`\\n\\n* [BoT-SORT](https://github.com/NirAharon/BoT-SORT) - `botsort.yaml`\\n* [ByteTrack](https://github.com/ifzhang/ByteTrack) - `bytetrack.yaml`\\n\\nThe default tracker is BoT-SORT.\\n\\n## Tracking\\n\\nUse a trained YOLOv8n/YOLOv8n-seg model to run tracker on video streams.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.pt\\')  # load an official detection model\\n        model = YOLO(\\'yolov8n-seg.pt\\')  # load an official segmentation model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Track with the model\\n        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True) \\n        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True, tracker=\"bytetrack.yaml\") \\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo track model=yolov8n.pt source=\"https://youtu.be/Zgi9g1ksQHc\"  # official detection model\\n        yolo track model=yolov8n-seg.pt source=...   # official segmentation model\\n        yolo track model=path/to/best.pt source=...  # custom model\\n        yolo track model=path/to/best.pt  tracker=\"bytetrack.yaml\" # bytetrack tracker\\n\\n        ```\\n\\nAs in the above usage, we support both the detection and segmentation models for tracking and the only thing you need to\\ndo is loading the corresponding (detection or segmentation) model.\\n\\n## Configuration\\n\\n### Tracking\\n\\nTracking shares the configuration with predict, i.e `conf`, `iou`, `show`. More configurations please refer\\nto [predict page](https://docs.ultralytics.com/modes/predict/).\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        model = YOLO(\\'yolov8n.pt\\')\\n        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", conf=0.3, iou=0.5, show=True) \\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo track model=yolov8n.pt source=\"https://youtu.be/Zgi9g1ksQHc\" conf=0.3, iou=0.5 show\\n\\n        ```\\n\\n### Tracker\\n\\nWe also support using a modified tracker config file, just copy a config file i.e `custom_tracker.yaml`\\nfrom [ultralytics/tracker/cfg](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/tracker/cfg) and modify\\nany configurations(expect the `tracker_type`) you need to.\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        model = YOLO(\\'yolov8n.pt\\')\\n        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", tracker=\\'custom_tracker.yaml\\') \\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo track model=yolov8n.pt source=\"https://youtu.be/Zgi9g1ksQHc\" tracker=\\'custom_tracker.yaml\\'\\n        ```\\n\\nPlease refer to [ultralytics/tracker/cfg](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/tracker/cfg)\\npage\\n\\n', metadata={'file_path': 'docs/modes/track.md', 'file_name': 'track.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n---\\ncomments: true\\n---\\n\\n<img width=\"1024\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\">\\n\\n**Train mode** is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the\\nspecified dataset and hyperparameters. The training process involves optimizing the model\\'s parameters so that it can\\naccurately predict the classes and locations of objects in an image.\\n\\n!!! tip \"Tip\"\\n\\n    * YOLOv8 datasets like COCO, VOC, ImageNet and many others automatically download on first use, i.e. `yolo train data=coco.yaml`\\n\\n## Usage Examples\\n\\nTrain YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. See Arguments section below for a full list of\\ntraining arguments.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.yaml\\')  # build a new model from YAML\\n        model = YOLO(\\'yolov8n.pt\\')  # load a pretrained model (recommended for training)\\n        model = YOLO(\\'yolov8n.yaml\\').load(\\'yolov8n.pt\\')  # build from YAML and transfer weights\\n        \\n        # Train the model\\n        model.train(data=\\'coco128.yaml\\', epochs=100, imgsz=640)\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        # Build a new model from YAML and start training from scratch\\n        yolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\\n\\n        # Start training from a pretrained *.pt model\\n        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\\n\\n        # Build a new model from YAML, transfer pretrained weights to it and start training\\n        yolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\\n        ```\\n\\n## Arguments\\n\\nTraining settings for YOLO models refer to the various hyperparameters and configurations used to train the model on a\\ndataset. These settings can affect the model\\'s performance, speed, and accuracy. Some common YOLO training settings\\ninclude the batch size, learning rate, momentum, and weight decay. Other factors that may affect the training process\\ninclude the choice of optimizer, the choice of loss function, and the size and composition of the training dataset. It\\nis important to carefully tune and experiment with these settings to achieve the best possible performance for a given\\ntask.\\n\\n| Key               | Value    | Description                                                                 |\\n|-------------------|----------|-----------------------------------------------------------------------------|\\n| `model`           | `None`   | path to model file, i.e. yolov8n.pt, yolov8n.yaml                           |\\n| `data`            | `None`   | path to data file, i.e. coco128.yaml                                        |\\n| `epochs`          | `100`    | number of epochs to train for                                               |\\n| `patience`        | `50`     | epochs to wait for no observable improvement for early stopping of training |\\n| `batch`           | `16`     | number of images per batch (-1 for AutoBatch)                               |\\n| `imgsz`           | `640`    | size of input images as integer or w,h                                      |\\n| `save`            | `True`   | save train checkpoints and predict results                                  |\\n| `save_period`     | `-1`     | Save checkpoint every x epochs (disabled if < 1)                            |\\n| `cache`           | `False`  | True/ram, disk or False. Use cache for data loading                         |\\n| `device`          | `None`   | device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu        |\\n| `workers`         | `8`      | number of worker threads for data loading (per RANK if DDP)                 |\\n| `project`         | `None`   | project name                                                                |\\n| `name`            | `None`   | experiment name                                                             |\\n| `exist_ok`        | `False`  | whether to overwrite existing experiment                                    |\\n| `pretrained`      | `False`  | whether to use a pretrained model                                           |\\n| `optimizer`       | `\\'SGD\\'`  | optimizer to use, choices=[\\'SGD\\', \\'Adam\\', \\'AdamW\\', \\'RMSProp\\']               |\\n| `verbose`         | `False`  | whether to print verbose output                                             |\\n| `seed`            | `0`      | random seed for reproducibility                                             |\\n| `deterministic`   | `True`   | whether to enable deterministic mode                                        |\\n| `single_cls`      | `False`  | train multi-class data as single-class                                      |\\n| `rect`            | `False`  | rectangular training with each batch collated for minimum padding           |\\n| `cos_lr`          | `False`  | use cosine learning rate scheduler                                          |\\n| `close_mosaic`    | `0`      | (int) disable mosaic augmentation for final epochs                          |\\n| `resume`          | `False`  | resume training from last checkpoint                                        |\\n| `amp`             | `True`   | Automatic Mixed Precision (AMP) training, choices=[True, False]             |\\n| `lr0`             | `0.01`   | initial learning rate (i.e. SGD=1E-2, Adam=1E-3)                            |\\n| `lrf`             | `0.01`   | final learning rate (lr0 * lrf)                                             |\\n| `momentum`        | `0.937`  | SGD momentum/Adam beta1                                                     |\\n| `weight_decay`    | `0.0005` | optimizer weight decay 5e-4                                                 |\\n| `warmup_epochs`   | `3.0`    | warmup epochs (fractions ok)                                                |\\n| `warmup_momentum` | `0.8`    | warmup initial momentum                                                     |\\n| `warmup_bias_lr`  | `0.1`    | warmup initial bias lr                                                      |\\n| `box`             | `7.5`    | box loss gain                                                               |\\n| `cls`             | `0.5`    | cls loss gain (scale with pixels)                                           |\\n| `dfl`             | `1.5`    | dfl loss gain                                                               |\\n| `pose`            | `12.0`   | pose loss gain (pose-only)                                                  |\\n| `kobj`            | `2.0`    | keypoint obj loss gain (pose-only)                                          |\\n| `label_smoothing` | `0.0`    | label smoothing (fraction)                                                  |\\n| `nbs`             | `64`     | nominal batch size                                                          |\\n| `overlap_mask`    | `True`   | masks should overlap during training (segment train only)                   |\\n| `mask_ratio`      | `4`      | mask downsample ratio (segment train only)                                  |\\n| `dropout`         | `0.0`    | use dropout regularization (classify train only)                            |\\n| `val`             | `True`   | validate/test during training                                               |\\n', metadata={'file_path': 'docs/modes/train.md', 'file_name': 'train.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n<img width=\"1024\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\">\\n\\n**Val mode** is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a\\nvalidation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters\\nof the model to improve its performance.\\n\\n!!! tip \"Tip\"\\n\\n    * YOLOv8 models automatically remember their training settings, so you can validate a model at the same image size and on the original dataset easily with just `yolo val model=yolov8n.pt` or `model(\\'yolov8n.pt\\').val()`\\n\\n## Usage Examples\\n\\nValidate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the `model` retains it\\'s\\ntraining `data` and arguments as model attributes. See Arguments section below for a full list of export arguments.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Validate the model\\n        metrics = model.val()  # no arguments needed, dataset and settings remembered\\n        metrics.box.map    # map50-95\\n        metrics.box.map50  # map50\\n        metrics.box.map75  # map75\\n        metrics.box.maps   # a list contains map50-95 of each category\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo detect val model=yolov8n.pt  # val official model\\n        yolo detect val model=path/to/best.pt  # val custom model\\n        ```\\n\\n## Arguments\\n\\nValidation settings for YOLO models refer to the various hyperparameters and configurations used to\\nevaluate the model\\'s performance on a validation dataset. These settings can affect the model\\'s performance, speed, and\\naccuracy. Some common YOLO validation settings include the batch size, the frequency with which validation is performed\\nduring training, and the metrics used to evaluate the model\\'s performance. Other factors that may affect the validation\\nprocess include the size and composition of the validation dataset and the specific task the model is being used for. It\\nis important to carefully tune and experiment with these settings to ensure that the model is performing well on the\\nvalidation dataset and to detect and prevent overfitting.\\n\\n| Key           | Value   | Description                                                        |\\n|---------------|---------|--------------------------------------------------------------------|\\n| `data`        | `None`  | path to data file, i.e. coco128.yaml                               |\\n| `imgsz`       | `640`   | image size as scalar or (h, w) list, i.e. (640, 480)               |\\n| `batch`       | `16`    | number of images per batch (-1 for AutoBatch)                      |\\n| `save_json`   | `False` | save results to JSON file                                          |\\n| `save_hybrid` | `False` | save hybrid version of labels (labels + additional predictions)    |\\n| `conf`        | `0.001` | object confidence threshold for detection                          |\\n| `iou`         | `0.6`   | intersection over union (IoU) threshold for NMS                    |\\n| `max_det`     | `300`   | maximum number of detections per image                             |\\n| `half`        | `True`  | use half precision (FP16)                                          |\\n| `device`      | `None`  | device to run on, i.e. cuda device=0/1/2/3 or device=cpu           |\\n| `dnn`         | `False` | use OpenCV DNN for ONNX inference                                  |\\n| `plots`       | `False` | show plots during training                                         |\\n| `rect`        | `False` | rectangular val with each batch collated for minimum padding       |\\n| `split`       | `val`   | dataset split to use for validation, i.e. \\'val\\', \\'test\\' or \\'train\\' |\\n\\n## Export Formats\\n\\nAvailable YOLOv8 export formats are in the table below. You can export to any format using the `format` argument,\\ni.e. `format=\\'onnx\\'` or `format=\\'engine\\'`.\\n\\n| Format                                                             | `format` Argument | Model                     | Metadata |\\n|--------------------------------------------------------------------|-------------------|---------------------------|----------|\\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | ✅        |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | ✅        |\\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | ✅        |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | ✅        |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | ✅        |\\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | ✅        |\\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | ✅        |\\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | ❌        |\\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | ✅        |\\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ✅        |\\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | ✅        |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | ✅        |\\n', metadata={'file_path': 'docs/modes/val.md', 'file_name': 'val.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\nImage classification is the simplest of the three tasks and involves classifying an entire image into one of a set of\\npredefined classes.\\n\\n<img width=\"1024\" src=\"https://user-images.githubusercontent.com/26833433/212094133-6bb8c21c-3d47-41df-a512-81c5931054ae.png\">\\n\\nThe output of an image classifier is a single class label and a confidence score. Image\\nclassification is useful when you need to know only what class an image belongs to and don\\'t need to know where objects\\nof that class are located or what their exact shape is.\\n\\n!!! tip \"Tip\"\\n\\n    YOLOv8 Classify models use the `-cls` suffix, i.e. `yolov8n-cls.pt` and are pretrained on [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml).\\n\\n## [Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models/v8)\\n\\nYOLOv8 pretrained Classify models are shown here. Detect, Segment and Pose models are pretrained on\\nthe [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) dataset, while Classify\\nmodels are pretrained on\\nthe [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) dataset.\\n\\n[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) download automatically from the latest\\nUltralytics [release](https://github.com/ultralytics/assets/releases) on first use.\\n\\n| Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 640 |\\n|----------------------------------------------------------------------------------------------|-----------------------|------------------|------------------|--------------------------------|-------------------------------------|--------------------|--------------------------|\\n| [YOLOv8n-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-cls.pt) | 224                   | 66.6             | 87.0             | 12.9                           | 0.31                                | 2.7                | 4.3                      |\\n| [YOLOv8s-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-cls.pt) | 224                   | 72.3             | 91.1             | 23.4                           | 0.35                                | 6.4                | 13.5                     |\\n| [YOLOv8m-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-cls.pt) | 224                   | 76.4             | 93.2             | 85.4                           | 0.62                                | 17.0               | 42.7                     |\\n| [YOLOv8l-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-cls.pt) | 224                   | 78.0             | 94.1             | 163.0                          | 0.87                                | 37.5               | 99.7                     |\\n| [YOLOv8x-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt) | 224                   | 78.4             | 94.3             | 232.0                          | 1.01                                | 57.4               | 154.8                    |\\n\\n- **acc** values are model accuracies on the [ImageNet](https://www.image-net.org/) dataset validation set.\\n  <br>Reproduce by `yolo val classify data=path/to/ImageNet device=0`\\n- **Speed** averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/)\\n  instance.\\n  <br>Reproduce by `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`\\n\\n## Train\\n\\nTrain YOLOv8n-cls on the MNIST160 dataset for 100 epochs at image size 64. For a full list of available arguments\\nsee the [Configuration](../usage/cfg.md) page.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-cls.yaml\\')  # build a new model from YAML\\n        model = YOLO(\\'yolov8n-cls.pt\\')  # load a pretrained model (recommended for training)\\n        model = YOLO(\\'yolov8n-cls.yaml\\').load(\\'yolov8n-cls.pt\\')  # build from YAML and transfer weights\\n        \\n        # Train the model\\n        model.train(data=\\'mnist160\\', epochs=100, imgsz=64)\\n        ```\\n\\n    === \"CLI\"\\n\\n        ```bash\\n        # Build a new model from YAML and start training from scratch\\n        yolo classify train data=mnist160 model=yolov8n-cls.yaml epochs=100 imgsz=64\\n\\n        # Start training from a pretrained *.pt model\\n        yolo classify train data=mnist160 model=yolov8n-cls.pt epochs=100 imgsz=64\\n\\n        # Build a new model from YAML, transfer pretrained weights to it and start training\\n        yolo classify train data=mnist160 model=yolov8n-cls.yaml pretrained=yolov8n-cls.pt epochs=100 imgsz=64\\n        ```\\n\\n### Dataset format\\nThe YOLO classification dataset format is same as the torchvision format. Each class of images has its own folder and you have to simply pass the path of the dataset folder, i.e, `yolo classify train data=\"path/to/dataset\"`\\n```\\ndataset/\\n├── class1/\\n├── class2/\\n├── class3/\\n├── ...\\n```\\n## Val\\n\\nValidate trained YOLOv8n-cls model accuracy on the MNIST160 dataset. No argument need to passed as the `model` retains\\nit\\'s training `data` and arguments as model attributes.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-cls.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Validate the model\\n        metrics = model.val()  # no arguments needed, dataset and settings remembered\\n        metrics.top1   # top1 accuracy\\n        metrics.top5   # top5 accuracy\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo classify val model=yolov8n-cls.pt  # val official model\\n        yolo classify val model=path/to/best.pt  # val custom model\\n        ```\\n\\n## Predict\\n\\nUse a trained YOLOv8n-cls model to run predictions on images.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-cls.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Predict with the model\\n        results = model(\\'https://ultralytics.com/images/bus.jpg\\')  # predict on an image\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo classify predict model=yolov8n-cls.pt source=\\'https://ultralytics.com/images/bus.jpg\\'  # predict with official model\\n        yolo classify predict model=path/to/best.pt source=\\'https://ultralytics.com/images/bus.jpg\\'  # predict with custom model\\n        ```\\n\\nSee full `predict` mode details in the [Predict](https://docs.ultralytics.com/modes/predict/) page.\\n\\n## Export\\n\\nExport a YOLOv8n-cls model to a different format like ONNX, CoreML, etc.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-cls.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom trained\\n        \\n        # Export the model\\n        model.export(format=\\'onnx\\')\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo export model=yolov8n-cls.pt format=onnx  # export official model\\n        yolo export model=path/to/best.pt format=onnx  # export custom trained model\\n        ```\\n\\nAvailable YOLOv8-cls export formats are in the table below. You can predict or validate directly on exported models,\\ni.e. `yolo predict model=yolov8n-cls.onnx`. Usage examples are shown for your model after export completes.\\n\\n| Format                                                             | `format` Argument | Model                         | Metadata |\\n|--------------------------------------------------------------------|-------------------|-------------------------------|----------|\\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n-cls.pt`              | ✅        |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n-cls.torchscript`     | ✅        |\\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n-cls.onnx`            | ✅        |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n-cls_openvino_model/` | ✅        |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n-cls.engine`          | ✅        |\\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n-cls.mlmodel`         | ✅        |\\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n-cls_saved_model/`    | ✅        |\\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n-cls.pb`              | ❌        |\\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n-cls.tflite`          | ✅        |\\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n-cls_edgetpu.tflite`  | ✅        |\\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n-cls_web_model/`      | ✅        |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n-cls_paddle_model/`   | ✅        |\\n\\nSee full `export` details in the [Export](https://docs.ultralytics.com/modes/export/) page.\\n', metadata={'file_path': 'docs/tasks/classify.md', 'file_name': 'classify.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\nObject detection is a task that involves identifying the location and class of objects in an image or video stream.\\n\\n<img width=\"1024\" src=\"https://user-images.githubusercontent.com/26833433/212094133-6bb8c21c-3d47-41df-a512-81c5931054ae.png\">\\n\\nThe output of an object detector is a set of bounding boxes that enclose the objects in the image, along with class\\nlabels\\nand confidence scores for each box. Object detection is a good choice when you need to identify objects of interest in a\\nscene, but don\\'t need to know exactly where the object is or its exact shape.\\n\\n!!! tip \"Tip\"\\n\\n    YOLOv8 Detect models are the default YOLOv8 models, i.e. `yolov8n.pt` and are pretrained on [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml).\\n\\n## [Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models/v8)\\n\\nYOLOv8 pretrained Detect models are shown here. Detect, Segment and Pose models are pretrained on\\nthe [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) dataset, while Classify\\nmodels are pretrained on\\nthe [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) dataset.\\n\\n[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) download automatically from the latest\\nUltralytics [release](https://github.com/ultralytics/assets/releases) on first use.\\n\\n| Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n|--------------------------------------------------------------------------------------|-----------------------|----------------------|--------------------------------|-------------------------------------|--------------------|-------------------|\\n| [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640                   | 37.3                 | 80.4                           | 0.99                                | 3.2                | 8.7               |\\n| [YOLOv8s](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt) | 640                   | 44.9                 | 128.4                          | 1.20                                | 11.2               | 28.6              |\\n| [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640                   | 50.2                 | 234.7                          | 1.83                                | 25.9               | 78.9              |\\n| [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640                   | 52.9                 | 375.2                          | 2.39                                | 43.7               | 165.2             |\\n| [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640                   | 53.9                 | 479.1                          | 3.53                                | 68.2               | 257.8             |\\n\\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO val2017](http://cocodataset.org) dataset.\\n  <br>Reproduce by `yolo val detect data=coco.yaml device=0`\\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/)\\n  instance.\\n  <br>Reproduce by `yolo val detect data=coco128.yaml batch=1 device=0|cpu`\\n\\n## Train\\n\\nTrain YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. For a full list of available arguments see\\nthe [Configuration](../usage/cfg.md) page.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.yaml\\')  # build a new model from YAML\\n        model = YOLO(\\'yolov8n.pt\\')  # load a pretrained model (recommended for training)\\n        model = YOLO(\\'yolov8n.yaml\\').load(\\'yolov8n.pt\\')  # build from YAML and transfer weights\\n        \\n        # Train the model\\n        model.train(data=\\'coco128.yaml\\', epochs=100, imgsz=640)\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        # Build a new model from YAML and start training from scratch\\n        yolo detect train data=coco128.yaml model=yolov8n.yaml epochs=100 imgsz=640\\n\\n        # Start training from a pretrained *.pt model\\n        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\\n\\n        # Build a new model from YAML, transfer pretrained weights to it and start training\\n        yolo detect train data=coco128.yaml model=yolov8n.yaml pretrained=yolov8n.pt epochs=100 imgsz=640\\n        ```\\n### Dataset format\\n\\nYOLO detection dataset format can be found in detail in the [Dataset Guide](../yolov5/tutorials/train_custom_data.md).\\nTo convert your existing dataset from other formats( like COCO, VOC etc.) to YOLO format, please use [json2yolo tool](https://github.com/ultralytics/JSON2YOLO) by Ultralytics.\\n\\n## Val\\n\\nValidate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the `model` retains it\\'s\\ntraining `data` and arguments as model attributes.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Validate the model\\n        metrics = model.val()  # no arguments needed, dataset and settings remembered\\n        metrics.box.map    # map50-95\\n        metrics.box.map50  # map50\\n        metrics.box.map75  # map75\\n        metrics.box.maps   # a list contains map50-95 of each category\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo detect val model=yolov8n.pt  # val official model\\n        yolo detect val model=path/to/best.pt  # val custom model\\n        ```\\n\\n## Predict\\n\\nUse a trained YOLOv8n model to run predictions on images.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Predict with the model\\n        results = model(\\'https://ultralytics.com/images/bus.jpg\\')  # predict on an image\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo detect predict model=yolov8n.pt source=\\'https://ultralytics.com/images/bus.jpg\\'  # predict with official model\\n        yolo detect predict model=path/to/best.pt source=\\'https://ultralytics.com/images/bus.jpg\\'  # predict with custom model\\n        ```\\n\\nSee full `predict` mode details in the [Predict](https://docs.ultralytics.com/modes/predict/) page.\\n\\n## Export\\n\\nExport a YOLOv8n model to a different format like ONNX, CoreML, etc.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom trained\\n        \\n        # Export the model\\n        model.export(format=\\'onnx\\')\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo export model=yolov8n.pt format=onnx  # export official model\\n        yolo export model=path/to/best.pt format=onnx  # export custom trained model\\n        ```\\n\\nAvailable YOLOv8 export formats are in the table below. You can predict or validate directly on exported models,\\ni.e. `yolo predict model=yolov8n.onnx`. Usage examples are shown for your model after export completes.\\n\\n| Format                                                             | `format` Argument | Model                     | Metadata |\\n|--------------------------------------------------------------------|-------------------|---------------------------|----------|\\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | ✅        |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | ✅        |\\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | ✅        |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | ✅        |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | ✅        |\\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | ✅        |\\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | ✅        |\\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | ❌        |\\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | ✅        |\\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ✅        |\\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | ✅        |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | ✅        |\\n\\nSee full `export` details in the [Export](https://docs.ultralytics.com/modes/export/) page.\\n', metadata={'file_path': 'docs/tasks/detect.md', 'file_name': 'detect.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Ultralytics YOLOv8 Tasks\\n\\nYOLOv8 is an AI framework that supports multiple computer vision **tasks**. The framework can be used to\\nperform [detection](detect.md), [segmentation](segment.md), [classification](classify.md),\\nand [pose](pose.md) estimation. Each of these tasks has a different objective and use case.\\n\\n<img width=\"1024\" src=\"https://user-images.githubusercontent.com/26833433/212094133-6bb8c21c-3d47-41df-a512-81c5931054ae.png\">\\n\\n## [Detection](detect.md)\\n\\nDetection is the primary task supported by YOLOv8. It involves detecting objects in an image or video frame and drawing\\nbounding boxes around them. The detected objects are classified into different categories based on their features.\\nYOLOv8 can detect multiple objects in a single image or video frame with high accuracy and speed.\\n\\n[Detection Examples](detect.md){ .md-button .md-button--primary}\\n\\n## [Segmentation](segment.md)\\n\\nSegmentation is a task that involves segmenting an image into different regions based on the content of the image. Each\\nregion is assigned a label based on its content. This task is useful in applications such as image segmentation and\\nmedical imaging. YOLOv8 uses a variant of the U-Net architecture to perform segmentation.\\n\\n[Segmentation Examples](segment.md){ .md-button .md-button--primary}\\n\\n## [Classification](classify.md)\\n\\nClassification is a task that involves classifying an image into different categories. YOLOv8 can be used to classify\\nimages based on their content. It uses a variant of the EfficientNet architecture to perform classification.\\n\\n[Classification Examples](classify.md){ .md-button .md-button--primary}\\n\\n## [Pose](pose.md)\\n\\nPose/keypoint detection is a task that involves detecting specific points in an image or video frame. These points are \\nreferred to as keypoints and are used to track movement or pose estimation. YOLOv8 can detect keypoints in an image or\\nvideo frame with high accuracy and speed.\\n\\n[Pose Examples](pose.md){ .md-button .md-button--primary}\\n\\n## Conclusion\\n\\nYOLOv8 supports multiple tasks, including detection, segmentation, classification, and keypoints detection. Each of\\nthese tasks has different objectives and use cases. By understanding the differences between these tasks, you can choose\\nthe appropriate task for your computer vision application.', metadata={'file_path': 'docs/tasks/index.md', 'file_name': 'index.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\nPose estimation is a task that involves identifying the location of specific points in an image, usually referred\\nto as keypoints. The keypoints can represent various parts of the object such as joints, landmarks, or other distinctive\\nfeatures. The locations of the keypoints are usually represented as a set of 2D `[x, y]` or 3D `[x, y, visible]`\\ncoordinates.\\n\\n<img width=\"1024\" src=\"https://user-images.githubusercontent.com/26833433/212094133-6bb8c21c-3d47-41df-a512-81c5931054ae.png\">\\n\\nThe output of a pose estimation model is a set of points that represent the keypoints on an object in the image, usually\\nalong with the confidence scores for each point. Pose estimation is a good choice when you need to identify specific\\nparts of an object in a scene, and their location in relation to each other.\\n\\n!!! tip \"Tip\"\\n\\n    YOLOv8 _pose_ models use the `-pose` suffix, i.e. `yolov8n-pose.pt`. These models are trained on the [COCO keypoints](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco-pose.yaml) dataset and are suitable for a variety of pose estimation tasks.\\n\\n## [Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models/v8)\\n\\nYOLOv8 pretrained Pose models are shown here. Detect, Segment and Pose models are pretrained on\\nthe [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) dataset, while Classify\\nmodels are pretrained on\\nthe [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) dataset.\\n\\n[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) download automatically from the latest\\nUltralytics [release](https://github.com/ultralytics/assets/releases) on first use.\\n\\n| Model                                                                                                | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n| ---------------------------------------------------------------------------------------------------- | --------------------- |-----------------------|--------------------| ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\\n| [YOLOv8n-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt)       | 640                   | 50.4                  | 80.1               | 131.8                          | 1.18                                | 3.3                | 9.2               |\\n| [YOLOv8s-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt)       | 640                   | 60.0                  | 86.2               | 233.2                          | 1.42                                | 11.6               | 30.2              |\\n| [YOLOv8m-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-pose.pt)       | 640                   | 65.0                  | 88.8               | 456.3                          | 2.00                                | 26.4               | 81.0              |\\n| [YOLOv8l-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-pose.pt)       | 640                   | 67.6                  | 90.0               | 784.5                          | 2.59                                | 44.4               | 168.6             |\\n| [YOLOv8x-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose.pt)       | 640                   | 69.2                  | 90.2               | 1607.1                         | 3.73                                | 69.4               | 263.2             |\\n| [YOLOv8x-pose-p6](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose-p6.pt) | 1280                  | 71.6                  | 91.2               | 4088.7                         | 10.04                               | 99.1               | 1066.4            |\\n\\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO Keypoints val2017](http://cocodataset.org)\\n  dataset.\\n  <br>Reproduce by `yolo val pose data=coco-pose.yaml device=0`\\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/)\\n  instance.\\n  <br>Reproduce by `yolo val pose data=coco8-pose.yaml batch=1 device=0|cpu`\\n\\n## Train\\n\\nTrain a YOLOv8-pose model on the COCO128-pose dataset.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-pose.yaml\\')  # build a new model from YAML\\n        model = YOLO(\\'yolov8n-pose.pt\\')  # load a pretrained model (recommended for training)\\n        model = YOLO(\\'yolov8n-pose.yaml\\').load(\\'yolov8n-pose.pt\\')  # build from YAML and transfer weights\\n        \\n        # Train the model\\n        model.train(data=\\'coco8-pose.yaml\\', epochs=100, imgsz=640)\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        # Build a new model from YAML and start training from scratch\\n        yolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml epochs=100 imgsz=640\\n\\n        # Start training from a pretrained *.pt model\\n        yolo pose train data=coco8-pose.yaml model=yolov8n-pose.pt epochs=100 imgsz=640\\n\\n        # Build a new model from YAML, transfer pretrained weights to it and start training\\n        yolo pose train data=coco8-pose.yaml model=yolov8n-pose.yaml pretrained=yolov8n-pose.pt epochs=100 imgsz=640\\n        ```\\n\\n## Val\\n\\nValidate trained YOLOv8n-pose model accuracy on the COCO128-pose dataset. No argument need to passed as the `model`\\nretains it\\'s\\ntraining `data` and arguments as model attributes.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-pose.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Validate the model\\n        metrics = model.val()  # no arguments needed, dataset and settings remembered\\n        metrics.box.map    # map50-95\\n        metrics.box.map50  # map50\\n        metrics.box.map75  # map75\\n        metrics.box.maps   # a list contains map50-95 of each category\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo pose val model=yolov8n-pose.pt  # val official model\\n        yolo pose val model=path/to/best.pt  # val custom model\\n        ```\\n\\n## Predict\\n\\nUse a trained YOLOv8n-pose model to run predictions on images.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-pose.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Predict with the model\\n        results = model(\\'https://ultralytics.com/images/bus.jpg\\')  # predict on an image\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo pose predict model=yolov8n-pose.pt source=\\'https://ultralytics.com/images/bus.jpg\\'  # predict with official model\\n        yolo pose predict model=path/to/best.pt source=\\'https://ultralytics.com/images/bus.jpg\\'  # predict with custom model\\n        ```\\n\\nSee full `predict` mode details in the [Predict](https://docs.ultralytics.com/modes/predict/) page.\\n\\n## Export\\n\\nExport a YOLOv8n Pose model to a different format like ONNX, CoreML, etc.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-pose.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom trained\\n        \\n        # Export the model\\n        model.export(format=\\'onnx\\')\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo export model=yolov8n-pose.pt format=onnx  # export official model\\n        yolo export model=path/to/best.pt format=onnx  # export custom trained model\\n        ```\\n\\nAvailable YOLOv8-pose export formats are in the table below. You can predict or validate directly on exported models,\\ni.e. `yolo predict model=yolov8n-pose.onnx`. Usage examples are shown for your model after export completes.\\n\\n| Format                                                             | `format` Argument | Model                          | Metadata |\\n|--------------------------------------------------------------------|-------------------|--------------------------------|----------|\\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n-pose.pt`              | ✅        |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n-pose.torchscript`     | ✅        |\\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n-pose.onnx`            | ✅        |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n-pose_openvino_model/` | ✅        |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n-pose.engine`          | ✅        |\\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n-pose.mlmodel`         | ✅        |\\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n-pose_saved_model/`    | ✅        |\\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n-pose.pb`              | ❌        |\\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n-pose.tflite`          | ✅        |\\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n-pose_edgetpu.tflite`  | ✅        |\\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n-pose_web_model/`      | ✅        |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n-pose_paddle_model/`   | ✅        |\\n\\nSee full `export` details in the [Export](https://docs.ultralytics.com/modes/export/) page.\\n', metadata={'file_path': 'docs/tasks/pose.md', 'file_name': 'pose.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\nInstance segmentation goes a step further than object detection and involves identifying individual objects in an image\\nand segmenting them from the rest of the image.\\n\\n<img width=\"1024\" src=\"https://user-images.githubusercontent.com/26833433/212094133-6bb8c21c-3d47-41df-a512-81c5931054ae.png\">\\n\\nThe output of an instance segmentation model is a set of masks or\\ncontours that outline each object in the image, along with class labels and confidence scores for each object. Instance\\nsegmentation is useful when you need to know not only where objects are in an image, but also what their exact shape is.\\n\\n!!! tip \"Tip\"\\n\\n    YOLOv8 Segment models use the `-seg` suffix, i.e. `yolov8n-seg.pt` and are pretrained on [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml).\\n\\n## [Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models/v8)\\n\\nYOLOv8 pretrained Segment models are shown here. Detect, Segment and Pose models are pretrained on\\nthe [COCO](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/coco.yaml) dataset, while Classify\\nmodels are pretrained on\\nthe [ImageNet](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/datasets/ImageNet.yaml) dataset.\\n\\n[Models](https://github.com/ultralytics/ultralytics/tree/main/ultralytics/models) download automatically from the latest\\nUltralytics [release](https://github.com/ultralytics/assets/releases) on first use.\\n\\n| Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\\n|----------------------------------------------------------------------------------------------|-----------------------|----------------------|-----------------------|--------------------------------|-------------------------------------|--------------------|-------------------|\\n| [YOLOv8n-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt) | 640                   | 36.7                 | 30.5                  | 96.1                           | 1.21                                | 3.4                | 12.6              |\\n| [YOLOv8s-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-seg.pt) | 640                   | 44.6                 | 36.8                  | 155.7                          | 1.47                                | 11.8               | 42.6              |\\n| [YOLOv8m-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-seg.pt) | 640                   | 49.9                 | 40.8                  | 317.0                          | 2.18                                | 27.3               | 110.2             |\\n| [YOLOv8l-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-seg.pt) | 640                   | 52.3                 | 42.6                  | 572.4                          | 2.79                                | 46.0               | 220.5             |\\n| [YOLOv8x-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt) | 640                   | 53.4                 | 43.4                  | 712.1                          | 4.02                                | 71.8               | 344.1             |\\n\\n- **mAP<sup>val</sup>** values are for single-model single-scale on [COCO val2017](http://cocodataset.org) dataset.\\n  <br>Reproduce by `yolo val segment data=coco.yaml device=0`\\n- **Speed** averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/)\\n  instance.\\n  <br>Reproduce by `yolo val segment data=coco128-seg.yaml batch=1 device=0|cpu`\\n\\n## Train\\n\\nTrain YOLOv8n-seg on the COCO128-seg dataset for 100 epochs at image size 640. For a full list of available\\narguments see the [Configuration](../usage/cfg.md) page.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-seg.yaml\\')  # build a new model from YAML\\n        model = YOLO(\\'yolov8n-seg.pt\\')  # load a pretrained model (recommended for training)\\n        model = YOLO(\\'yolov8n-seg.yaml\\').load(\\'yolov8n.pt\\')  # build from YAML and transfer weights\\n        \\n        # Train the model\\n        model.train(data=\\'coco128-seg.yaml\\', epochs=100, imgsz=640)\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        # Build a new model from YAML and start training from scratch\\n        yolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml epochs=100 imgsz=640\\n\\n        # Start training from a pretrained *.pt model\\n        yolo segment train data=coco128-seg.yaml model=yolov8n-seg.pt epochs=100 imgsz=640\\n\\n        # Build a new model from YAML, transfer pretrained weights to it and start training\\n        yolo segment train data=coco128-seg.yaml model=yolov8n-seg.yaml pretrained=yolov8n-seg.pt epochs=100 imgsz=640\\n        ```\\n\\n### Dataset format\\nYOLO segmentation dataset label format extends detection format with segment points.\\n\\n`cls x1 y1 x2 y2 p1 p2 ... pn`\\n\\nTo convert your existing dataset from other formats( like COCO, VOC etc.) to YOLO format, please use [json2yolo tool](https://github.com/ultralytics/JSON2YOLO) by Ultralytics.\\n\\n## Val\\n\\nValidate trained YOLOv8n-seg model accuracy on the COCO128-seg dataset. No argument need to passed as the `model`\\nretains it\\'s training `data` and arguments as model attributes.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-seg.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Validate the model\\n        metrics = model.val()  # no arguments needed, dataset and settings remembered\\n        metrics.box.map    # map50-95(B)\\n        metrics.box.map50  # map50(B)\\n        metrics.box.map75  # map75(B)\\n        metrics.box.maps   # a list contains map50-95(B) of each category\\n        metrics.seg.map    # map50-95(M)\\n        metrics.seg.map50  # map50(M)\\n        metrics.seg.map75  # map75(M)\\n        metrics.seg.maps   # a list contains map50-95(M) of each category\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo segment val model=yolov8n-seg.pt  # val official model\\n        yolo segment val model=path/to/best.pt  # val custom model\\n        ```\\n\\n## Predict\\n\\nUse a trained YOLOv8n-seg model to run predictions on images.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-seg.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Predict with the model\\n        results = model(\\'https://ultralytics.com/images/bus.jpg\\')  # predict on an image\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo segment predict model=yolov8n-seg.pt source=\\'https://ultralytics.com/images/bus.jpg\\'  # predict with official model\\n        yolo segment predict model=path/to/best.pt source=\\'https://ultralytics.com/images/bus.jpg\\'  # predict with custom model\\n        ```\\n\\nSee full `predict` mode details in the [Predict](https://docs.ultralytics.com/modes/predict/) page.\\n\\n## Export\\n\\nExport a YOLOv8n-seg model to a different format like ONNX, CoreML, etc.\\n\\n!!! example \"\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n-seg.pt\\')  # load an official model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom trained\\n        \\n        # Export the model\\n        model.export(format=\\'onnx\\')\\n        ```\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo export model=yolov8n-seg.pt format=onnx  # export official model\\n        yolo export model=path/to/best.pt format=onnx  # export custom trained model\\n        ```\\n\\nAvailable YOLOv8-seg export formats are in the table below. You can predict or validate directly on exported models,\\ni.e. `yolo predict model=yolov8n-seg.onnx`. Usage examples are shown for your model after export completes.\\n\\n| Format                                                             | `format` Argument | Model                         | Metadata |\\n|--------------------------------------------------------------------|-------------------|-------------------------------|----------|\\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n-seg.pt`              | ✅        |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n-seg.torchscript`     | ✅        |\\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n-seg.onnx`            | ✅        |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n-seg_openvino_model/` | ✅        |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n-seg.engine`          | ✅        |\\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n-seg.mlmodel`         | ✅        |\\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n-seg_saved_model/`    | ✅        |\\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n-seg.pb`              | ❌        |\\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n-seg.tflite`          | ✅        |\\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n-seg_edgetpu.tflite`  | ✅        |\\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n-seg_web_model/`      | ✅        |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n-seg_paddle_model/`   | ✅        |\\n\\nSee full `export` details in the [Export](https://docs.ultralytics.com/modes/export/) page.\\n', metadata={'file_path': 'docs/tasks/segment.md', 'file_name': 'segment.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n## Callbacks\\n\\nUltralytics framework supports callbacks as entry points in strategic stages of train, val, export, and predict modes.\\nEach callback accepts a `Trainer`, `Validator`, or `Predictor` object depending on the operation type. All properties of\\nthese objects can be found in Reference section of the docs.\\n\\n## Examples\\n\\n### Returning additional information with Prediction\\n\\nIn this example, we want to return the original frame with each result object. Here\\'s how we can do that\\n\\n```python\\ndef on_predict_batch_end(predictor):\\n    # Retrieve the batch data\\n    _, im0s, _, _ = predictor.batch\\n    \\n    # Ensure that im0s is a list\\n    im0s = im0s if isinstance(im0s, list) else [im0s]\\n    \\n    # Combine the prediction results with the corresponding frames\\n    predictor.results = zip(predictor.results, im0s)\\n\\n# Create a YOLO model instance\\nmodel = YOLO(f\\'yolov8n.pt\\')\\n\\n# Add the custom callback to the model\\nmodel.add_callback(\"on_predict_batch_end\", on_predict_batch_end)\\n\\n# Iterate through the results and frames\\nfor (result, frame) in model.track/predict():\\n    pass\\n```\\n\\n## All callbacks\\n\\nHere are all supported callbacks. See callbacks [source code](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/utils/callbacks/base.py) for additional details.\\n\\n\\n### Trainer Callbacks\\n\\n| Callback                    | Description                                             |\\n|-----------------------------|---------------------------------------------------------|\\n| `on_pretrain_routine_start` | Triggered at the beginning of pre-training routine      |\\n| `on_pretrain_routine_end`   | Triggered at the end of pre-training routine            |\\n| `on_train_start`            | Triggered when the training starts                      |\\n| `on_train_epoch_start`      | Triggered at the start of each training epoch           |\\n| `on_train_batch_start`      | Triggered at the start of each training batch           |\\n| `optimizer_step`            | Triggered during the optimizer step                     |\\n| `on_before_zero_grad`       | Triggered before gradients are zeroed                   |\\n| `on_train_batch_end`        | Triggered at the end of each training batch             |\\n| `on_train_epoch_end`        | Triggered at the end of each training epoch             |\\n| `on_fit_epoch_end`          | Triggered at the end of each fit epoch                  |\\n| `on_model_save`             | Triggered when the model is saved                       |\\n| `on_train_end`              | Triggered when the training process ends                |\\n| `on_params_update`          | Triggered when model parameters are updated             |\\n| `teardown`                  | Triggered when the training process is being cleaned up |\\n\\n\\n### Validator Callbacks\\n\\n| Callback             | Description                                     |\\n|----------------------|-------------------------------------------------|\\n| `on_val_start`       | Triggered when the validation starts            |\\n| `on_val_batch_start` | Triggered at the start of each validation batch |\\n| `on_val_batch_end`   | Triggered at the end of each validation batch   |\\n| `on_val_end`         | Triggered when the validation ends              |\\n\\n\\n### Predictor Callbacks\\n\\n| Callback                     | Description                                       |\\n|------------------------------|---------------------------------------------------|\\n| `on_predict_start`           | Triggered when the prediction process starts      |\\n| `on_predict_batch_start`     | Triggered at the start of each prediction batch   |\\n| `on_predict_postprocess_end` | Triggered at the end of prediction postprocessing |\\n| `on_predict_batch_end`       | Triggered at the end of each prediction batch     |\\n| `on_predict_end`             | Triggered when the prediction process ends        |\\n\\n### Exporter Callbacks\\n\\n| Callback          | Description                              |\\n|-------------------|------------------------------------------|\\n| `on_export_start` | Triggered when the export process starts |\\n| `on_export_end`   | Triggered when the export process ends   |\\n', metadata={'file_path': 'docs/usage/callbacks.md', 'file_name': 'callbacks.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\nYOLO settings and hyperparameters play a critical role in the model\\'s performance, speed, and accuracy. These settings\\nand hyperparameters can affect the model\\'s behavior at various stages of the model development process, including\\ntraining, validation, and prediction.\\n\\nYOLOv8 \\'yolo\\' CLI commands use the following syntax:\\n\\n!!! example \"\"\\n\\n    === \"CLI\"\\n    \\n        ```bash\\n        yolo TASK MODE ARGS\\n        ```\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a YOLOv8 model from a pre-trained weights file\\n        model = YOLO(\\'yolov8n.pt\\')\\n         \\n        # Run MODE mode using the custom arguments ARGS (guess TASK)\\n        model.MODE(ARGS)\\n        ```\\n\\nWhere:\\n\\n- `TASK` (optional) is one of `[detect, segment, classify, pose]`. If it is not passed explicitly YOLOv8 will try to\\n  guess\\n  the `TASK` from the model type.\\n- `MODE` (required) is one of `[train, val, predict, export, track, benchmark]`\\n- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults.\\n  For a full list of available `ARGS` see the [Configuration](cfg.md) page and `defaults.yaml`\\n  GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/cfg/default.yaml).\\n\\n#### Tasks\\n\\nYOLO models can be used for a variety of tasks, including detection, segmentation, classification and pose. These tasks\\ndiffer in the type of output they produce and the specific problem they are designed to solve.\\n\\n**Detect**: For identifying and localizing objects or regions of interest in an image or video.  \\n**Segment**: For dividing an image or video into regions or pixels that correspond to different objects or classes.  \\n**Classify**: For predicting the class label of an input image.  \\n**Pose**: For identifying objects and estimating their keypoints in an image or video.\\n\\n| Key    | Value      | Description                                     |\\n|--------|------------|-------------------------------------------------|\\n| `task` | `\\'detect\\'` | YOLO task, i.e. detect, segment, classify, pose |\\n\\n[Tasks Guide](../tasks/index.md){ .md-button .md-button--primary}\\n\\n#### Modes\\n\\nYOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes\\ninclude:\\n\\n**Train**: For training a YOLOv8 model on a custom dataset.  \\n**Val**: For validating a YOLOv8 model after it has been trained.  \\n**Predict**: For making predictions using a trained YOLOv8 model on new images or videos.  \\n**Export**: For exporting a YOLOv8 model to a format that can be used for deployment.  \\n**Track**: For tracking objects in real-time using a YOLOv8 model.  \\n**Benchmark**: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.\\n\\n| Key    | Value     | Description                                                   |\\n|--------|-----------|---------------------------------------------------------------|\\n| `mode` | `\\'train\\'` | YOLO mode, i.e. train, val, predict, export, track, benchmark |\\n\\n[Modes Guide](../modes/index.md){ .md-button .md-button--primary}\\n\\n## Train\\n\\nThe training settings for YOLO models encompass various hyperparameters and configurations used during the training process. These settings influence the model\\'s performance, speed, and accuracy. Key training settings include batch size, learning rate, momentum, and weight decay. Additionally, the choice of optimizer, loss function, and training dataset composition can impact the training process. Careful tuning and experimentation with these settings are crucial for optimizing performance.\\n\\n| Key               | Value    | Description                                                                 |\\n|-------------------|----------|-----------------------------------------------------------------------------|\\n| `model`           | `None`   | path to model file, i.e. yolov8n.pt, yolov8n.yaml                           |\\n| `data`            | `None`   | path to data file, i.e. coco128.yaml                                        |\\n| `epochs`          | `100`    | number of epochs to train for                                               |\\n| `patience`        | `50`     | epochs to wait for no observable improvement for early stopping of training |\\n| `batch`           | `16`     | number of images per batch (-1 for AutoBatch)                               |\\n| `imgsz`           | `640`    | size of input images as integer or w,h                                      |\\n| `save`            | `True`   | save train checkpoints and predict results                                  |\\n| `save_period`     | `-1`     | Save checkpoint every x epochs (disabled if < 1)                            |\\n| `cache`           | `False`  | True/ram, disk or False. Use cache for data loading                         |\\n| `device`          | `None`   | device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu        |\\n| `workers`         | `8`      | number of worker threads for data loading (per RANK if DDP)                 |\\n| `project`         | `None`   | project name                                                                |\\n| `name`            | `None`   | experiment name                                                             |\\n| `exist_ok`        | `False`  | whether to overwrite existing experiment                                    |\\n| `pretrained`      | `False`  | whether to use a pretrained model                                           |\\n| `optimizer`       | `\\'SGD\\'`  | optimizer to use, choices=[\\'SGD\\', \\'Adam\\', \\'AdamW\\', \\'RMSProp\\']               |\\n| `verbose`         | `False`  | whether to print verbose output                                             |\\n| `seed`            | `0`      | random seed for reproducibility                                             |\\n| `deterministic`   | `True`   | whether to enable deterministic mode                                        |\\n| `single_cls`      | `False`  | train multi-class data as single-class                                      |\\n| `rect`            | `False`  | rectangular training with each batch collated for minimum padding           |\\n| `cos_lr`          | `False`  | use cosine learning rate scheduler                                          |\\n| `close_mosaic`    | `0`      | (int) disable mosaic augmentation for final epochs                          |\\n| `resume`          | `False`  | resume training from last checkpoint                                        |\\n| `amp`             | `True`   | Automatic Mixed Precision (AMP) training, choices=[True, False]             |\\n| `lr0`             | `0.01`   | initial learning rate (i.e. SGD=1E-2, Adam=1E-3)                            |\\n| `lrf`             | `0.01`   | final learning rate (lr0 * lrf)                                             |\\n| `momentum`        | `0.937`  | SGD momentum/Adam beta1                                                     |\\n| `weight_decay`    | `0.0005` | optimizer weight decay 5e-4                                                 |\\n| `warmup_epochs`   | `3.0`    | warmup epochs (fractions ok)                                                |\\n| `warmup_momentum` | `0.8`    | warmup initial momentum                                                     |\\n| `warmup_bias_lr`  | `0.1`    | warmup initial bias lr                                                      |\\n| `box`             | `7.5`    | box loss gain                                                               |\\n| `cls`             | `0.5`    | cls loss gain (scale with pixels)                                           |\\n| `dfl`             | `1.5`    | dfl loss gain                                                               |\\n| `pose`            | `12.0`   | pose loss gain (pose-only)                                                  |\\n| `kobj`            | `2.0`    | keypoint obj loss gain (pose-only)                                          |\\n| `label_smoothing` | `0.0`    | label smoothing (fraction)                                                  |\\n| `nbs`             | `64`     | nominal batch size                                                          |\\n| `overlap_mask`    | `True`   | masks should overlap during training (segment train only)                   |\\n| `mask_ratio`      | `4`      | mask downsample ratio (segment train only)                                  |\\n| `dropout`         | `0.0`    | use dropout regularization (classify train only)                            |\\n| `val`             | `True`   | validate/test during training                                               |\\n\\n[Train Guide](../modes/train.md){ .md-button .md-button--primary}\\n\\n## Predict\\n\\nThe prediction settings for YOLO models encompass a range of hyperparameters and configurations that influence the model\\'s performance, speed, and accuracy during inference on new data. Careful tuning and experimentation with these settings are essential to achieve optimal performance for a specific task. Key settings include the confidence threshold, Non-Maximum Suppression (NMS) threshold, and the number of classes considered. Additional factors affecting the prediction process are input data size and format, the presence of supplementary features such as masks or multiple labels per box, and the particular task the model is employed for.\\n\\n| Key              | Value                  | Description                                              |\\n|------------------|------------------------|----------------------------------------------------------|\\n| `source`         | `\\'ultralytics/assets\\'` | source directory for images or videos                    |\\n| `conf`           | `0.25`                 | object confidence threshold for detection                |\\n| `iou`            | `0.7`                  | intersection over union (IoU) threshold for NMS          |\\n| `half`           | `False`                | use half precision (FP16)                                |\\n| `device`         | `None`                 | device to run on, i.e. cuda device=0/1/2/3 or device=cpu |\\n| `show`           | `False`                | show results if possible                                 |\\n| `save`           | `False`                | save images with results                                 |\\n| `save_txt`       | `False`                | save results as .txt file                                |\\n| `save_conf`      | `False`                | save results with confidence scores                      |\\n| `save_crop`      | `False`                | save cropped images with results                         |\\n| `show_labels`    | `True`                 | show object labels in plots                              |\\n| `show_conf`      | `True`                 | show object confidence scores in plots                   |\\n| `max_det`        | `300`                  | maximum number of detections per image                   |\\n| `vid_stride`     | `False`                | video frame-rate stride                                  |\\n| `line_thickness` | `3`                    | bounding box thickness (pixels)                          |\\n| `visualize`      | `False`                | visualize model features                                 |\\n| `augment`        | `False`                | apply image augmentation to prediction sources           |\\n| `agnostic_nms`   | `False`                | class-agnostic NMS                                       |\\n| `retina_masks`   | `False`                | use high-resolution segmentation masks                   |\\n| `classes`        | `None`                 | filter results by class, i.e. class=0, or class=[0,2,3]  |\\n| `boxes`          | `True`                 | Show boxes in segmentation predictions                   |\\n\\n[Predict Guide](../modes/predict.md){ .md-button .md-button--primary}\\n\\n## Val\\n\\nThe val (validation) settings for YOLO models involve various hyperparameters and configurations used to evaluate the model\\'s performance on a validation dataset. These settings influence the model\\'s performance, speed, and accuracy. Common YOLO validation settings include batch size, validation frequency during training, and performance evaluation metrics. Other factors affecting the validation process include the validation dataset\\'s size and composition, as well as the specific task the model is employed for. Careful tuning and experimentation with these settings are crucial to ensure optimal performance on the validation dataset and detect and prevent overfitting.\\n\\n| Key           | Value   | Description                                                        |\\n|---------------|---------|--------------------------------------------------------------------|\\n| `save_json`   | `False` | save results to JSON file                                          |\\n| `save_hybrid` | `False` | save hybrid version of labels (labels + additional predictions)    |\\n| `conf`        | `0.001` | object confidence threshold for detection                          |\\n| `iou`         | `0.6`   | intersection over union (IoU) threshold for NMS                    |\\n| `max_det`     | `300`   | maximum number of detections per image                             |\\n| `half`        | `True`  | use half precision (FP16)                                          |\\n| `device`      | `None`  | device to run on, i.e. cuda device=0/1/2/3 or device=cpu           |\\n| `dnn`         | `False` | use OpenCV DNN for ONNX inference                                  |\\n| `plots`       | `False` | show plots during training                                         |\\n| `rect`        | `False` | rectangular val with each batch collated for minimum padding       |\\n| `split`       | `val`   | dataset split to use for validation, i.e. \\'val\\', \\'test\\' or \\'train\\' |\\n\\n[Val Guide](../modes/val.md){ .md-button .md-button--primary}\\n\\n## Export\\n\\nExport settings for YOLO models encompass configurations and options related to saving or exporting the model for use in different environments or platforms. These settings can impact the model\\'s performance, size, and compatibility with various systems. Key export settings include the exported model file format (e.g., ONNX, TensorFlow SavedModel), the target device (e.g., CPU, GPU), and additional features such as masks or multiple labels per box. The export process may also be affected by the model\\'s specific task and the requirements or constraints of the destination environment or platform. It is crucial to thoughtfully configure these settings to ensure the exported model is optimized for the intended use case and functions effectively in the target environment.\\n\\n| Key         | Value           | Description                                          |\\n|-------------|-----------------|------------------------------------------------------|\\n| `format`    | `\\'torchscript\\'` | format to export to                                  |\\n| `imgsz`     | `640`           | image size as scalar or (h, w) list, i.e. (640, 480) |\\n| `keras`     | `False`         | use Keras for TF SavedModel export                   |\\n| `optimize`  | `False`         | TorchScript: optimize for mobile                     |\\n| `half`      | `False`         | FP16 quantization                                    |\\n| `int8`      | `False`         | INT8 quantization                                    |\\n| `dynamic`   | `False`         | ONNX/TF/TensorRT: dynamic axes                       |\\n| `simplify`  | `False`         | ONNX: simplify model                                 |\\n| `opset`     | `None`          | ONNX: opset version (optional, defaults to latest)   |\\n| `workspace` | `4`             | TensorRT: workspace size (GB)                        |\\n| `nms`       | `False`         | CoreML: add NMS                                      |\\n\\n[Export Guide](../modes/export.md){ .md-button .md-button--primary}\\n\\n## Augmentation\\n\\nAugmentation settings for YOLO models refer to the various transformations and modifications\\napplied to the training data to increase the diversity and size of the dataset. These settings can affect the model\\'s\\nperformance, speed, and accuracy. Some common YOLO augmentation settings include the type and intensity of the\\ntransformations applied (e.g. random flips, rotations, cropping, color changes), the probability with which each\\ntransformation is applied, and the presence of additional features such as masks or multiple labels per box. Other\\nfactors that may affect the augmentation process include the size and composition of the original dataset and the\\nspecific task the model is being used for. It is important to carefully tune and experiment with these settings to\\nensure that the augmented dataset is diverse and representative enough to train a high-performing model.\\n\\n| Key           | Value | Description                                     |\\n|---------------|-------|-------------------------------------------------|\\n| `hsv_h`       | 0.015 | image HSV-Hue augmentation (fraction)           |\\n| `hsv_s`       | 0.7   | image HSV-Saturation augmentation (fraction)    |\\n| `hsv_v`       | 0.4   | image HSV-Value augmentation (fraction)         |\\n| `degrees`     | 0.0   | image rotation (+/- deg)                        |\\n| `translate`   | 0.1   | image translation (+/- fraction)                |\\n| `scale`       | 0.5   | image scale (+/- gain)                          |\\n| `shear`       | 0.0   | image shear (+/- deg)                           |\\n| `perspective` | 0.0   | image perspective (+/- fraction), range 0-0.001 |\\n| `flipud`      | 0.0   | image flip up-down (probability)                |\\n| `fliplr`      | 0.5   | image flip left-right (probability)             |\\n| `mosaic`      | 1.0   | image mosaic (probability)                      |\\n| `mixup`       | 0.0   | image mixup (probability)                       |\\n| `copy_paste`  | 0.0   | segment copy-paste (probability)                |\\n\\n## Logging, checkpoints, plotting and file management\\n\\nLogging, checkpoints, plotting, and file management are important considerations when training a YOLO model.\\n\\n- Logging: It is often helpful to log various metrics and statistics during training to track the model\\'s progress and\\n  diagnose any issues that may arise. This can be done using a logging library such as TensorBoard or by writing log\\n  messages to a file.\\n- Checkpoints: It is a good practice to save checkpoints of the model at regular intervals during training. This allows\\n  you to resume training from a previous point if the training process is interrupted or if you want to experiment with\\n  different training configurations.\\n- Plotting: Visualizing the model\\'s performance and training progress can be helpful for understanding how the model is\\n  behaving and identifying potential issues. This can be done using a plotting library such as matplotlib or by\\n  generating plots using a logging library such as TensorBoard.\\n- File management: Managing the various files generated during the training process, such as model checkpoints, log\\n  files, and plots, can be challenging. It is important to have a clear and organized file structure to keep track of\\n  these files and make it easy to access and analyze them as needed.\\n\\nEffective logging, checkpointing, plotting, and file management can help you keep track of the model\\'s progress and make\\nit easier to debug and optimize the training process.\\n\\n| Key        | Value    | Description                                                                                    |\\n|------------|----------|------------------------------------------------------------------------------------------------|\\n| `project`  | `\\'runs\\'` | project name                                                                                   |\\n| `name`     | `\\'exp\\'`  | experiment name. `exp` gets automatically incremented if not specified, i.e, `exp`, `exp2` ... |\\n| `exist_ok` | `False`  | whether to overwrite existing experiment                                                       |\\n| `plots`    | `False`  | save plots during train/val                                                                    |\\n| `save`     | `False`  | save train checkpoints and predict results                                                     |\\n', metadata={'file_path': 'docs/usage/cfg.md', 'file_name': 'cfg.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Command Line Interface Usage\\n\\nThe YOLO command line interface (CLI) allows for simple single-line commands without the need for a Python environment.\\nCLI requires no customization or Python code. You can simply run all tasks from the terminal with the `yolo` command.\\n\\n!!! example\\n\\n    === \"Syntax\"\\n\\n        Ultralytics `yolo` commands use the following syntax:\\n        ```bash\\n        yolo TASK MODE ARGS\\n\\n        Where   TASK (optional) is one of [detect, segment, classify]\\n                MODE (required) is one of [train, val, predict, export, track]\\n                ARGS (optional) are any number of custom \\'arg=value\\' pairs like \\'imgsz=320\\' that override defaults.\\n        ```\\n        See all ARGS in the full [Configuration Guide](./cfg.md) or with `yolo cfg`\\n\\n    === \"Train\"\\n\\n        Train a detection model for 10 epochs with an initial learning_rate of 0.01\\n        ```bash\\n        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\\n        ```\\n\\n    === \"Predict\"\\n\\n        Predict a YouTube video using a pretrained segmentation model at image size 320:\\n        ```bash\\n        yolo predict model=yolov8n-seg.pt source=\\'https://youtu.be/Zgi9g1ksQHc\\' imgsz=320\\n        ```\\n\\n    === \"Val\"\\n\\n        Val a pretrained detection model at batch-size 1 and image size 640:\\n        ```bash\\n        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\\n        ```\\n\\n    === \"Export\"\\n\\n        Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\\n        ```bash\\n        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\\n        ```\\n\\n    === \"Special\"\\n\\n        Run special commands to see version, view settings, run checks and more:\\n        ```bash\\n        yolo help\\n        yolo checks\\n        yolo version\\n        yolo settings\\n        yolo copy-cfg\\n        yolo cfg\\n        ```\\n\\nWhere:\\n\\n- `TASK` (optional) is one of `[detect, segment, classify]`. If it is not passed explicitly YOLOv8 will try to guess\\n  the `TASK` from the model type.\\n- `MODE` (required) is one of `[train, val, predict, export, track]`\\n- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults.\\n  For a full list of available `ARGS` see the [Configuration](cfg.md) page and `defaults.yaml`\\n  GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/cfg/default.yaml).\\n\\n!!! warning \"Warning\"\\n\\n    Arguments must be passed as `arg=val` pairs, split by an equals `=` sign and delimited by spaces ` ` between pairs. Do not use `--` argument prefixes or commas `,` beteen arguments.\\n\\n    - `yolo predict model=yolov8n.pt imgsz=640 conf=0.25` &nbsp; ✅\\n    - `yolo predict model yolov8n.pt imgsz 640 conf 0.25` &nbsp; ❌\\n    - `yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25` &nbsp; ❌\\n\\n## Train\\n\\nTrain YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. For a full list of available arguments see\\nthe [Configuration](cfg.md) page.\\n\\n!!! example \"Example\"\\n\\n    === \"Train\"\\n        \\n        Start training YOLOv8n on COCO128 for 100 epochs at image-size 640.\\n        ```bash\\n        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\\n        ```\\n\\n    === \"Resume\"\\n\\n        Resume an interrupted training.\\n        ```bash\\n        yolo detect train resume model=last.pt\\n        ```\\n\\n## Val\\n\\nValidate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the `model` retains it\\'s\\ntraining `data` and arguments as model attributes.\\n\\n!!! example \"Example\"\\n\\n    === \"Official\"\\n\\n        Validate an official YOLOv8n model.\\n        ```bash\\n        yolo detect val model=yolov8n.pt\\n        ```\\n\\n    === \"Custom\"\\n\\n        Validate a custom-trained model.\\n        ```bash\\n        yolo detect val model=path/to/best.pt\\n        ```\\n\\n## Predict\\n\\nUse a trained YOLOv8n model to run predictions on images.\\n\\n!!! example \"Example\"\\n\\n    === \"Official\"\\n\\n        Predict with an official YOLOv8n model.\\n        ```bash\\n        yolo detect predict model=yolov8n.pt source=\\'https://ultralytics.com/images/bus.jpg\\'\\n        ```\\n\\n    === \"Custom\"\\n\\n        Predict with a custom model.\\n        ```bash\\n        yolo detect predict model=path/to/best.pt source=\\'https://ultralytics.com/images/bus.jpg\\'\\n        ```\\n\\n## Export\\n\\nExport a YOLOv8n model to a different format like ONNX, CoreML, etc.\\n\\n!!! example \"Example\"\\n\\n    === \"Official\"\\n\\n        Export an official YOLOv8n model to ONNX format.\\n        ```bash\\n        yolo export model=yolov8n.pt format=onnx\\n        ```\\n\\n    === \"Custom\"\\n\\n        Export a custom-trained model to ONNX format.\\n        ```bash\\n        yolo export model=path/to/best.pt format=onnx\\n        ```\\n\\nAvailable YOLOv8 export formats are in the table below. You can export to any format using the `format` argument,\\ni.e. `format=\\'onnx\\'` or `format=\\'engine\\'`.\\n\\n| Format                                                             | `format` Argument | Model                     | Metadata |\\n|--------------------------------------------------------------------|-------------------|---------------------------|----------|\\n| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | ✅        |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | ✅        |\\n| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | ✅        |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | ✅        |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | ✅        |\\n| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | ✅        |\\n| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | ✅        |\\n| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | ❌        |\\n| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | ✅        |\\n| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ✅        |\\n| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | ✅        |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | ✅        |\\n\\n---\\n\\n## Overriding default arguments\\n\\nDefault arguments can be overridden by simply passing them as arguments in the CLI in `arg=value` pairs.\\n\\n!!! tip \"\"\\n\\n    === \"Train\"\\n        Train a detection model for `10 epochs` with `learning_rate` of `0.01`\\n        ```bash\\n        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\\n        ```\\n\\n    === \"Predict\"\\n        Predict a YouTube video using a pretrained segmentation model at image size 320:\\n        ```bash\\n        yolo segment predict model=yolov8n-seg.pt source=\\'https://youtu.be/Zgi9g1ksQHc\\' imgsz=320\\n        ```\\n\\n    === \"Val\"\\n        Validate a pretrained detection model at batch-size 1 and image size 640:\\n        ```bash\\n        yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\\n        ```\\n\\n---\\n\\n## Overriding default config file\\n\\nYou can override the `default.yaml` config file entirely by passing a new file with the `cfg` arguments,\\ni.e. `cfg=custom.yaml`.\\n\\nTo do this first create a copy of `default.yaml` in your current working dir with the `yolo copy-cfg` command.\\n\\nThis will create `default_copy.yaml`, which you can then pass as `cfg=default_copy.yaml` along with any additional args,\\nlike `imgsz=320` in this example:\\n\\n!!! example \"\"\\n\\n    === \"CLI\"\\n        ```bash\\n        yolo copy-cfg\\n        yolo cfg=default_copy.yaml imgsz=320\\n        ```\\n', metadata={'file_path': 'docs/usage/cli.md', 'file_name': 'cli.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\nBoth the Ultralytics YOLO command-line and python interfaces are simply a high-level abstraction on the base engine\\nexecutors. Let\\'s take a look at the Trainer engine.\\n\\n## BaseTrainer\\n\\nBaseTrainer contains the generic boilerplate training routine. It can be customized for any task based over overriding\\nthe required functions or operations as long the as correct formats are followed. For example, you can support your own\\ncustom model and dataloader by just overriding these functions:\\n\\n* `get_model(cfg, weights)` - The function that builds the model to be trained\\n* `get_dataloder()` - The function that builds the dataloader\\n  More details and source code can be found in [`BaseTrainer` Reference](../reference/yolo/engine/trainer.md)\\n\\n## DetectionTrainer\\n\\nHere\\'s how you can use the YOLOv8 `DetectionTrainer` and customize it.\\n\\n```python\\nfrom ultralytics.yolo.v8.detect import DetectionTrainer\\n\\ntrainer = DetectionTrainer(overrides={...})\\ntrainer.train()\\ntrained_model = trainer.best  # get best model\\n```\\n\\n### Customizing the DetectionTrainer\\n\\nLet\\'s customize the trainer **to train a custom detection model** that is not supported directly. You can do this by\\nsimply overloading the existing the `get_model` functionality:\\n\\n```python\\nfrom ultralytics.yolo.v8.detect import DetectionTrainer\\n\\n\\nclass CustomTrainer(DetectionTrainer):\\n    def get_model(self, cfg, weights):\\n        ...\\n\\n\\ntrainer = CustomTrainer(overrides={...})\\ntrainer.train()\\n```\\n\\nYou now realize that you need to customize the trainer further to:\\n\\n* Customize the `loss function`.\\n* Add `callback` that uploads model to your Google Drive after every 10 `epochs`\\n  Here\\'s how you can do it:\\n\\n```python\\nfrom ultralytics.yolo.v8.detect import DetectionTrainer\\n\\n\\nclass CustomTrainer(DetectionTrainer):\\n    def get_model(self, cfg, weights):\\n        ...\\n\\n    def criterion(self, preds, batch):\\n        # get ground truth\\n        imgs = batch[\"imgs\"]\\n        bboxes = batch[\"bboxes\"]\\n        ...\\n        return loss, loss_items  # see Reference-> Trainer for details on the expected format\\n\\n\\n# callback to upload model weights\\ndef log_model(trainer):\\n    last_weight_path = trainer.last\\n    ...\\n\\n\\ntrainer = CustomTrainer(overrides={...})\\ntrainer.add_callback(\"on_train_epoch_end\", log_model)  # Adds to existing callback\\ntrainer.train()\\n```\\n\\nTo know more about Callback triggering events and entry point, checkout our [Callbacks Guide](callbacks.md)\\n\\n## Other engine components\\n\\nThere are other components that can be customized similarly like `Validators` and `Predictors`\\nSee Reference section for more information on these.\\n\\n', metadata={'file_path': 'docs/usage/engine.md', 'file_name': 'engine.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Hyperparameter Tuning with Ray Tune and YOLOv8\\n\\nHyperparameter tuning (or hyperparameter optimization) is the process of determining the right combination of hyperparameters that maximizes model performance. It works by running multiple trials in a single training process, evaluating the performance of each trial, and selecting the best hyperparameter values based on the evaluation results.\\n\\n## Ultralytics YOLOv8 and Ray Tune Integration\\n\\n[Ultralytics](https://ultralytics.com) YOLOv8 integrates hyperparameter tuning with Ray Tune, allowing you to easily optimize your YOLOv8 model\\'s hyperparameters. By using Ray Tune, you can leverage advanced search algorithms, parallelism, and early stopping to speed up the tuning process and achieve better model performance.\\n\\n###  Ray Tune\\n\\n<div align=\"center\">\\n<a href=\"https://docs.ray.io/en/latest/tune/index.html\" target=\"_blank\">\\n<img width=\"480\" src=\"https://docs.ray.io/en/latest/_images/tune_overview.png\"></a>\\n</div>\\n\\n[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) is a powerful and flexible hyperparameter tuning library for machine learning models. It provides an efficient way to optimize hyperparameters by supporting various search algorithms, parallelism, and early stopping strategies. Ray Tune\\'s flexible architecture enables seamless integration with popular machine learning frameworks, including Ultralytics YOLOv8.\\n\\n### Weights & Biases\\n\\nYOLOv8 also supports optional integration with [Weights & Biases](https://wandb.ai/site) (wandb) for tracking the tuning progress.\\n\\n## Installation\\n\\nTo install the required packages, run:\\n\\n!!! tip \"Installation\"\\n\\n    ```bash\\n    pip install -U ultralytics \"ray[tune]\"  # install and/or update\\n    pip install wandb  # optional\\n    ```\\n\\n## Usage\\n\\n!!! example \"Usage\"\\n\\n    ```python\\n    from ultralytics import YOLO\\n\\n    model = YOLO(\"yolov8n.pt\")\\n    results = model.tune(data=\"coco128.yaml\")\\n    ```\\n\\n## `tune()` Method Parameters\\n\\nThe `tune()` method in YOLOv8 provides an easy-to-use interface for hyperparameter tuning with Ray Tune. It accepts several arguments that allow you to customize the tuning process. Below is a detailed explanation of each parameter:\\n\\n| Parameter       | Type           | Description                                                                                                                                                                                                                                                                                                                   | Default Value |\\n|-----------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\\n| `data`          | str            | The dataset configuration file (in YAML format) to run the tuner on. This file should specify the training and validation data paths, as well as other dataset-specific settings.                                                                                                                                             |               |\\n| `space`         | dict, optional | A dictionary defining the hyperparameter search space for Ray Tune. Each key corresponds to a hyperparameter name, and the value specifies the range of values to explore during tuning. If not provided, YOLOv8 uses a default search space with various hyperparameters.                                                    |               |\\n| `grace_period`  | int, optional  | The grace period in epochs for the [ASHA scheduler](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-asha) in Ray Tune. The scheduler will not terminate any trial before this number of epochs, allowing the model to have some minimum training before making a decision on early stopping. | 10            |\\n| `gpu_per_trial` | int, optional  | The number of GPUs to allocate per trial during tuning. This helps manage GPU usage, particularly in multi-GPU environments. If not provided, the tuner will use all available GPUs.                                                                                                                                          | None          |\\n| `max_samples`   | int, optional  | The maximum number of trials to run during tuning. This parameter helps control the total number of hyperparameter combinations tested, ensuring the tuning process does not run indefinitely.                                                                                                                                | 10            |\\n| `train_args`    | dict, optional | A dictionary of additional arguments to pass to the `train()` method during tuning. These arguments can include settings like the number of training epochs, batch size, and other training-specific configurations.                                                                                                          | {}            |\\n\\nBy customizing these parameters, you can fine-tune the hyperparameter optimization process to suit your specific needs and available computational resources.\\n\\n## Default Search Space Description\\n\\nThe following table lists the default search space parameters for hyperparameter tuning in YOLOv8 with Ray Tune. Each parameter has a specific value range defined by `tune.uniform()`.\\n\\n| Parameter       | Value Range                | Description                              |\\n|-----------------|----------------------------|------------------------------------------|\\n| lr0             | `tune.uniform(1e-5, 1e-1)` | Initial learning rate                    |\\n| lrf             | `tune.uniform(0.01, 1.0)`  | Final learning rate factor               |\\n| momentum        | `tune.uniform(0.6, 0.98)`  | Momentum                                 |\\n| weight_decay    | `tune.uniform(0.0, 0.001)` | Weight decay                             |\\n| warmup_epochs   | `tune.uniform(0.0, 5.0)`   | Warmup epochs                            |\\n| warmup_momentum | `tune.uniform(0.0, 0.95)`  | Warmup momentum                          |\\n| box             | `tune.uniform(0.02, 0.2)`  | Box loss weight                          |\\n| cls             | `tune.uniform(0.2, 4.0)`   | Class loss weight                        |\\n| hsv_h           | `tune.uniform(0.0, 0.1)`   | Hue augmentation range                   |\\n| hsv_s           | `tune.uniform(0.0, 0.9)`   | Saturation augmentation range            |\\n| hsv_v           | `tune.uniform(0.0, 0.9)`   | Value (brightness) augmentation range    |\\n| degrees         | `tune.uniform(0.0, 45.0)`  | Rotation augmentation range (degrees)    |\\n| translate       | `tune.uniform(0.0, 0.9)`   | Translation augmentation range           |\\n| scale           | `tune.uniform(0.0, 0.9)`   | Scaling augmentation range               |\\n| shear           | `tune.uniform(0.0, 10.0)`  | Shear augmentation range (degrees)       |\\n| perspective     | `tune.uniform(0.0, 0.001)` | Perspective augmentation range           |\\n| flipud          | `tune.uniform(0.0, 1.0)`   | Vertical flip augmentation probability   |\\n| fliplr          | `tune.uniform(0.0, 1.0)`   | Horizontal flip augmentation probability |\\n| mosaic          | `tune.uniform(0.0, 1.0)`   | Mosaic augmentation probability          |\\n| mixup           | `tune.uniform(0.0, 1.0)`   | Mixup augmentation probability           |\\n| copy_paste      | `tune.uniform(0.0, 1.0)`   | Copy-paste augmentation probability      |\\n\\n\\n## Custom Search Space Example\\n\\nIn this example, we demonstrate how to use a custom search space for hyperparameter tuning with Ray Tune and YOLOv8. By providing a custom search space, you can focus the tuning process on specific hyperparameters of interest.\\n\\n!!! example \"Usage\"\\n\\n    ```python\\n    from ultralytics import YOLO\\n    from ray import tune\\n    \\n    model = YOLO(\"yolov8n.pt\")\\n    result = model.tune(\\n        data=\"coco128.yaml\",\\n        space={\"lr0\": tune.uniform(1e-5, 1e-1)},\\n        train_args={\"epochs\": 50}\\n    )\\n    ```\\n\\nIn the code snippet above, we create a YOLO model with the \"yolov8n.pt\" pretrained weights. Then, we call the `tune()` method, specifying the dataset configuration with \"coco128.yaml\". We provide a custom search space for the initial learning rate `lr0` using a dictionary with the key \"lr0\" and the value `tune.uniform(1e-5, 1e-1)`. Finally, we pass additional training arguments, such as the number of epochs, using the `train_args` parameter.', metadata={'file_path': 'docs/usage/hyperparameter_tuning.md', 'file_name': 'hyperparameter_tuning.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Python Usage\\n\\nWelcome to the YOLOv8 Python Usage documentation! This guide is designed to help you seamlessly integrate YOLOv8 into\\nyour Python projects for object detection, segmentation, and classification. Here, you\\'ll learn how to load and use\\npretrained models, train new models, and perform predictions on images. The easy-to-use Python interface is a valuable\\nresource for anyone looking to incorporate YOLOv8 into their Python projects, allowing you to quickly implement advanced\\nobject detection capabilities. Let\\'s get started!\\n\\nFor example, users can load a model, train it, evaluate its performance on a validation set, and even export it to ONNX\\nformat with just a few lines of code.\\n\\n!!! example \"Python\"\\n\\n    ```python\\n    from ultralytics import YOLO\\n    \\n    # Create a new YOLO model from scratch\\n    model = YOLO(\\'yolov8n.yaml\\')\\n    \\n    # Load a pretrained YOLO model (recommended for training)\\n    model = YOLO(\\'yolov8n.pt\\')\\n    \\n    # Train the model using the \\'coco128.yaml\\' dataset for 3 epochs\\n    results = model.train(data=\\'coco128.yaml\\', epochs=3)\\n    \\n    # Evaluate the model\\'s performance on the validation set\\n    results = model.val()\\n    \\n    # Perform object detection on an image using the model\\n    results = model(\\'https://ultralytics.com/images/bus.jpg\\')\\n    \\n    # Export the model to ONNX format\\n    success = model.export(format=\\'onnx\\')\\n    ```\\n\\n## [Train](../modes/train.md)\\n\\nTrain mode is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the\\nspecified dataset and hyperparameters. The training process involves optimizing the model\\'s parameters so that it can\\naccurately predict the classes and locations of objects in an image.\\n\\n!!! example \"Train\"\\n\\n    === \"From pretrained(recommended)\"\\n        ```python\\n        from ultralytics import YOLO\\n\\n        model = YOLO(\\'yolov8n.pt\\') # pass any model type\\n        model.train(epochs=5)\\n        ```\\n\\n    === \"From scratch\"\\n        ```python\\n        from ultralytics import YOLO\\n\\n        model = YOLO(\\'yolov8n.yaml\\')\\n        model.train(data=\\'coco128.yaml\\', epochs=5)\\n        ```\\n\\n    === \"Resume\"\\n        ```python\\n        model = YOLO(\"last.pt\")\\n        model.train(resume=True)\\n        ```\\n\\n[Train Examples](../modes/train.md){ .md-button .md-button--primary}\\n\\n## [Val](../modes/val.md)\\n\\nVal mode is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a\\nvalidation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters\\nof the model to improve its performance.\\n\\n!!! example \"Val\"\\n\\n    === \"Val after training\"\\n        ```python\\n          from ultralytics import YOLO\\n\\n          model = YOLO(\\'yolov8n.yaml\\')\\n          model.train(data=\\'coco128.yaml\\', epochs=5)\\n          model.val()  # It\\'ll automatically evaluate the data you trained.\\n        ```\\n\\n    === \"Val independently\"\\n        ```python\\n          from ultralytics import YOLO\\n\\n          model = YOLO(\"model.pt\")\\n          # It\\'ll use the data yaml file in model.pt if you don\\'t set data.\\n          model.val()\\n          # or you can set the data you want to val\\n          model.val(data=\\'coco128.yaml\\')\\n        ```\\n\\n[Val Examples](../modes/val.md){ .md-button .md-button--primary}\\n\\n## [Predict](../modes/predict.md)\\n\\nPredict mode is used for making predictions using a trained YOLOv8 model on new images or videos. In this mode, the\\nmodel is loaded from a checkpoint file, and the user can provide images or videos to perform inference. The model\\npredicts the classes and locations of objects in the input images or videos.\\n\\n!!! example \"Predict\"\\n\\n    === \"From source\"\\n        ```python\\n        from ultralytics import YOLO\\n        from PIL import Image\\n        import cv2\\n\\n        model = YOLO(\"model.pt\")\\n        # accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\\n        results = model.predict(source=\"0\")\\n        results = model.predict(source=\"folder\", show=True) # Display preds. Accepts all YOLO predict arguments\\n\\n        # from PIL\\n        im1 = Image.open(\"bus.jpg\")\\n        results = model.predict(source=im1, save=True)  # save plotted images\\n\\n        # from ndarray\\n        im2 = cv2.imread(\"bus.jpg\")\\n        results = model.predict(source=im2, save=True, save_txt=True)  # save predictions as labels\\n\\n        # from list of PIL/ndarray\\n        results = model.predict(source=[im1, im2])\\n        ```\\n\\n    === \"Results usage\"\\n        ```python\\n        # results would be a list of Results object including all the predictions by default\\n        # but be careful as it could occupy a lot memory when there\\'re many images, \\n        # especially the task is segmentation.\\n        # 1. return as a list\\n        results = model.predict(source=\"folder\")\\n\\n        # results would be a generator which is more friendly to memory by setting stream=True\\n        # 2. return as a generator\\n        results = model.predict(source=0, stream=True)\\n\\n        for result in results:\\n            # Detection\\n            result.boxes.xyxy   # box with xyxy format, (N, 4)\\n            result.boxes.xywh   # box with xywh format, (N, 4)\\n            result.boxes.xyxyn  # box with xyxy format but normalized, (N, 4)\\n            result.boxes.xywhn  # box with xywh format but normalized, (N, 4)\\n            result.boxes.conf   # confidence score, (N, 1)\\n            result.boxes.cls    # cls, (N, 1)\\n\\n            # Segmentation\\n            result.masks.data      # masks, (N, H, W)\\n            result.masks.xy        # x,y segments (pixels), List[segment] * N\\n            result.masks.xyn       # x,y segments (normalized), List[segment] * N\\n\\n            # Classification\\n            result.probs     # cls prob, (num_class, )\\n\\n        # Each result is composed of torch.Tensor by default, \\n        # in which you can easily use following functionality:\\n        result = result.cuda()\\n        result = result.cpu()\\n        result = result.to(\"cpu\")\\n        result = result.numpy()\\n        ```\\n\\n[Predict Examples](../modes/predict.md){ .md-button .md-button--primary}\\n\\n## [Export](../modes/export.md)\\n\\nExport mode is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the model is\\nconverted to a format that can be used by other software applications or hardware devices. This mode is useful when\\ndeploying the model to production environments.\\n\\n!!! example \"Export\"\\n\\n    === \"Export to ONNX\"\\n\\n        Export an official YOLOv8n model to ONNX with dynamic batch-size and image-size.\\n        ```python\\n          from ultralytics import YOLO\\n\\n          model = YOLO(\\'yolov8n.pt\\')\\n          model.export(format=\\'onnx\\', dynamic=True)\\n        ```\\n\\n    === \"Export to TensorRT\"\\n\\n        Export an official YOLOv8n model to TensorRT on `device=0` for acceleration on CUDA devices.\\n        ```python\\n          from ultralytics import YOLO\\n\\n          model = YOLO(\\'yolov8n.pt\\')\\n          model.export(format=\\'onnx\\', device=0)\\n        ```\\n\\n[Export Examples](../modes/export.md){ .md-button .md-button--primary}\\n\\n## [Track](../modes/track.md)\\n\\nTrack mode is used for tracking objects in real-time using a YOLOv8 model. In this mode, the model is loaded from a\\ncheckpoint file, and the user can provide a live video stream to perform real-time object tracking. This mode is useful\\nfor applications such as surveillance systems or self-driving cars.\\n\\n!!! example \"Track\"\\n\\n    === \"Python\"\\n    \\n        ```python\\n        from ultralytics import YOLO\\n        \\n        # Load a model\\n        model = YOLO(\\'yolov8n.pt\\')  # load an official detection model\\n        model = YOLO(\\'yolov8n-seg.pt\\')  # load an official segmentation model\\n        model = YOLO(\\'path/to/best.pt\\')  # load a custom model\\n        \\n        # Track with the model\\n        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True) \\n        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True, tracker=\"bytetrack.yaml\") \\n        ```\\n\\n[Track Examples](../modes/track.md){ .md-button .md-button--primary}\\n\\n## [Benchmark](../modes/benchmark.md)\\n\\nBenchmark mode is used to profile the speed and accuracy of various export formats for YOLOv8. The benchmarks provide\\ninformation on the size of the exported format, its `mAP50-95` metrics (for object detection and segmentation)\\nor `accuracy_top5` metrics (for classification), and the inference time in milliseconds per image across various export\\nformats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for\\ntheir specific use case based on their requirements for speed and accuracy.\\n\\n!!! example \"Benchmark\"\\n\\n    === \"Python\"\\n    \\n        Benchmark an official YOLOv8n model across all export formats.\\n        ```python\\n        from ultralytics.yolo.utils.benchmarks import benchmark\\n        \\n        # Benchmark\\n        benchmark(model=\\'yolov8n.pt\\', imgsz=640, half=False, device=0)\\n        ```\\n\\n[Benchmark Examples](../modes/benchmark.md){ .md-button .md-button--primary}\\n\\n## Using Trainers\\n\\n`YOLO` model class is a high-level wrapper on the Trainer classes. Each YOLO task has its own trainer that inherits\\nfrom `BaseTrainer`.\\n\\n!!! tip \"Detection Trainer Example\"\\n\\n        ```python\\n        from ultralytics.yolo import v8 import DetectionTrainer, DetectionValidator, DetectionPredictor\\n\\n        # trainer\\n        trainer = DetectionTrainer(overrides={})\\n        trainer.train()\\n        trained_model = trainer.best\\n\\n        # Validator\\n        val = DetectionValidator(args=...)\\n        val(model=trained_model)\\n\\n        # predictor\\n        pred = DetectionPredictor(overrides={})\\n        pred(source=SOURCE, model=trained_model)\\n\\n        # resume from last weight\\n        overrides[\"resume\"] = trainer.last\\n        trainer = detect.DetectionTrainer(overrides=overrides)\\n        ```\\n\\nYou can easily customize Trainers to support custom tasks or explore R&D ideas.\\nLearn more about Customizing `Trainers`, `Validators` and `Predictors` to suit your project needs in the Customization\\nSection.\\n\\n[Customization tutorials](engine.md){ .md-button .md-button--primary}\\n', metadata={'file_path': 'docs/usage/python.md', 'file_name': 'python.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Ultralytics YOLOv5\\n\\n<div align=\"center\">\\n  <p>\\n    <a href=\"https://ultralytics.com/yolov5\" target=\"_blank\">\\n    <img width=\"100%\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov5/v70/splash.png\"></a>\\n  </p>\\n\\n  <a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n  <a href=\"https://zenodo.org/badge/latestdoi/264818686\"><img src=\"https://zenodo.org/badge/264818686.svg\" alt=\"YOLOv5 Citation\"></a>\\n  <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n  <br>\\n  <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a>\\n  <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\\n  <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n<br>\\n<br>\\n\\nWelcome to the Ultralytics YOLOv5 🚀 Docs! YOLOv5, or You Only Look Once version 5, is an Ultralytics object detection model designed to deliver fast and accurate real-time results.\\n<br><br>\\nThis powerful deep learning framework is built on the PyTorch platform and has gained immense popularity due to its ease of use, high performance, and versatility. In this documentation, we will guide you through the installation process, explain the model\\'s architecture, showcase various use-cases, and provide detailed tutorials to help you harness the full potential of YOLOv5 for your computer vision projects. Let\\'s dive in!\\n\\n</div>\\n\\n## Tutorials\\n\\n* [Train Custom Data](tutorials/train_custom_data.md) 🚀 RECOMMENDED\\n* [Tips for Best Training Results](tutorials/tips_for_best_training_results.md) ☘️\\n* [Multi-GPU Training](tutorials/multi_gpu_training.md)\\n* [PyTorch Hub](tutorials/pytorch_hub_model_loading.md) 🌟 NEW\\n* [TFLite, ONNX, CoreML, TensorRT Export](tutorials/model_export.md) 🚀\\n* [NVIDIA Jetson platform Deployment](tutorials/running_on_jetson_nano.md) 🌟 NEW\\n* [Test-Time Augmentation (TTA)](tutorials/test_time_augmentation.md)\\n* [Model Ensembling](tutorials/model_ensembling.md)\\n* [Model Pruning/Sparsity](tutorials/model_pruning_and_sparsity.md)\\n* [Hyperparameter Evolution](tutorials/hyperparameter_evolution.md)\\n* [Transfer Learning with Frozen Layers](tutorials/transfer_learning_with_frozen_layers.md)\\n* [Architecture Summary](tutorials/architecture_description.md) 🌟 NEW\\n* [Roboflow for Datasets, Labeling, and Active Learning](tutorials/roboflow_datasets_integration.md)\\n* [ClearML Logging](tutorials/clearml_logging_integration.md) 🌟 NEW\\n* [YOLOv5 with Neural Magic\\'s Deepsparse](tutorials/neural_magic_pruning_quantization.md) 🌟 NEW\\n* [Comet Logging](tutorials/comet_logging_integration.md) 🌟 NEW\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies\\nincluding [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/)\\nand [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free\\n  GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM.\\n  See [GCP Quickstart Guide](environments/google_cloud_quickstart_tutorial.md)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](environments/aws_quickstart_tutorial.md)\\n- **Docker Image**.\\n  See [Docker Quickstart Guide](environments/docker_image_quickstart_tutorial.md) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous\\nIntegration (CI) tests are currently passing. CI tests verify correct operation of\\nYOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py)\\nand [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24\\nhours and on every commit.\\n\\n<br>\\n<div align=\"center\">\\n  <a href=\"https://github.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-github.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.linkedin.com/company/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-linkedin.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://twitter.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-twitter.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://youtube.com/ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-youtube.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.tiktok.com/@ultralytics\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-tiktok.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://www.instagram.com/ultralytics/\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-social-instagram.png\" width=\"3%\" alt=\"\" /></a>\\n  <img src=\"https://github.com/ultralytics/assets/raw/main/social/logo-transparent.png\" width=\"3%\" alt=\"\" />\\n  <a href=\"https://discord.gg/n6cFeSPZdD\" style=\"text-decoration:none;\">\\n    <img src=\"https://github.com/ultralytics/assets/blob/main/social/logo-social-discord.png\" width=\"3%\" alt=\"\" /></a>\\n</div>', metadata={'file_path': 'docs/yolov5/index.md', 'file_name': 'index.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# YOLOv5 Quickstart\\n\\nSee below for quickstart examples.\\n\\n## Install\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a\\n[**Python>=3.7.0**](https://www.python.org/) environment, including\\n[**PyTorch>=1.7**](https://pytorch.org/get-started/locally/).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n\\n\\n## Inference\\n\\nYOLOv5 [PyTorch Hub](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading) inference. [Models](https://github.com/ultralytics/yolov5/tree/master/models) download automatically from the latest\\nYOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```python\\nimport torch\\n\\n# Model\\nmodel = torch.hub.load(\"ultralytics/yolov5\", \"yolov5s\")  # or yolov5n - yolov5x6, custom\\n\\n# Images\\nimg = \"https://ultralytics.com/images/zidane.jpg\"  # or file, Path, PIL, OpenCV, numpy, list\\n\\n# Inference\\nresults = model(img)\\n\\n# Results\\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.\\n```\\n\\n## Inference with detect.py\\n\\n`detect.py` runs inference on a variety of sources, downloading [models](https://github.com/ultralytics/yolov5/tree/master/models) automatically from\\nthe latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases) and saving results to `runs/detect`.\\n\\n```bash\\npython detect.py --weights yolov5s.pt --source 0                               # webcam\\n                                               img.jpg                         # image\\n                                               vid.mp4                         # video\\n                                               screen                          # screenshot\\n                                               path/                           # directory\\n                                               list.txt                        # list of images\\n                                               list.streams                    # list of streams\\n                                               \\'path/*.jpg\\'                    # glob\\n                                               \\'https://youtu.be/Zgi9g1ksQHc\\'  # YouTube\\n                                               \\'rtsp://example.com/media.mp4\\'  # RTSP, RTMP, HTTP stream\\n```\\n\\n## Training\\n\\nThe commands below reproduce YOLOv5 [COCO](https://github.com/ultralytics/yolov5/blob/master/data/scripts/get_coco.sh)\\nresults. [Models](https://github.com/ultralytics/yolov5/tree/master/models)\\nand [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest\\nYOLOv5 [release](https://github.com/ultralytics/yolov5/releases). Training times for YOLOv5n/s/m/l/x are\\n1/2/4/6/8 days on a V100 GPU ([Multi-GPU](https://docs.ultralytics.com/yolov5/tutorials/multi_gpu_training) times faster). Use the\\nlargest `--batch-size` possible, or pass `--batch-size -1` for\\nYOLOv5 [AutoBatch](https://github.com/ultralytics/yolov5/pull/5092). Batch sizes shown for V100-16GB.\\n\\n```bash\\npython train.py --data coco.yaml --epochs 300 --weights \\'\\' --cfg yolov5n.yaml  --batch-size 128\\n                                                                 yolov5s                    64\\n                                                                 yolov5m                    40\\n                                                                 yolov5l                    24\\n                                                                 yolov5x                    16\\n```\\n\\n<img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/90222759-949d8800-ddc1-11ea-9fa1-1c97eed2b963.png\">\\n', metadata={'file_path': 'docs/yolov5/quickstart_tutorial.md', 'file_name': 'quickstart_tutorial.md', 'file_type': '.md'}), Document(page_content=\"# YOLOv8/YOLOv5 Inference C++\\n\\nThis example demonstrates how to perform inference using YOLOv8 and YOLOv5 models in C++ with OpenCV's DNN API.\\n\\n## Usage\\n\\n```commandline\\ngit clone ultralytics\\ncd ultralytics\\npip install .\\ncd examples/cpp_\\n\\n# Add a **yolov8\\\\_.onnx** and/or **yolov5\\\\_.onnx** model(s) to the ultralytics folder.\\n# Edit the **main.cpp** to change the **projectBasePath** to match your user.\\n\\n# Note that by default the CMake file will try and import the CUDA library to be used with the OpenCVs dnn (cuDNN) GPU Inference.\\n# If your OpenCV build does not use CUDA/cuDNN you can remove that import call and run the example on CPU.\\n\\nmkdir build\\ncd build\\ncmake ..\\nmake\\n./Yolov8CPPInference\\n```\\n\\n## Exporting YOLOv8 and YOLOv5 Models\\n\\nTo export YOLOv8 models:\\n\\n```commandline\\nyolo export model=yolov8s.pt imgsz=480,640 format=onnx opset=12\\n```\\n\\nTo export YOLOv5 models:\\n\\n```commandline\\npython3 export.py --weights yolov5s.pt --img 480 640 --include onnx --opset 12\\n```\\n\\nyolov8s.onnx:\\n\\n![image](https://user-images.githubusercontent.com/40023722/217356132-a4cecf2e-2729-4acb-b80a-6559022d7707.png)\\n\\nyolov5s.onnx:\\n\\n![image](https://user-images.githubusercontent.com/40023722/217357005-07464492-d1da-42e3-98a7-fc753f87d5e6.png)\\n\\nThis repository utilizes OpenCV's DNN API to run ONNX exported models of YOLOv5 and YOLOv8. In theory, it should work for YOLOv6 and YOLOv7 as well, but they have not been tested. Note that the example networks are exported with rectangular (640x480) resolutions, but any exported resolution will work. You may want to use the letterbox approach for square images, depending on your use case.\\n\\nThe **main** branch version uses Qt as a GUI wrapper. The primary focus here is the **Inference** class file, which demonstrates how to transpose YOLOv8 models to work as YOLOv5 models.\\n\", metadata={'file_path': 'examples/YOLOv8-CPP-Inference/README.md', 'file_name': 'README.md', 'file_type': '.md'}), Document(page_content='# YOLOv8 - OpenCV\\n\\nImplementation YOLOv8 on OpenCV using ONNX Format.\\n\\nJust simply clone and run\\n\\n```bash\\npip install -r requirements.txt\\npython main.py --model yolov8n.onnx --img image.jpg\\n```\\n\\nIf you start from scratch:\\n\\n```bash\\npip install ultralytics\\nyolo export model=yolov8n.pt imgsz=640 format=onnx opset=12\\n```\\n\\n_\\\\*Make sure to include \"opset=12\"_\\n', metadata={'file_path': 'examples/YOLOv8-OpenCV-ONNX-Python/README.md', 'file_name': 'README.md', 'file_type': '.md'}), Document(page_content='## Models\\n\\nWelcome to the Ultralytics Models directory! Here you will find a wide variety of pre-configured model configuration\\nfiles (`*.yaml`s) that can be used to create custom YOLO models. The models in this directory have been expertly crafted\\nand fine-tuned by the Ultralytics team to provide the best performance for a wide range of object detection and image\\nsegmentation tasks.\\n\\nThese model configurations cover a wide range of scenarios, from simple object detection to more complex tasks like\\ninstance segmentation and object tracking. They are also designed to run efficiently on a variety of hardware platforms,\\nfrom CPUs to GPUs. Whether you are a seasoned machine learning practitioner or just getting started with YOLO, this\\ndirectory provides a great starting point for your custom model development needs.\\n\\nTo get started, simply browse through the models in this directory and find one that best suits your needs. Once you\\'ve\\nselected a model, you can use the provided `*.yaml` file to train and deploy your custom YOLO model with ease. See full\\ndetails at the Ultralytics [Docs](https://docs.ultralytics.com/models), and if you need help or have any questions, feel free\\nto reach out to the Ultralytics team for support. So, don\\'t wait, start creating your custom YOLO model now!\\n\\n### Usage\\n\\nModel `*.yaml` files may be used directly in the Command Line Interface (CLI) with a `yolo` command:\\n\\n```bash\\nyolo task=detect mode=train model=yolov8n.yaml data=coco128.yaml epochs=100\\n```\\n\\nThey may also be used directly in a Python environment, and accepts the same\\n[arguments](https://docs.ultralytics.com/usage/cfg/) as in the CLI example above:\\n\\n```python\\nfrom ultralytics import YOLO\\n\\nmodel = YOLO(\"model.yaml\")  # build a YOLOv8n model from scratch\\n# YOLO(\"model.pt\")  use pre-trained model if available\\nmodel.info()  # display model information\\nmodel.train(data=\"coco128.yaml\", epochs=100)  # train the model\\n```\\n\\n## Pre-trained Model Architectures\\n\\nUltralytics supports many model architectures. Visit https://docs.ultralytics.com/models to view detailed information\\nand usage. Any of these models can be used by loading their configs or pretrained checkpoints if available.\\n\\n## Contributing New Models\\n\\nIf you\\'ve developed a new model architecture or have improvements for existing models that you\\'d like to contribute to the Ultralytics community, please submit your contribution in a new Pull Request. For more details, visit our [Contributing Guide](https://docs.ultralytics.com/help/contributing).\\n', metadata={'file_path': 'ultralytics/models/README.md', 'file_name': 'README.md', 'file_type': '.md'}), Document(page_content='# Tracker\\n\\n## Supported Trackers\\n\\n- [x] ByteTracker\\n- [x] BoT-SORT\\n\\n## Usage\\n\\n### python interface:\\n\\nYou can use the Python interface to track objects using the YOLO model.\\n\\n```python\\nfrom ultralytics import YOLO\\n\\nmodel = YOLO(\"yolov8n.pt\")  # or a segmentation model .i.e yolov8n-seg.pt\\nmodel.track(\\n    source=\"video/streams\",\\n    stream=True,\\n    tracker=\"botsort.yaml\",  # or \\'bytetrack.yaml\\'\\n    show=True,\\n)\\n```\\n\\nYou can get the IDs of the tracked objects using the following code:\\n\\n```python\\nfrom ultralytics import YOLO\\n\\nmodel = YOLO(\"yolov8n.pt\")\\n\\nfor result in model.track(source=\"video.mp4\"):\\n    print(\\n        result.boxes.id.cpu().numpy().astype(int)\\n    )  # this will print the IDs of the tracked objects in the frame\\n```\\n\\nIf you want to use the tracker with a folder of images or when you loop on the video frames, you should use the `persist` parameter to tell the model that these frames are related to each other so the IDs will be fixed for the same objects. Otherwise, the IDs will be different in each frame because in each loop, the model creates a new object for tracking, but the `persist` parameter makes it use the same object for tracking.\\n\\n```python\\nimport cv2\\nfrom ultralytics import YOLO\\n\\ncap = cv2.VideoCapture(\"video.mp4\")\\nmodel = YOLO(\"yolov8n.pt\")\\nwhile True:\\n    ret, frame = cap.read()\\n    if not ret:\\n        break\\n    results = model.track(frame, persist=True)\\n    boxes = results[0].boxes.xyxy.cpu().numpy().astype(int)\\n    ids = results[0].boxes.id.cpu().numpy().astype(int)\\n    for box, id in zip(boxes, ids):\\n        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\\n        cv2.putText(\\n            frame,\\n            f\"Id {id}\",\\n            (box[0], box[1]),\\n            cv2.FONT_HERSHEY_SIMPLEX,\\n            1,\\n            (0, 0, 255),\\n            2,\\n        )\\n    cv2.imshow(\"frame\", frame)\\n    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\\n        break\\n```\\n\\n## Change tracker parameters\\n\\nYou can change the tracker parameters by eding the `tracker.yaml` file which is located in the ultralytics/tracker/cfg folder.\\n\\n## Command Line Interface (CLI)\\n\\nYou can also use the command line interface to track objects using the YOLO model.\\n\\n```bash\\nyolo detect track source=... tracker=...\\nyolo segment track source=... tracker=...\\nyolo pose track source=... tracker=...\\n```\\n\\nBy default, trackers will use the configuration in `ultralytics/tracker/cfg`.\\nWe also support using a modified tracker config file. Please refer to the tracker config files\\nin `ultralytics/tracker/cfg`.<br>\\n', metadata={'file_path': 'ultralytics/tracker/README.md', 'file_name': 'README.md', 'file_type': '.md'}), Document(page_content='# Auth\\n---\\n:::ultralytics.hub.auth.Auth\\n<br><br>\\n', metadata={'file_path': 'docs/reference/hub/auth.md', 'file_name': 'auth.md', 'file_type': '.md'}), Document(page_content='# HUBTrainingSession\\n---\\n:::ultralytics.hub.session.HUBTrainingSession\\n<br><br>\\n', metadata={'file_path': 'docs/reference/hub/session.md', 'file_name': 'session.md', 'file_type': '.md'}), Document(page_content='# Events\\n---\\n:::ultralytics.hub.utils.Events\\n<br><br>\\n\\n# request_with_credentials\\n---\\n:::ultralytics.hub.utils.request_with_credentials\\n<br><br>\\n\\n# requests_with_progress\\n---\\n:::ultralytics.hub.utils.requests_with_progress\\n<br><br>\\n\\n# smart_request\\n---\\n:::ultralytics.hub.utils.smart_request\\n<br><br>\\n', metadata={'file_path': 'docs/reference/hub/utils.md', 'file_name': 'utils.md', 'file_type': '.md'}), Document(page_content='# AutoBackend\\n---\\n:::ultralytics.nn.autobackend.AutoBackend\\n<br><br>\\n\\n# check_class_names\\n---\\n:::ultralytics.nn.autobackend.check_class_names\\n<br><br>\\n', metadata={'file_path': 'docs/reference/nn/autobackend.md', 'file_name': 'autobackend.md', 'file_type': '.md'}), Document(page_content='# AutoShape\\n---\\n:::ultralytics.nn.autoshape.AutoShape\\n<br><br>\\n\\n# Detections\\n---\\n:::ultralytics.nn.autoshape.Detections\\n<br><br>\\n', metadata={'file_path': 'docs/reference/nn/autoshape.md', 'file_name': 'autoshape.md', 'file_type': '.md'}), Document(page_content='# Conv\\n---\\n:::ultralytics.nn.modules.Conv\\n<br><br>\\n\\n# DWConv\\n---\\n:::ultralytics.nn.modules.DWConv\\n<br><br>\\n\\n# DWConvTranspose2d\\n---\\n:::ultralytics.nn.modules.DWConvTranspose2d\\n<br><br>\\n\\n# ConvTranspose\\n---\\n:::ultralytics.nn.modules.ConvTranspose\\n<br><br>\\n\\n# DFL\\n---\\n:::ultralytics.nn.modules.DFL\\n<br><br>\\n\\n# TransformerLayer\\n---\\n:::ultralytics.nn.modules.TransformerLayer\\n<br><br>\\n\\n# TransformerBlock\\n---\\n:::ultralytics.nn.modules.TransformerBlock\\n<br><br>\\n\\n# Bottleneck\\n---\\n:::ultralytics.nn.modules.Bottleneck\\n<br><br>\\n\\n# BottleneckCSP\\n---\\n:::ultralytics.nn.modules.BottleneckCSP\\n<br><br>\\n\\n# C3\\n---\\n:::ultralytics.nn.modules.C3\\n<br><br>\\n\\n# C2\\n---\\n:::ultralytics.nn.modules.C2\\n<br><br>\\n\\n# C2f\\n---\\n:::ultralytics.nn.modules.C2f\\n<br><br>\\n\\n# ChannelAttention\\n---\\n:::ultralytics.nn.modules.ChannelAttention\\n<br><br>\\n\\n# SpatialAttention\\n---\\n:::ultralytics.nn.modules.SpatialAttention\\n<br><br>\\n\\n# CBAM\\n---\\n:::ultralytics.nn.modules.CBAM\\n<br><br>\\n\\n# C1\\n---\\n:::ultralytics.nn.modules.C1\\n<br><br>\\n\\n# C3x\\n---\\n:::ultralytics.nn.modules.C3x\\n<br><br>\\n\\n# C3TR\\n---\\n:::ultralytics.nn.modules.C3TR\\n<br><br>\\n\\n# C3Ghost\\n---\\n:::ultralytics.nn.modules.C3Ghost\\n<br><br>\\n\\n# SPP\\n---\\n:::ultralytics.nn.modules.SPP\\n<br><br>\\n\\n# SPPF\\n---\\n:::ultralytics.nn.modules.SPPF\\n<br><br>\\n\\n# Focus\\n---\\n:::ultralytics.nn.modules.Focus\\n<br><br>\\n\\n# GhostConv\\n---\\n:::ultralytics.nn.modules.GhostConv\\n<br><br>\\n\\n# GhostBottleneck\\n---\\n:::ultralytics.nn.modules.GhostBottleneck\\n<br><br>\\n\\n# Concat\\n---\\n:::ultralytics.nn.modules.Concat\\n<br><br>\\n\\n# Proto\\n---\\n:::ultralytics.nn.modules.Proto\\n<br><br>\\n\\n# Ensemble\\n---\\n:::ultralytics.nn.modules.Ensemble\\n<br><br>\\n\\n# Detect\\n---\\n:::ultralytics.nn.modules.Detect\\n<br><br>\\n\\n# MLPBlock\\n---\\n:::ultralytics.nn.modules.MLPBlock\\n<br><br>\\n\\n# LayerNorm2d\\n---\\n:::ultralytics.nn.modules.LayerNorm2d\\n<br><br>\\n\\n# Segment\\n---\\n:::ultralytics.nn.modules.Segment\\n<br><br>\\n\\n# Pose\\n---\\n:::ultralytics.nn.modules.Pose\\n<br><br>\\n\\n# Classify\\n---\\n:::ultralytics.nn.modules.Classify\\n<br><br>\\n\\n# autopad\\n---\\n:::ultralytics.nn.modules.autopad\\n<br><br>\\n', metadata={'file_path': 'docs/reference/nn/modules.md', 'file_name': 'modules.md', 'file_type': '.md'}), Document(page_content='# BaseModel\\n---\\n:::ultralytics.nn.tasks.BaseModel\\n<br><br>\\n\\n# DetectionModel\\n---\\n:::ultralytics.nn.tasks.DetectionModel\\n<br><br>\\n\\n# SegmentationModel\\n---\\n:::ultralytics.nn.tasks.SegmentationModel\\n<br><br>\\n\\n# PoseModel\\n---\\n:::ultralytics.nn.tasks.PoseModel\\n<br><br>\\n\\n# ClassificationModel\\n---\\n:::ultralytics.nn.tasks.ClassificationModel\\n<br><br>\\n\\n# torch_safe_load\\n---\\n:::ultralytics.nn.tasks.torch_safe_load\\n<br><br>\\n\\n# attempt_load_weights\\n---\\n:::ultralytics.nn.tasks.attempt_load_weights\\n<br><br>\\n\\n# attempt_load_one_weight\\n---\\n:::ultralytics.nn.tasks.attempt_load_one_weight\\n<br><br>\\n\\n# parse_model\\n---\\n:::ultralytics.nn.tasks.parse_model\\n<br><br>\\n\\n# yaml_model_load\\n---\\n:::ultralytics.nn.tasks.yaml_model_load\\n<br><br>\\n\\n# guess_model_scale\\n---\\n:::ultralytics.nn.tasks.guess_model_scale\\n<br><br>\\n\\n# guess_model_task\\n---\\n:::ultralytics.nn.tasks.guess_model_task\\n<br><br>\\n', metadata={'file_path': 'docs/reference/nn/tasks.md', 'file_name': 'tasks.md', 'file_type': '.md'}), Document(page_content='# on_predict_start\\n---\\n:::ultralytics.tracker.track.on_predict_start\\n<br><br>\\n\\n# on_predict_postprocess_end\\n---\\n:::ultralytics.tracker.track.on_predict_postprocess_end\\n<br><br>\\n\\n# register_tracker\\n---\\n:::ultralytics.tracker.track.register_tracker\\n<br><br>\\n', metadata={'file_path': 'docs/reference/tracker/track.md', 'file_name': 'track.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# YOLOv5 🚀 on AWS Deep Learning Instance: A Comprehensive Guide\\n\\nThis guide will help new users run YOLOv5 on an Amazon Web Services (AWS) Deep Learning instance. AWS offers a [Free Tier](https://aws.amazon.com/free/) and a [credit program](https://aws.amazon.com/activate/) for a quick and affordable start. \\n\\nOther quickstart options for YOLOv5 include our [Colab Notebook](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb) <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>, [GCP Deep Learning VM](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial), and our Docker image at [Docker Hub](https://hub.docker.com/r/ultralytics/yolov5) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>. *Updated: 21 April 2023*.\\n\\n## 1. AWS Console Sign-in\\n\\nCreate an account or sign in to the AWS console at [https://aws.amazon.com/console/](https://aws.amazon.com/console/) and select the **EC2** service.\\n\\n![Console](https://user-images.githubusercontent.com/26833433/106323804-debddd00-622c-11eb-997f-b8217dc0e975.png)\\n\\n## 2. Launch Instance\\n\\nIn the EC2 section of the AWS console, click the **Launch instance** button.\\n\\n![Launch](https://user-images.githubusercontent.com/26833433/106323950-204e8800-622d-11eb-915d-5c90406973ea.png)\\n\\n### Choose an Amazon Machine Image (AMI)\\n\\nEnter \\'Deep Learning\\' in the search field and select the most recent Ubuntu Deep Learning AMI (recommended), or an alternative Deep Learning AMI. For more information on selecting an AMI, see [Choosing Your DLAMI](https://docs.aws.amazon.com/dlami/latest/devguide/options.html).\\n\\n![Choose AMI](https://user-images.githubusercontent.com/26833433/106326107-c9e34880-6230-11eb-97c9-3b5fc2f4e2ff.png)\\n\\n### Select an Instance Type\\n\\nA GPU instance is recommended for most deep learning purposes. Training new models will be faster on a GPU instance than a CPU instance. Multi-GPU instances or distributed training across multiple instances with GPUs can offer sub-linear scaling. To set up distributed training, see [Distributed Training](https://docs.aws.amazon.com/dlami/latest/devguide/distributed-training.html).\\n\\n**Note:** The size of your model should be a factor in selecting an instance. If your model exceeds an instance\\'s available RAM, select a different instance type with enough memory for your application.\\n\\nRefer to [EC2 Instance Types](https://aws.amazon.com/ec2/instance-types/) and choose Accelerated Computing to see the different GPU instance options.\\n\\n![Choose Type](https://user-images.githubusercontent.com/26833433/106324624-52141e80-622e-11eb-9662-1a376d9c887d.png)\\n\\nFor more information on GPU monitoring and optimization, see [GPU Monitoring and Optimization](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-gpu.html). For pricing, see [On-Demand Pricing](https://aws.amazon.com/ec2/pricing/on-demand/) and [Spot Pricing](https://aws.amazon.com/ec2/spot/pricing/).\\n\\n### Configure Instance Details\\n\\nAmazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 70% discount compared to On-Demand prices. We recommend a persistent spot instance, which will save your data and restart automatically when spot instance availability returns after spot instance termination. For full-price On-Demand instances, leave these settings at their default values.\\n\\n![Spot Request](https://user-images.githubusercontent.com/26833433/106324835-ac14e400-622e-11eb-8853-df5ec9b16dfc.png)\\n\\nComplete Steps 4-7 to finalize your instance hardware and security settings, and then launch the instance.\\n\\n## 3. Connect to Instance\\n\\nSelect the checkbox next to your running instance, and then click Connect. Copy and paste the SSH terminal command into a terminal of your choice to connect to your instance.\\n\\n![Connect](https://user-images.githubusercontent.com/26833433/106325530-cf8c5e80-622f-11eb-9f64-5b313a9d57a1.png)\\n\\n## 4. Run YOLOv5\\n\\nOnce you have logged in to your instance, clone the repository and install the dependencies in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\nThen, start training, testing, detecting, and exporting YOLOv5 models:\\n\\n```bash\\npython train.py  # train a model\\npython val.py --weights yolov5s.pt  # validate a model for Precision, Recall, and mAP\\npython detect.py --weights yolov5s.pt --source path/to/images  # run inference on images and videos\\npython export.py --weights yolov5s.pt --include onnx coreml tflite  # export models to other formats\\n```\\n\\n## Optional Extras\\n\\nAdd 64GB of swap memory (to `--cache` large datasets):\\n\\n```bash\\nsudo fallocate -l 64G /swapfile\\nsudo chmod 600 /swapfile\\nsudo mkswap /swapfile\\nsudo swapon /swapfile\\nfree -h  # check memory\\n```\\n\\nNow you have successfully set up and run YOLOv5 on an AWS Deep Learning instance. Enjoy training, testing, and deploying your object detection models!', metadata={'file_path': 'docs/yolov5/environments/aws_quickstart_tutorial.md', 'file_name': 'aws_quickstart_tutorial.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Get Started with YOLOv5 🚀 in Docker\\n\\nThis tutorial will guide you through the process of setting up and running YOLOv5 in a Docker container. \\n\\nYou can also explore other quickstart options for YOLOv5, such as our [Colab Notebook](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb) <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>, [GCP Deep Learning VM](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial), and [Amazon AWS](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial). *Updated: 21 April 2023*.\\n\\n## Prerequisites\\n\\n1. **Nvidia Driver**: Version 455.23 or higher. Download from [Nvidia\\'s website](https://www.nvidia.com/Download/index.aspx).\\n2. **Nvidia-Docker**: Allows Docker to interact with your local GPU. Installation instructions are available on the [Nvidia-Docker GitHub repository](https://github.com/NVIDIA/nvidia-docker).\\n3. **Docker Engine - CE**: Version 19.03 or higher. Download and installation instructions can be found on the [Docker website](https://docs.docker.com/install/).\\n\\n## Step 1: Pull the YOLOv5 Docker Image\\n\\nThe Ultralytics YOLOv5 DockerHub repository is available at [https://hub.docker.com/r/ultralytics/yolov5](https://hub.docker.com/r/ultralytics/yolov5). Docker Autobuild ensures that the `ultralytics/yolov5:latest` image is always in sync with the most recent repository commit. To pull the latest image, run the following command:\\n\\n```bash\\nsudo docker pull ultralytics/yolov5:latest\\n```\\n\\n## Step 2: Run the Docker Container\\n\\n### Basic container:\\n\\nRun an interactive instance of the YOLOv5 Docker image (called a \"container\") using the `-it` flag:\\n\\n```bash\\nsudo docker run --ipc=host -it ultralytics/yolov5:latest\\n```\\n\\n### Container with local file access:\\n\\nTo run a container with access to local files (e.g., COCO training data in `/datasets`), use the `-v` flag:\\n\\n```bash\\nsudo docker run --ipc=host -it -v \"$(pwd)\"/datasets:/usr/src/datasets ultralytics/yolov5:latest\\n```\\n\\n### Container with GPU access:\\n\\nTo run a container with GPU access, use the `--gpus all` flag:\\n\\n```bash\\nsudo docker run --ipc=host -it --gpus all ultralytics/yolov5:latest\\n```\\n\\n## Step 3: Use YOLOv5 🚀 within the Docker Container\\n\\nNow you can train, test, detect, and export YOLOv5 models within the running Docker container:\\n\\n```bash\\npython train.py  # train a model\\npython val.py --weights yolov5s.pt  # validate a model for Precision, Recall, and mAP\\npython detect.py --weights yolov5s.pt --source path/to/images  # run inference on images and videos\\npython export.py --weights yolov5s.pt --include onnx coreml tflite  # export models to other formats\\n```\\n\\n<p align=\"center\"><img width=\"1000\" src=\"https://user-images.githubusercontent.com/26833433/142224770-6e57caaf-ac01-4719-987f-c37d1b6f401f.png\"></p>', metadata={'file_path': 'docs/yolov5/environments/docker_image_quickstart_tutorial.md', 'file_name': 'docker_image_quickstart_tutorial.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Run YOLOv5 🚀 on Google Cloud Platform (GCP) Deep Learning Virtual Machine (VM) ⭐\\n\\nThis tutorial will guide you through the process of setting up and running YOLOv5 on a GCP Deep Learning VM. New GCP users are eligible for a [$300 free credit offer](https://cloud.google.com/free/docs/gcp-free-tier#free-trial). \\n\\nYou can also explore other quickstart options for YOLOv5, such as our [Colab Notebook](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb) <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>, [Amazon AWS](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial) and our Docker image at [Docker Hub](https://hub.docker.com/r/ultralytics/yolov5) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>. *Updated: 21 April 2023*.\\n\\n**Last Updated**: 6 May 2022\\n\\n## Step 1: Create a Deep Learning VM\\n\\n1. Go to the [GCP marketplace](https://console.cloud.google.com/marketplace/details/click-to-deploy-images/deeplearning) and select a **Deep Learning VM**.\\n2. Choose an **n1-standard-8** instance (with 8 vCPUs and 30 GB memory).\\n3. Add a GPU of your choice.\\n4. Check \\'Install NVIDIA GPU driver automatically on first startup?\\'\\n5. Select a 300 GB SSD Persistent Disk for sufficient I/O speed.\\n6. Click \\'Deploy\\'.\\n\\nThe preinstalled [Anaconda](https://docs.anaconda.com/anaconda/packages/pkg-docs/) Python environment includes all dependencies.\\n\\n<img width=\"1000\" alt=\"GCP Marketplace\" src=\"https://user-images.githubusercontent.com/26833433/105811495-95863880-5f61-11eb-841d-c2f2a5aa0ffe.png\">\\n\\n## Step 2: Set Up the VM\\n\\nClone the YOLOv5 repository and install the [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) will be downloaded automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n## Step 3: Run YOLOv5 🚀 on the VM\\n\\nYou can now train, test, detect, and export YOLOv5 models on your VM:\\n\\n```bash\\npython train.py  # train a model\\npython val.py --weights yolov5s.pt  # validate a model for Precision, Recall, and mAP\\npython detect.py --weights yolov5s.pt --source path/to/images  # run inference on images and videos\\npython export.py --weights yolov5s.pt --include onnx coreml tflite  # export models to other formats\\n```\\n\\n<img width=\"1000\" alt=\"GCP terminal\" src=\"https://user-images.githubusercontent.com/26833433/142223900-275e5c9e-e2b5-43f7-a21c-35c4ca7de87c.png\">\\n', metadata={'file_path': 'docs/yolov5/environments/google_cloud_quickstart_tutorial.md', 'file_name': 'google_cloud_quickstart_tutorial.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n## 1. Model Structure\\n\\nYOLOv5 (v6.0/6.1) consists of:\\n- **Backbone**: `New CSP-Darknet53`\\n- **Neck**: `SPPF`, `New CSP-PAN`\\n- **Head**: `YOLOv3 Head`\\n\\nModel structure (`yolov5l.yaml`):\\n\\n![yolov5](https://user-images.githubusercontent.com/31005897/172404576-c260dcf9-76bb-4bc8-b6a9-f2d987792583.png)\\n\\n\\nSome minor changes compared to previous versions:\\n\\n1. Replace the `Focus` structure with `6x6 Conv2d`(more efficient, refer #4825)  \\n2. Replace the `SPP` structure with `SPPF`(more than double the speed)\\n\\n<details markdown>\\n<summary>test code</summary>\\n\\n```python\\nimport time\\nimport torch\\nimport torch.nn as nn\\n\\n\\nclass SPP(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.maxpool1 = nn.MaxPool2d(5, 1, padding=2)\\n        self.maxpool2 = nn.MaxPool2d(9, 1, padding=4)\\n        self.maxpool3 = nn.MaxPool2d(13, 1, padding=6)\\n\\n    def forward(self, x):\\n        o1 = self.maxpool1(x)\\n        o2 = self.maxpool2(x)\\n        o3 = self.maxpool3(x)\\n        return torch.cat([x, o1, o2, o3], dim=1)\\n\\n\\nclass SPPF(nn.Module):\\n    def __init__(self):\\n        super().__init__()\\n        self.maxpool = nn.MaxPool2d(5, 1, padding=2)\\n\\n    def forward(self, x):\\n        o1 = self.maxpool(x)\\n        o2 = self.maxpool(o1)\\n        o3 = self.maxpool(o2)\\n        return torch.cat([x, o1, o2, o3], dim=1)\\n\\n\\ndef main():\\n    input_tensor = torch.rand(8, 32, 16, 16)\\n    spp = SPP()\\n    sppf = SPPF()\\n    output1 = spp(input_tensor)\\n    output2 = sppf(input_tensor)\\n\\n    print(torch.equal(output1, output2))\\n\\n    t_start = time.time()\\n    for _ in range(100):\\n        spp(input_tensor)\\n    print(f\"spp time: {time.time() - t_start}\")\\n\\n    t_start = time.time()\\n    for _ in range(100):\\n        sppf(input_tensor)\\n    print(f\"sppf time: {time.time() - t_start}\")\\n\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n```\\n\\nresult:\\n```\\nTrue\\nspp time: 0.5373051166534424\\nsppf time: 0.20780706405639648\\n```\\n\\n</details>\\n\\n\\n\\n## 2. Data Augmentation\\n\\n- Mosaic\\n<img src=\"https://user-images.githubusercontent.com/31005897/159109235-c7aad8f2-1d4f-41f9-8d5f-b2fde6f2885e.png#pic_center\" width=80%>\\n\\n- Copy paste\\n<img src=\"https://user-images.githubusercontent.com/31005897/159116277-91b45033-6bec-4f82-afc4-41138866628e.png#pic_center\" width=80%>\\n\\n- Random affine(Rotation, Scale, Translation and Shear)\\n<img src=\"https://user-images.githubusercontent.com/31005897/159109326-45cd5acb-14fa-43e7-9235-0f21b0021c7d.png#pic_center\" width=80%>\\n\\n- MixUp\\n<img src=\"https://user-images.githubusercontent.com/31005897/159109361-3b24333b-f481-478b-ae00-df7838f0b5cd.png#pic_center\" width=80%>\\n\\n- Albumentations\\n- Augment HSV(Hue, Saturation, Value)\\n<img src=\"https://user-images.githubusercontent.com/31005897/159109407-83d100ba-1aba-4f4b-aa03-4f048f815981.png#pic_center\" width=80%>\\n\\n- Random horizontal flip\\n<img src=\"https://user-images.githubusercontent.com/31005897/159109429-0d44619a-a76a-49eb-bfc0-6709860c043e.png#pic_center\" width=80%>\\n\\n\\n\\n## 3. Training Strategies\\n\\n- Multi-scale training(0.5~1.5x)\\n- AutoAnchor(For training custom data)\\n- Warmup and Cosine LR scheduler\\n- EMA(Exponential Moving Average)\\n- Mixed precision\\n- Evolve hyper-parameters\\n\\n\\n\\n## 4. Others\\n\\n### 4.1 Compute Losses\\n\\nThe YOLOv5 loss consists of three parts: \\n\\n- Classes loss(BCE loss)\\n- Objectness loss(BCE loss)\\n- Location loss(CIoU loss)\\n\\n![loss](https://latex.codecogs.com/svg.image?Loss=\\\\lambda_1L_{cls}+\\\\lambda_2L_{obj}+\\\\lambda_3L_{loc})\\n\\n### 4.2 Balance Losses\\nThe objectness losses of the three prediction layers(`P3`, `P4`, `P5`) are weighted differently. The balance weights are `[4.0, 1.0, 0.4]` respectively.\\n\\n![obj_loss](https://latex.codecogs.com/svg.image?L_{obj}=4.0\\\\cdot&space;L_{obj}^{small}+1.0\\\\cdot&space;L_{obj}^{medium}+0.4\\\\cdot&space;L_{obj}^{large})\\n\\n### 4.3 Eliminate Grid Sensitivity\\nIn YOLOv2 and YOLOv3, the formula for calculating the predicted target information is:  \\n\\n![b_x](https://latex.codecogs.com/svg.image?b_x=\\\\sigma(t_x)+c_x)  \\n![b_y](https://latex.codecogs.com/svg.image?b_y=\\\\sigma(t_y)+c_y)  \\n![b_w](https://latex.codecogs.com/svg.image?b_w=p_w\\\\cdot&space;e^{t_w})  \\n![b_h](https://latex.codecogs.com/svg.image?b_h=p_h\\\\cdot&space;e^{t_h})\\n\\n<img src=\"https://user-images.githubusercontent.com/31005897/158508027-8bf63c28-8290-467b-8a3e-4ad09235001a.png#pic_center\" width=40%>\\n\\n\\n\\nIn YOLOv5, the formula is:  \\n\\n![bx](https://latex.codecogs.com/svg.image?b_x=(2\\\\cdot\\\\sigma(t_x)-0.5)+c_x)  \\n![by](https://latex.codecogs.com/svg.image?b_y=(2\\\\cdot\\\\sigma(t_y)-0.5)+c_y)  \\n![bw](https://latex.codecogs.com/svg.image?b_w=p_w\\\\cdot(2\\\\cdot\\\\sigma(t_w))^2)    \\n![bh](https://latex.codecogs.com/svg.image?b_h=p_h\\\\cdot(2\\\\cdot\\\\sigma(t_h))^2)  \\n\\nCompare the center point offset before and after scaling. The center point offset range is adjusted from (0, 1) to (-0.5, 1.5).\\nTherefore, offset can easily get 0 or 1.\\n\\n<img src=\"https://user-images.githubusercontent.com/31005897/158508052-c24bc5e8-05c1-4154-ac97-2e1ec71f582e.png#pic_center\" width=40%>\\n\\nCompare the height and width scaling ratio(relative to anchor) before and after adjustment. The original yolo/darknet box equations have a serious flaw. Width and Height are completely unbounded as they are simply out=exp(in), which is dangerous, as it can lead to runaway gradients, instabilities, NaN losses and ultimately a complete loss of training. [refer this issue](https://github.com/ultralytics/yolov5/issues/471#issuecomment-662009779)\\n\\n<img src=\"https://user-images.githubusercontent.com/31005897/158508089-5ac0c7a3-6358-44b7-863e-a6e45babb842.png#pic_center\" width=40%>\\n\\n\\n### 4.4 Build Targets\\nMatch positive samples:\\n\\n- Calculate the aspect ratio of GT and Anchor Templates\\n\\n![rw](https://latex.codecogs.com/svg.image?r_w=w_{gt}/w_{at})\\n\\n![rh](https://latex.codecogs.com/svg.image?r_h=h_{gt}/h_{at})\\n\\n![rwmax](https://latex.codecogs.com/svg.image?r_w^{max}=max(r_w,1/r_w))\\n\\n![rhmax](https://latex.codecogs.com/svg.image?r_h^{max}=max(r_h,1/r_h))\\n\\n![rmax](https://latex.codecogs.com/svg.image?r^{max}=max(r_w^{max},r_h^{max}))\\n\\n![match](https://latex.codecogs.com/svg.image?r^{max}<{\\\\rm&space;anchor_t})\\n\\n<img src=\"https://user-images.githubusercontent.com/31005897/158508119-fbb2e483-7b8c-4975-8e1f-f510d367f8ff.png#pic_center\" width=70%>\\n\\n- Assign the successfully matched Anchor Templates to the corresponding cells\\n\\n<img src=\"https://user-images.githubusercontent.com/31005897/158508771-b6e7cab4-8de6-47f9-9abf-cdf14c275dfe.png#pic_center\" width=70%>\\n\\n- Because the center point offset range is adjusted from (0, 1) to (-0.5, 1.5). GT Box can be assigned to more anchors.\\n\\n<img src=\"https://user-images.githubusercontent.com/31005897/158508139-9db4e8c2-cf96-47e0-bc80-35d11512f296.png#pic_center\" width=70%>\\n', metadata={'file_path': 'docs/yolov5/tutorials/architecture_description.md', 'file_name': 'architecture_description.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# ClearML Integration\\n\\n<img align=\"center\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/logos_dark.png#gh-light-mode-only\" alt=\"Clear|ML\"><img align=\"center\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/logos_light.png#gh-dark-mode-only\" alt=\"Clear|ML\">\\n\\n## About ClearML\\n\\n[ClearML](https://cutt.ly/yolov5-tutorial-clearml) is an [open-source](https://github.com/allegroai/clearml) toolbox designed to save you time ⏱️.\\n\\n🔨 Track every YOLOv5 training run in the <b>experiment manager</b>\\n\\n🔧 Version and easily access your custom training data with the integrated ClearML <b>Data Versioning Tool</b>\\n\\n🔦 <b>Remotely train and monitor</b> your YOLOv5 training runs using ClearML Agent\\n\\n🔬 Get the very best mAP using ClearML <b>Hyperparameter Optimization</b>\\n\\n🔭 Turn your newly trained <b>YOLOv5 model into an API</b> with just a few commands using ClearML Serving\\n\\n<br />\\nAnd so much more. It\\'s up to you how many of these tools you want to use, you can stick to the experiment manager, or chain them all together into an impressive pipeline!\\n<br />\\n<br />\\n\\n![ClearML scalars dashboard](https://github.com/thepycoder/clearml_screenshots/raw/main/experiment_manager_with_compare.gif)\\n\\n<br />\\n<br />\\n\\n## 🦾 Setting Things Up\\n\\nTo keep track of your experiments and/or data, ClearML needs to communicate to a server. You have 2 options to get one:\\n\\nEither sign up for free to the [ClearML Hosted Service](https://cutt.ly/yolov5-tutorial-clearml) or you can set up your own server, see [here](https://clear.ml/docs/latest/docs/deploying_clearml/clearml_server). Even the server is open-source, so even if you\\'re dealing with sensitive data, you should be good to go!\\n\\n1. Install the `clearml` python package:\\n\\n   ```bash\\n   pip install clearml\\n   ```\\n\\n2. Connect the ClearML SDK to the server by [creating credentials](https://app.clear.ml/settings/workspace-configuration) (go right top to Settings -> Workspace -> Create new credentials), then execute the command below and follow the instructions:\\n\\n   ```bash\\n   clearml-init\\n   ```\\n\\nThat\\'s it! You\\'re done 😎\\n\\n<br />\\n\\n## 🚀 Training YOLOv5 With ClearML\\n\\nTo enable ClearML experiment tracking, simply install the ClearML pip package.\\n\\n```bash\\npip install clearml>=1.2.0\\n```\\n\\nThis will enable integration with the YOLOv5 training script. Every training run from now on, will be captured and stored by the ClearML experiment manager.\\n\\nIf you want to change the `project_name` or `task_name`, use the `--project` and `--name` arguments of the `train.py` script, by default the project will be called `YOLOv5` and the task `Training`.\\nPLEASE NOTE: ClearML uses `/` as a delimiter for subprojects, so be careful when using `/` in your project name!\\n\\n```bash\\npython train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache\\n```\\n\\nor with custom project and task name:\\n\\n```bash\\npython train.py --project my_project --name my_training --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache\\n```\\n\\nThis will capture:\\n\\n- Source code + uncommitted changes\\n- Installed packages\\n- (Hyper)parameters\\n- Model files (use `--save-period n` to save a checkpoint every n epochs)\\n- Console output\\n- Scalars (mAP_0.5, mAP_0.5:0.95, precision, recall, losses, learning rates, ...)\\n- General info such as machine details, runtime, creation date etc.\\n- All produced plots such as label correlogram and confusion matrix\\n- Images with bounding boxes per epoch\\n- Mosaic per epoch\\n- Validation images per epoch\\n- ...\\n\\nThat\\'s a lot right? 🤯\\nNow, we can visualize all of this information in the ClearML UI to get an overview of our training progress. Add custom columns to the table view (such as e.g. mAP_0.5) so you can easily sort on the best performing model. Or select multiple experiments and directly compare them!\\n\\nThere even more we can do with all of this information, like hyperparameter optimization and remote execution, so keep reading if you want to see how that works!\\n\\n<br />\\n\\n## 🔗 Dataset Version Management\\n\\nVersioning your data separately from your code is generally a good idea and makes it easy to acquire the latest version too. This repository supports supplying a dataset version ID, and it will make sure to get the data if it\\'s not there yet. Next to that, this workflow also saves the used dataset ID as part of the task parameters, so you will always know for sure which data was used in which experiment!\\n\\n![ClearML Dataset Interface](https://github.com/thepycoder/clearml_screenshots/raw/main/clearml_data.gif)\\n\\n### Prepare Your Dataset\\n\\nThe YOLOv5 repository supports a number of different datasets by using yaml files containing their information. By default datasets are downloaded to the `../datasets` folder in relation to the repository root folder. So if you downloaded the `coco128` dataset using the link in the yaml or with the scripts provided by yolov5, you get this folder structure:\\n\\n```\\n..\\n|_ yolov5\\n|_ datasets\\n    |_ coco128\\n        |_ images\\n        |_ labels\\n        |_ LICENSE\\n        |_ README.txt\\n```\\n\\nBut this can be any dataset you wish. Feel free to use your own, as long as you keep to this folder structure.\\n\\nNext, ⚠️**copy the corresponding yaml file to the root of the dataset folder**⚠️. This yaml files contains the information ClearML will need to properly use the dataset. You can make this yourself too, of course, just follow the structure of the example yamls.\\n\\nBasically we need the following keys: `path`, `train`, `test`, `val`, `nc`, `names`.\\n\\n```\\n..\\n|_ yolov5\\n|_ datasets\\n    |_ coco128\\n        |_ images\\n        |_ labels\\n        |_ coco128.yaml  # <---- HERE!\\n        |_ LICENSE\\n        |_ README.txt\\n```\\n\\n### Upload Your Dataset\\n\\nTo get this dataset into ClearML as a versioned dataset, go to the dataset root folder and run the following command:\\n\\n```bash\\ncd coco128\\nclearml-data sync --project YOLOv5 --name coco128 --folder .\\n```\\n\\nThe command `clearml-data sync` is actually a shorthand command. You could also run these commands one after the other:\\n\\n```bash\\n# Optionally add --parent <parent_dataset_id> if you want to base\\n# this version on another dataset version, so no duplicate files are uploaded!\\nclearml-data create --name coco128 --project YOLOv5\\nclearml-data add --files .\\nclearml-data close\\n```\\n\\n### Run Training Using A ClearML Dataset\\n\\nNow that you have a ClearML dataset, you can very simply use it to train custom YOLOv5 🚀 models!\\n\\n```bash\\npython train.py --img 640 --batch 16 --epochs 3 --data clearml://<your_dataset_id> --weights yolov5s.pt --cache\\n```\\n\\n<br />\\n\\n## 👀 Hyperparameter Optimization\\n\\nNow that we have our experiments and data versioned, it\\'s time to take a look at what we can build on top!\\n\\nUsing the code information, installed packages and environment details, the experiment itself is now **completely reproducible**. In fact, ClearML allows you to clone an experiment and even change its parameters. We can then just rerun it with these new parameters automatically, this is basically what HPO does!\\n\\nTo **run hyperparameter optimization locally**, we\\'ve included a pre-made script for you. Just make sure a training task has been run at least once, so it is in the ClearML experiment manager, we will essentially clone it and change its hyperparameters.\\n\\nYou\\'ll need to fill in the ID of this `template task` in the script found at `utils/loggers/clearml/hpo.py` and then just run it :) You can change `task.execute_locally()` to `task.execute()` to put it in a ClearML queue and have a remote agent work on it instead.\\n\\n```bash\\n# To use optuna, install it first, otherwise you can change the optimizer to just be RandomSearch\\npip install optuna\\npython utils/loggers/clearml/hpo.py\\n```\\n\\n![HPO](https://github.com/thepycoder/clearml_screenshots/raw/main/hpo.png)\\n\\n## 🤯 Remote Execution (advanced)\\n\\nRunning HPO locally is really handy, but what if we want to run our experiments on a remote machine instead? Maybe you have access to a very powerful GPU machine on-site, or you have some budget to use cloud GPUs.\\nThis is where the ClearML Agent comes into play. Check out what the agent can do here:\\n\\n- [YouTube video](https://youtu.be/MX3BrXnaULs)\\n- [Documentation](https://clear.ml/docs/latest/docs/clearml_agent)\\n\\nIn short: every experiment tracked by the experiment manager contains enough information to reproduce it on a different machine (installed packages, uncommitted changes etc.). So a ClearML agent does just that: it listens to a queue for incoming tasks and when it finds one, it recreates the environment and runs it while still reporting scalars, plots etc. to the experiment manager.\\n\\nYou can turn any machine (a cloud VM, a local GPU machine, your own laptop ... ) into a ClearML agent by simply running:\\n\\n```bash\\nclearml-agent daemon --queue <queues_to_listen_to> [--docker]\\n```\\n\\n### Cloning, Editing And Enqueuing\\n\\nWith our agent running, we can give it some work. Remember from the HPO section that we can clone a task and edit the hyperparameters? We can do that from the interface too!\\n\\n🪄 Clone the experiment by right-clicking it\\n\\n🎯 Edit the hyperparameters to what you wish them to be\\n\\n⏳ Enqueue the task to any of the queues by right-clicking it\\n\\n![Enqueue a task from the UI](https://github.com/thepycoder/clearml_screenshots/raw/main/enqueue.gif)\\n\\n### Executing A Task Remotely\\n\\nNow you can clone a task like we explained above, or simply mark your current script by adding `task.execute_remotely()` and on execution it will be put into a queue, for the agent to start working on!\\n\\nTo run the YOLOv5 training script remotely, all you have to do is add this line to the training.py script after the clearml logger has been instantiated:\\n\\n```python\\n# ...\\n# Loggers\\ndata_dict = None\\nif RANK in {-1, 0}:\\n    loggers = Loggers(save_dir, weights, opt, hyp, LOGGER)  # loggers instance\\n    if loggers.clearml:\\n        loggers.clearml.task.execute_remotely(queue=\"my_queue\")  # <------ ADD THIS LINE\\n        # Data_dict is either None is user did not choose for ClearML dataset or is filled in by ClearML\\n        data_dict = loggers.clearml.data_dict\\n# ...\\n```\\n\\nWhen running the training script after this change, python will run the script up until that line, after which it will package the code and send it to the queue instead!\\n\\n### Autoscaling workers\\n\\nClearML comes with autoscalers too! This tool will automatically spin up new remote machines in the cloud of your choice (AWS, GCP, Azure) and turn them into ClearML agents for you whenever there are experiments detected in the queue. Once the tasks are processed, the autoscaler will automatically shut down the remote machines, and you stop paying!\\n\\nCheck out the autoscalers getting started video below.\\n\\n[![Watch the video](https://img.youtube.com/vi/j4XVMAaUt3E/0.jpg)](https://youtu.be/j4XVMAaUt3E)\\n', metadata={'file_path': 'docs/yolov5/tutorials/clearml_logging_integration.md', 'file_name': 'clearml_logging_integration.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n<img src=\"https://cdn.comet.ml/img/notebook_logo.png\">\\n\\n# YOLOv5 with Comet\\n\\nThis guide will cover how to use YOLOv5 with [Comet](https://bit.ly/yolov5-readme-comet2)\\n\\n# About Comet\\n\\nComet builds tools that help data scientists, engineers, and team leaders accelerate and optimize machine learning and deep learning models.\\n\\nTrack and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://www.comet.com/docs/v2/guides/comet-dashboard/code-panels/about-panels/?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=github)!\\nComet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes!\\n\\n# Getting Started\\n\\n## Install Comet\\n\\n```shell\\npip install comet_ml\\n```\\n\\n## Configure Comet Credentials\\n\\nThere are two ways to configure Comet with YOLOv5.\\n\\nYou can either set your credentials through environment variables\\n\\n**Environment Variables**\\n\\n```shell\\nexport COMET_API_KEY=<Your Comet API Key>\\nexport COMET_PROJECT_NAME=<Your Comet Project Name> # This will default to \\'yolov5\\'\\n```\\n\\nOr create a `.comet.config` file in your working directory and set your credentials there.\\n\\n**Comet Configuration File**\\n\\n```\\n[comet]\\napi_key=<Your Comet API Key>\\nproject_name=<Your Comet Project Name> # This will default to \\'yolov5\\'\\n```\\n\\n## Run the Training Script\\n\\n```shell\\n# Train YOLOv5s on COCO128 for 5 epochs\\npython train.py --img 640 --batch 16 --epochs 5 --data coco128.yaml --weights yolov5s.pt\\n```\\n\\nThat\\'s it! Comet will automatically log your hyperparameters, command line arguments, training and validation metrics. You can visualize and analyze your runs in the Comet UI\\n\\n<img width=\"1920\" alt=\"yolo-ui\" src=\"https://user-images.githubusercontent.com/26833433/202851203-164e94e1-2238-46dd-91f8-de020e9d6b41.png\">\\n\\n# Try out an Example!\\n\\nCheck out an example of a [completed run here](https://www.comet.com/examples/comet-example-yolov5/a0e29e0e9b984e4a822db2a62d0cb357?experiment-tab=chart&showOutliers=true&smoothing=0&transformY=smoothing&xAxis=step&utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=github)\\n\\nOr better yet, try it out yourself in this Colab Notebook\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)\\n\\n# Log automatically\\n\\nBy default, Comet will log the following items\\n\\n## Metrics\\n\\n- Box Loss, Object Loss, Classification Loss for the training and validation data\\n- mAP_0.5, mAP_0.5:0.95 metrics for the validation data.\\n- Precision and Recall for the validation data\\n\\n## Parameters\\n\\n- Model Hyperparameters\\n- All parameters passed through the command line options\\n\\n## Visualizations\\n\\n- Confusion Matrix of the model predictions on the validation data\\n- Plots for the PR and F1 curves across all classes\\n- Correlogram of the Class Labels\\n\\n# Configure Comet Logging\\n\\nComet can be configured to log additional data either through command line flags passed to the training script\\nor through environment variables.\\n\\n```shell\\nexport COMET_MODE=online # Set whether to run Comet in \\'online\\' or \\'offline\\' mode. Defaults to online\\nexport COMET_MODEL_NAME=<your model name> #Set the name for the saved model. Defaults to yolov5\\nexport COMET_LOG_CONFUSION_MATRIX=false # Set to disable logging a Comet Confusion Matrix. Defaults to true\\nexport COMET_MAX_IMAGE_UPLOADS=<number of allowed images to upload to Comet> # Controls how many total image predictions to log to Comet. Defaults to 100.\\nexport COMET_LOG_PER_CLASS_METRICS=true # Set to log evaluation metrics for each detected class at the end of training. Defaults to false\\nexport COMET_DEFAULT_CHECKPOINT_FILENAME=<your checkpoint filename> # Set this if you would like to resume training from a different checkpoint. Defaults to \\'last.pt\\'\\nexport COMET_LOG_BATCH_LEVEL_METRICS=true # Set this if you would like to log training metrics at the batch level. Defaults to false.\\nexport COMET_LOG_PREDICTIONS=true # Set this to false to disable logging model predictions\\n```\\n\\n## Logging Checkpoints with Comet\\n\\nLogging Models to Comet is disabled by default. To enable it, pass the `save-period` argument to the training script. This will save the\\nlogged checkpoints to Comet based on the interval value provided by `save-period`\\n\\n```shell\\npython train.py \\\\\\n--img 640 \\\\\\n--batch 16 \\\\\\n--epochs 5 \\\\\\n--data coco128.yaml \\\\\\n--weights yolov5s.pt \\\\\\n--save-period 1\\n```\\n\\n## Logging Model Predictions\\n\\nBy default, model predictions (images, ground truth labels and bounding boxes) will be logged to Comet.\\n\\nYou can control the frequency of logged predictions and the associated images by passing the `bbox_interval` command line argument. Predictions can be visualized using Comet\\'s Object Detection Custom Panel. This frequency corresponds to every Nth batch of data per epoch. In the example below, we are logging every 2nd batch of data for each epoch.\\n\\n**Note:** The YOLOv5 validation dataloader will default to a batch size of 32, so you will have to set the logging frequency accordingly.\\n\\nHere is an [example project using the Panel](https://www.comet.com/examples/comet-example-yolov5?shareable=YcwMiJaZSXfcEXpGOHDD12vA1&utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=github)\\n\\n```shell\\npython train.py \\\\\\n--img 640 \\\\\\n--batch 16 \\\\\\n--epochs 5 \\\\\\n--data coco128.yaml \\\\\\n--weights yolov5s.pt \\\\\\n--bbox_interval 2\\n```\\n\\n### Controlling the number of Prediction Images logged to Comet\\n\\nWhen logging predictions from YOLOv5, Comet will log the images associated with each set of predictions. By default a maximum of 100 validation images are logged. You can increase or decrease this number using the `COMET_MAX_IMAGE_UPLOADS` environment variable.\\n\\n```shell\\nenv COMET_MAX_IMAGE_UPLOADS=200 python train.py \\\\\\n--img 640 \\\\\\n--batch 16 \\\\\\n--epochs 5 \\\\\\n--data coco128.yaml \\\\\\n--weights yolov5s.pt \\\\\\n--bbox_interval 1\\n```\\n\\n### Logging Class Level Metrics\\n\\nUse the `COMET_LOG_PER_CLASS_METRICS` environment variable to log mAP, precision, recall, f1 for each class.\\n\\n```shell\\nenv COMET_LOG_PER_CLASS_METRICS=true python train.py \\\\\\n--img 640 \\\\\\n--batch 16 \\\\\\n--epochs 5 \\\\\\n--data coco128.yaml \\\\\\n--weights yolov5s.pt\\n```\\n\\n## Uploading a Dataset to Comet Artifacts\\n\\nIf you would like to store your data using [Comet Artifacts](https://www.comet.com/docs/v2/guides/data-management/using-artifacts/#learn-more?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=github), you can do so using the `upload_dataset` flag.\\n\\nThe dataset be organized in the way described in the [YOLOv5 documentation](train_custom_data.md). The dataset config `yaml` file must follow the same format as that of the `coco128.yaml` file.\\n\\n```shell\\npython train.py \\\\\\n--img 640 \\\\\\n--batch 16 \\\\\\n--epochs 5 \\\\\\n--data coco128.yaml \\\\\\n--weights yolov5s.pt \\\\\\n--upload_dataset\\n```\\n\\nYou can find the uploaded dataset in the Artifacts tab in your Comet Workspace\\n<img width=\"1073\" alt=\"artifact-1\" src=\"https://user-images.githubusercontent.com/7529846/186929193-162718bf-ec7b-4eb9-8c3b-86b3763ef8ea.png\">\\n\\nYou can preview the data directly in the Comet UI.\\n<img width=\"1082\" alt=\"artifact-2\" src=\"https://user-images.githubusercontent.com/7529846/186929215-432c36a9-c109-4eb0-944b-84c2786590d6.png\">\\n\\nArtifacts are versioned and also support adding metadata about the dataset. Comet will automatically log the metadata from your dataset `yaml` file\\n<img width=\"963\" alt=\"artifact-3\" src=\"https://user-images.githubusercontent.com/7529846/186929256-9d44d6eb-1a19-42de-889a-bcbca3018f2e.png\">\\n\\n### Using a saved Artifact\\n\\nIf you would like to use a dataset from Comet Artifacts, set the `path` variable in your dataset `yaml` file to point to the following Artifact resource URL.\\n\\n```\\n# contents of artifact.yaml file\\npath: \"comet://<workspace name>/<artifact name>:<artifact version or alias>\"\\n```\\n\\nThen pass this file to your training script in the following way\\n\\n```shell\\npython train.py \\\\\\n--img 640 \\\\\\n--batch 16 \\\\\\n--epochs 5 \\\\\\n--data artifact.yaml \\\\\\n--weights yolov5s.pt\\n```\\n\\nArtifacts also allow you to track the lineage of data as it flows through your Experimentation workflow. Here you can see a graph that shows you all the experiments that have used your uploaded dataset.\\n<img width=\"1391\" alt=\"artifact-4\" src=\"https://user-images.githubusercontent.com/7529846/186929264-4c4014fa-fe51-4f3c-a5c5-f6d24649b1b4.png\">\\n\\n## Resuming a Training Run\\n\\nIf your training run is interrupted for any reason, e.g. disrupted internet connection, you can resume the run using the `resume` flag and the Comet Run Path.\\n\\nThe Run Path has the following format `comet://<your workspace name>/<your project name>/<experiment id>`.\\n\\nThis will restore the run to its state before the interruption, which includes restoring the  model from a checkpoint, restoring all hyperparameters and training arguments and downloading Comet dataset Artifacts if they were used in the original run. The resumed run will continue logging to the existing Experiment in the Comet UI\\n\\n```shell\\npython train.py \\\\\\n--resume \"comet://<your run path>\"\\n```\\n\\n## Hyperparameter Search with the Comet Optimizer\\n\\nYOLOv5 is also integrated with Comet\\'s Optimizer, making is simple to visualize hyperparameter sweeps in the Comet UI.\\n\\n### Configuring an Optimizer Sweep\\n\\nTo configure the Comet Optimizer, you will have to create a JSON file with the information about the sweep. An example file has been provided in `utils/loggers/comet/optimizer_config.json`\\n\\n```shell\\npython utils/loggers/comet/hpo.py \\\\\\n  --comet_optimizer_config \"utils/loggers/comet/optimizer_config.json\"\\n```\\n\\nThe `hpo.py` script accepts the same arguments as `train.py`. If you wish to pass additional arguments to your sweep simply add them after\\nthe script.\\n\\n```shell\\npython utils/loggers/comet/hpo.py \\\\\\n  --comet_optimizer_config \"utils/loggers/comet/optimizer_config.json\" \\\\\\n  --save-period 1 \\\\\\n  --bbox_interval 1\\n```\\n\\n### Running a Sweep in Parallel\\n\\n```shell\\ncomet optimizer -j <set number of workers> utils/loggers/comet/hpo.py \\\\\\n  utils/loggers/comet/optimizer_config.json\"\\n```\\n\\n### Visualizing Results\\n\\nComet provides a number of ways to visualize the results of your sweep. Take a look at a [project with a completed sweep here](https://www.comet.com/examples/comet-example-yolov5/view/PrlArHGuuhDTKC1UuBmTtOSXD/panels?utm_source=yolov5&utm_medium=partner&utm_campaign=partner_yolov5_2022&utm_content=github)\\n\\n<img width=\"1626\" alt=\"hyperparameter-yolo\" src=\"https://user-images.githubusercontent.com/7529846/186914869-7dc1de14-583f-4323-967b-c9a66a29e495.png\">\\n', metadata={'file_path': 'docs/yolov5/tutorials/comet_logging_integration.md', 'file_name': 'comet_logging_integration.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n📚  This guide explains **hyperparameter evolution** for YOLOv5 🚀. Hyperparameter evolution is a method of [Hyperparameter Optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) using a [Genetic Algorithm](https://en.wikipedia.org/wiki/Genetic_algorithm) (GA) for optimization. UPDATED 25 September 2022.\\n\\nHyperparameters in ML control various aspects of training, and finding optimal values for them can be a challenge. Traditional methods like grid searches can quickly become intractable due to 1) the high dimensional search space 2) unknown correlations among the dimensions, and 3) expensive nature of evaluating the fitness at each point, making GA a suitable candidate for hyperparameter searches.\\n\\n\\n## Before You Start\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n\\n## 1. Initialize Hyperparameters\\n\\nYOLOv5 has about 30 hyperparameters used for various training settings. These are defined in `*.yaml` files in the `/data/hyps` directory. Better initial guesses will produce better final results, so it is important to initialize these values properly before evolving. If in doubt, simply use the default values, which are optimized for YOLOv5 COCO training from scratch.\\n\\n```yaml\\n# YOLOv5 🚀 by Ultralytics, AGPL-3.0 license\\n# Hyperparameters for low-augmentation COCO training from scratch\\n# python train.py --batch 64 --cfg yolov5n6.yaml --weights \\'\\' --data coco.yaml --img 640 --epochs 300 --linear\\n# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\\n\\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\\nlrf: 0.01  # final OneCycleLR learning rate (lr0 * lrf)\\nmomentum: 0.937  # SGD momentum/Adam beta1\\nweight_decay: 0.0005  # optimizer weight decay 5e-4\\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\\nwarmup_momentum: 0.8  # warmup initial momentum\\nwarmup_bias_lr: 0.1  # warmup initial bias lr\\nbox: 0.05  # box loss gain\\ncls: 0.5  # cls loss gain\\ncls_pw: 1.0  # cls BCELoss positive_weight\\nobj: 1.0  # obj loss gain (scale with pixels)\\nobj_pw: 1.0  # obj BCELoss positive_weight\\niou_t: 0.20  # IoU training threshold\\nanchor_t: 4.0  # anchor-multiple threshold\\n# anchors: 3  # anchors per output layer (0 to ignore)\\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\\ndegrees: 0.0  # image rotation (+/- deg)\\ntranslate: 0.1  # image translation (+/- fraction)\\nscale: 0.5  # image scale (+/- gain)\\nshear: 0.0  # image shear (+/- deg)\\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\\nflipud: 0.0  # image flip up-down (probability)\\nfliplr: 0.5  # image flip left-right (probability)\\nmosaic: 1.0  # image mosaic (probability)\\nmixup: 0.0  # image mixup (probability)\\ncopy_paste: 0.0  # segment copy-paste (probability)\\n```\\n\\n## 2. Define Fitness\\n\\nFitness is the value we seek to maximize. In YOLOv5 we define a default fitness function as a weighted combination of metrics: `mAP@0.5` contributes 10% of the weight and `mAP@0.5:0.95` contributes the remaining 90%, with [Precision `P` and Recall `R`](https://en.wikipedia.org/wiki/Precision_and_recall) absent. You may adjust these as you see fit or use the default fitness definition in utils/metrics.py (recommended).\\n\\n```python\\ndef fitness(x): \\n    # Model fitness as a weighted combination of metrics \\n    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95] \\n    return (x[:, :4] * w).sum(1) \\n```\\n\\n## 3. Evolve\\n\\nEvolution is performed about a base scenario which we seek to improve upon. The base scenario in this example is finetuning COCO128 for 10 epochs using pretrained YOLOv5s. The base scenario training command is:\\n```bash\\npython train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache\\n```\\nTo evolve hyperparameters **specific to this scenario**, starting from our initial values defined in **Section 1.**, and maximizing the fitness defined in **Section 2.**, append `--evolve`:\\n```bash\\n# Single-GPU\\npython train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --evolve\\n\\n# Multi-GPU\\nfor i in 0 1 2 3 4 5 6 7; do\\n  sleep $(expr 30 \\\\* $i) &&  # 30-second delay (optional)\\n  echo \\'Starting GPU \\'$i\\'...\\' &&\\n  nohup python train.py --epochs 10 --data coco128.yaml --weights yolov5s.pt --cache --device $i --evolve > evolve_gpu_$i.log &\\ndone\\n\\n# Multi-GPU bash-while (not recommended)\\nfor i in 0 1 2 3 4 5 6 7; do\\n  sleep $(expr 30 \\\\* $i) &&  # 30-second delay (optional)\\n  echo \\'Starting GPU \\'$i\\'...\\' &&\\n  \"$(while true; do nohup python train.py... --device $i --evolve 1 > evolve_gpu_$i.log; done)\" &\\ndone\\n```\\n\\nThe default evolution settings will run the base scenario 300 times, i.e. for 300 generations. You can modify generations via the `--evolve` argument, i.e. `python train.py --evolve 1000`.\\nhttps://github.com/ultralytics/yolov5/blob/6a3ee7cf03efb17fbffde0e68b1a854e80fe3213/train.py#L608\\n\\nThe main genetic operators are **crossover** and **mutation**. In this work mutation is used, with an 80% probability and a 0.04 variance to create new offspring based on a combination of the best parents from all previous generations. Results are logged to `runs/evolve/exp/evolve.csv`, and the highest fitness offspring is saved every generation as `runs/evolve/hyp_evolved.yaml`:\\n```yaml\\n# YOLOv5 Hyperparameter Evolution Results\\n# Best generation: 287\\n# Last generation: 300\\n#    metrics/precision,       metrics/recall,      metrics/mAP_0.5, metrics/mAP_0.5:0.95,         val/box_loss,         val/obj_loss,         val/cls_loss\\n#              0.54634,              0.55625,              0.58201,              0.33665,             0.056451,             0.042892,             0.013441\\n\\nlr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\\nlrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\\nmomentum: 0.937  # SGD momentum/Adam beta1\\nweight_decay: 0.0005  # optimizer weight decay 5e-4\\nwarmup_epochs: 3.0  # warmup epochs (fractions ok)\\nwarmup_momentum: 0.8  # warmup initial momentum\\nwarmup_bias_lr: 0.1  # warmup initial bias lr\\nbox: 0.05  # box loss gain\\ncls: 0.5  # cls loss gain\\ncls_pw: 1.0  # cls BCELoss positive_weight\\nobj: 1.0  # obj loss gain (scale with pixels)\\nobj_pw: 1.0  # obj BCELoss positive_weight\\niou_t: 0.20  # IoU training threshold\\nanchor_t: 4.0  # anchor-multiple threshold\\n# anchors: 3  # anchors per output layer (0 to ignore)\\nfl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\\nhsv_h: 0.015  # image HSV-Hue augmentation (fraction)\\nhsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\\nhsv_v: 0.4  # image HSV-Value augmentation (fraction)\\ndegrees: 0.0  # image rotation (+/- deg)\\ntranslate: 0.1  # image translation (+/- fraction)\\nscale: 0.5  # image scale (+/- gain)\\nshear: 0.0  # image shear (+/- deg)\\nperspective: 0.0  # image perspective (+/- fraction), range 0-0.001\\nflipud: 0.0  # image flip up-down (probability)\\nfliplr: 0.5  # image flip left-right (probability)\\nmosaic: 1.0  # image mosaic (probability)\\nmixup: 0.0  # image mixup (probability)\\ncopy_paste: 0.0  # segment copy-paste (probability)\\n```\\n\\nWe recommend a minimum of 300 generations of evolution for best results. Note that **evolution is generally expensive and time-consuming**, as the base scenario is trained hundreds of times, possibly requiring hundreds or thousands of GPU hours.\\n\\n\\n## 4. Visualize\\n\\n`evolve.csv` is plotted as `evolve.png` by `utils.plots.plot_evolve()` after evolution finishes with one subplot per hyperparameter showing fitness (y-axis) vs hyperparameter values (x-axis). Yellow indicates higher concentrations. Vertical distributions indicate that a parameter has been disabled and does not mutate. This is user selectable in the `meta` dictionary in train.py, and is useful for fixing parameters and preventing them from evolving.\\n\\n![evolve](https://user-images.githubusercontent.com/26833433/89130469-f43e8e00-d4b9-11ea-9e28-f8ae3622516d.png)\\n\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.', metadata={'file_path': 'docs/yolov5/tutorials/hyperparameter_evolution.md', 'file_name': 'hyperparameter_evolution.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n📚  This guide explains how to use YOLOv5 🚀 **model ensembling** during testing and inference for improved mAP and Recall.  \\nUPDATED 25 September 2022.\\n\\nFrom [https://en.wikipedia.org/wiki/Ensemble_learning](https://en.wikipedia.org/wiki/Ensemble_learning):\\n> Ensemble modeling is a process where multiple diverse models are created to predict an outcome, either by using many different modeling algorithms or using different training data sets. The ensemble model then aggregates the prediction of each base model and results in once final prediction for the unseen data. The motivation for using ensemble models is to reduce the generalization error of the prediction. As long as the base models are diverse and independent, the prediction error of the model decreases when the ensemble approach is used. The approach seeks the wisdom of crowds in making a prediction. Even though the ensemble model has multiple base models within the model, it acts and performs as a single model.\\n\\n\\n## Before You Start\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n## Test Normally\\n\\nBefore ensembling we want to establish the baseline performance of a single model. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. `yolov5x.pt` is the largest and most accurate model available. Other options are `yolov5s.pt`, `yolov5m.pt` and `yolov5l.pt`, or you own checkpoint from training a custom dataset `./weights/best.pt`. For details on all available models please see our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints).\\n```bash\\npython val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\\n```\\n\\nOutput:\\n```shell\\nval: data=./data/coco.yaml, weights=[\\'yolov5x.pt\\'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\\nYOLOv5 🚀 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\\n\\nFusing layers... \\nModel Summary: 476 layers, 87730285 parameters, 0 gradients\\n\\nval: Scanning \\'../datasets/coco/val2017\\' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 2846.03it/s]\\nval: New cache created: ../datasets/coco/val2017.cache\\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [02:30<00:00,  1.05it/s]\\n                 all       5000      36335      0.746      0.626       0.68       0.49\\nSpeed: 0.1ms pre-process, 22.4ms inference, 1.4ms NMS per image at shape (32, 3, 640, 640)  # <--- baseline speed\\n\\nEvaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...\\n...\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504  # <--- baseline mAP\\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.628\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681  # <--- baseline mAR\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.524\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826\\n```\\n\\n## Ensemble Test\\n\\nMultiple pretrained models may be ensembled together at test and inference time by simply appending extra models to the `--weights` argument in any existing val.py or detect.py command. This example tests an ensemble of 2 models together:\\n- YOLOv5x\\n- YOLOv5l6\\n\\n```bash\\npython val.py --weights yolov5x.pt yolov5l6.pt --data coco.yaml --img 640 --half\\n```\\n\\nOutput:\\n```shell\\nval: data=./data/coco.yaml, weights=[\\'yolov5x.pt\\', \\'yolov5l6.pt\\'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\\nYOLOv5 🚀 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\\n\\nFusing layers... \\nModel Summary: 476 layers, 87730285 parameters, 0 gradients  # Model 1\\nFusing layers... \\nModel Summary: 501 layers, 77218620 parameters, 0 gradients  # Model 2\\nEnsemble created with [\\'yolov5x.pt\\', \\'yolov5l6.pt\\']  # Ensemble notice\\n\\nval: Scanning \\'../datasets/coco/val2017.cache\\' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:00<00:00, 49695545.02it/s]\\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [03:58<00:00,  1.52s/it]\\n                 all       5000      36335      0.747      0.637      0.692      0.502\\nSpeed: 0.1ms pre-process, 39.5ms inference, 2.0ms NMS per image at shape (32, 3, 640, 640)  # <--- ensemble speed\\n\\nEvaluating pycocotools mAP... saving runs/val/exp3/yolov5x_predictions.json...\\n...\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.515  # <--- ensemble mAP\\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.699\\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.557\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.356\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.563\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.668\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.387\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.638\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.689  # <--- ensemble mAR\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.743\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.844\\n```\\n\\n## Ensemble Inference\\n\\nAppend extra models to the `--weights` argument to run ensemble inference:\\n```bash\\npython detect.py --weights yolov5x.pt yolov5l6.pt --img 640 --source data/images\\n```\\n\\nOutput:\\n```bash\\ndetect: weights=[\\'yolov5x.pt\\', \\'yolov5l6.pt\\'], source=data/images, imgsz=640, conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\\nYOLOv5 🚀 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\\n\\nFusing layers... \\nModel Summary: 476 layers, 87730285 parameters, 0 gradients\\nFusing layers... \\nModel Summary: 501 layers, 77218620 parameters, 0 gradients\\nEnsemble created with [\\'yolov5x.pt\\', \\'yolov5l6.pt\\']\\n\\nimage 1/2 /content/yolov5/data/images/bus.jpg: 640x512 4 persons, 1 bus, 1 tie, Done. (0.063s)\\nimage 2/2 /content/yolov5/data/images/zidane.jpg: 384x640 3 persons, 2 ties, Done. (0.056s)\\nResults saved to runs/detect/exp2\\nDone. (0.223s)\\n```\\n<img src=\"https://user-images.githubusercontent.com/26833433/124489091-ea4f9a00-ddb0-11eb-8ef1-d6f335c97f6f.jpg\" width=\"500\">\\n\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.', metadata={'file_path': 'docs/yolov5/tutorials/model_ensembling.md', 'file_name': 'model_ensembling.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# TFLite, ONNX, CoreML, TensorRT Export\\n\\n📚 This guide explains how to export a trained YOLOv5 🚀 model from PyTorch to ONNX and TorchScript formats.  \\nUPDATED 8 December 2022.\\n\\n## Before You Start\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\nFor [TensorRT](https://developer.nvidia.com/tensorrt) export example (requires GPU) see our Colab [notebook](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb#scrollTo=VTRwsvA9u7ln&line=2&uniqifier=1) appendix section. <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\\n\\n## Formats\\n\\nYOLOv5 inference is officially supported in 11 formats:\\n\\n💡 ProTip: Export to ONNX or OpenVINO for up to 3x CPU speedup. See [CPU Benchmarks](https://github.com/ultralytics/yolov5/pull/6613).\\n💡 ProTip: Export to TensorRT for up to 5x GPU speedup. See [GPU Benchmarks](https://github.com/ultralytics/yolov5/pull/6963).\\n\\n| Format                                                                     | `export.py --include` | Model                     |\\n|:---------------------------------------------------------------------------|:----------------------|:--------------------------|\\n| [PyTorch](https://pytorch.org/)                                            | -                     | `yolov5s.pt`              |\\n| [TorchScript](https://pytorch.org/docs/stable/jit.html)                    | `torchscript`         | `yolov5s.torchscript`     |\\n| [ONNX](https://onnx.ai/)                                                   | `onnx`                | `yolov5s.onnx`            |\\n| [OpenVINO](https://docs.openvino.ai/latest/index.html)                     | `openvino`            | `yolov5s_openvino_model/` |\\n| [TensorRT](https://developer.nvidia.com/tensorrt)                          | `engine`              | `yolov5s.engine`          |\\n| [CoreML](https://github.com/apple/coremltools)                             | `coreml`              | `yolov5s.mlmodel`         |\\n| [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`         | `yolov5s_saved_model/`    |\\n| [TensorFlow GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`                  | `yolov5s.pb`              |\\n| [TensorFlow Lite](https://www.tensorflow.org/lite)                         | `tflite`              | `yolov5s.tflite`          |\\n| [TensorFlow Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`             | `yolov5s_edgetpu.tflite`  |\\n| [TensorFlow.js](https://www.tensorflow.org/js)                             | `tfjs`                | `yolov5s_web_model/`      |\\n| [PaddlePaddle](https://github.com/PaddlePaddle)                            | `paddle`              | `yolov5s_paddle_model/`   |\\n\\n\\n## Benchmarks\\n\\nBenchmarks below run on a Colab Pro with the YOLOv5 tutorial notebook <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>. To reproduce:\\n```bash\\npython benchmarks.py --weights yolov5s.pt --imgsz 640 --device 0\\n```\\n\\n### Colab Pro V100 GPU\\n\\n```\\nbenchmarks: weights=/content/yolov5/yolov5s.pt, imgsz=640, batch_size=1, data=/content/yolov5/data/coco128.yaml, device=0, half=False, test=False\\nChecking setup...\\nYOLOv5 🚀 v6.1-135-g7926afc torch 1.10.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\\nSetup complete ✅ (8 CPUs, 51.0 GB RAM, 46.7/166.8 GB disk)\\n\\nBenchmarks complete (458.07s)\\n                   Format  mAP@0.5:0.95  Inference time (ms)\\n0                 PyTorch        0.4623                10.19\\n1             TorchScript        0.4623                 6.85\\n2                    ONNX        0.4623                14.63\\n3                OpenVINO           NaN                  NaN\\n4                TensorRT        0.4617                 1.89\\n5                  CoreML           NaN                  NaN\\n6   TensorFlow SavedModel        0.4623                21.28\\n7     TensorFlow GraphDef        0.4623                21.22\\n8         TensorFlow Lite           NaN                  NaN\\n9     TensorFlow Edge TPU           NaN                  NaN\\n10          TensorFlow.js           NaN                  NaN\\n```\\n\\n### Colab Pro CPU\\n\\n```\\nbenchmarks: weights=/content/yolov5/yolov5s.pt, imgsz=640, batch_size=1, data=/content/yolov5/data/coco128.yaml, device=cpu, half=False, test=False\\nChecking setup...\\nYOLOv5 🚀 v6.1-135-g7926afc torch 1.10.0+cu111 CPU\\nSetup complete ✅ (8 CPUs, 51.0 GB RAM, 41.5/166.8 GB disk)\\n\\nBenchmarks complete (241.20s)\\n                   Format  mAP@0.5:0.95  Inference time (ms)\\n0                 PyTorch        0.4623               127.61\\n1             TorchScript        0.4623               131.23\\n2                    ONNX        0.4623                69.34\\n3                OpenVINO        0.4623                66.52\\n4                TensorRT           NaN                  NaN\\n5                  CoreML           NaN                  NaN\\n6   TensorFlow SavedModel        0.4623               123.79\\n7     TensorFlow GraphDef        0.4623               121.57\\n8         TensorFlow Lite        0.4623               316.61\\n9     TensorFlow Edge TPU           NaN                  NaN\\n10          TensorFlow.js           NaN                  NaN\\n```\\n\\n## Export a Trained YOLOv5 Model\\n\\nThis command exports a pretrained YOLOv5s model to TorchScript and ONNX formats. `yolov5s.pt` is the \\'small\\' model, the second-smallest model available. Other options are `yolov5n.pt`, `yolov5m.pt`, `yolov5l.pt` and `yolov5x.pt`, along with their P6 counterparts i.e. `yolov5s6.pt` or you own custom training checkpoint i.e. `runs/exp/weights/best.pt`. For details on all available models please see our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints).\\n```bash\\npython export.py --weights yolov5s.pt --include torchscript onnx\\n```\\n\\n💡 ProTip: Add `--half` to export models at FP16 half precision for smaller file sizes\\n\\nOutput:\\n```bash\\nexport: data=data/coco128.yaml, weights=[\\'yolov5s.pt\\'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, train=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=12, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=[\\'torchscript\\', \\'onnx\\']\\nYOLOv5 🚀 v6.2-104-ge3e5122 Python-3.7.13 torch-1.12.1+cu113 CPU\\n\\nDownloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\\n100% 14.1M/14.1M [00:00<00:00, 274MB/s]\\n\\nFusing layers... \\nYOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients\\n\\nPyTorch: starting from yolov5s.pt with output shape (1, 25200, 85) (14.1 MB)\\n\\nTorchScript: starting export with torch 1.12.1+cu113...\\nTorchScript: export success ✅ 1.7s, saved as yolov5s.torchscript (28.1 MB)\\n\\nONNX: starting export with onnx 1.12.0...\\nONNX: export success ✅ 2.3s, saved as yolov5s.onnx (28.0 MB)\\n\\nExport complete (5.5s)\\nResults saved to /content/yolov5\\nDetect:          python detect.py --weights yolov5s.onnx \\nValidate:        python val.py --weights yolov5s.onnx \\nPyTorch Hub:     model = torch.hub.load(\\'ultralytics/yolov5\\', \\'custom\\', \\'yolov5s.onnx\\')\\nVisualize:       https://netron.app/\\n```\\n\\nThe 3 exported models will be saved alongside the original PyTorch model:\\n<p align=\"center\"><img width=\"700\" src=\"https://user-images.githubusercontent.com/26833433/122827190-57a8f880-d2e4-11eb-860e-dbb7f9fc57fb.png\"></p>\\n\\n[Netron Viewer](https://github.com/lutzroeder/netron) is recommended for visualizing exported models:\\n<p align=\"center\"><img width=\"850\" src=\"https://user-images.githubusercontent.com/26833433/191003260-f94011a7-5b2e-4fe3-93c1-e1a935e0a728.png\"></p>\\n\\n\\n## Exported Model Usage Examples\\n\\n`detect.py` runs inference on exported models:\\n```bash\\npython detect.py --weights yolov5s.pt                 # PyTorch\\n                           yolov5s.torchscript        # TorchScript\\n                           yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\\n                           yolov5s_openvino_model     # OpenVINO\\n                           yolov5s.engine             # TensorRT\\n                           yolov5s.mlmodel            # CoreML (macOS only)\\n                           yolov5s_saved_model        # TensorFlow SavedModel\\n                           yolov5s.pb                 # TensorFlow GraphDef\\n                           yolov5s.tflite             # TensorFlow Lite\\n                           yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\\n                           yolov5s_paddle_model       # PaddlePaddle\\n```\\n\\n`val.py` runs validation on exported models:\\n```bash\\npython val.py --weights yolov5s.pt                 # PyTorch\\n                        yolov5s.torchscript        # TorchScript\\n                        yolov5s.onnx               # ONNX Runtime or OpenCV DNN with --dnn\\n                        yolov5s_openvino_model     # OpenVINO\\n                        yolov5s.engine             # TensorRT\\n                        yolov5s.mlmodel            # CoreML (macOS Only)\\n                        yolov5s_saved_model        # TensorFlow SavedModel\\n                        yolov5s.pb                 # TensorFlow GraphDef\\n                        yolov5s.tflite             # TensorFlow Lite\\n                        yolov5s_edgetpu.tflite     # TensorFlow Edge TPU\\n                        yolov5s_paddle_model       # PaddlePaddle\\n```\\n\\nUse PyTorch Hub with exported YOLOv5 models:\\n``` python\\nimport torch\\n\\n# Model\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'custom\\', \\'yolov5s.pt\\')\\n                                                       \\'yolov5s.torchscript \\')       # TorchScript\\n                                                       \\'yolov5s.onnx\\')               # ONNX Runtime\\n                                                       \\'yolov5s_openvino_model\\')     # OpenVINO\\n                                                       \\'yolov5s.engine\\')             # TensorRT\\n                                                       \\'yolov5s.mlmodel\\')            # CoreML (macOS Only)\\n                                                       \\'yolov5s_saved_model\\')        # TensorFlow SavedModel\\n                                                       \\'yolov5s.pb\\')                 # TensorFlow GraphDef\\n                                                       \\'yolov5s.tflite\\')             # TensorFlow Lite\\n                                                       \\'yolov5s_edgetpu.tflite\\')     # TensorFlow Edge TPU\\n                                                       \\'yolov5s_paddle_model\\')       # PaddlePaddle\\n\\n# Images\\nimg = \\'https://ultralytics.com/images/zidane.jpg\\'  # or file, Path, PIL, OpenCV, numpy, list\\n\\n# Inference\\nresults = model(img)\\n\\n# Results\\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.\\n```\\n\\n## OpenCV DNN inference\\n\\nOpenCV inference with ONNX models:\\n```bash\\npython export.py --weights yolov5s.pt --include onnx\\n\\npython detect.py --weights yolov5s.onnx --dnn  # detect\\npython val.py --weights yolov5s.onnx --dnn  # validate\\n```\\n\\n## C++ Inference\\n\\nYOLOv5 OpenCV DNN C++ inference on exported ONNX model examples:\\n\\n- [https://github.com/Hexmagic/ONNX-yolov5/blob/master/src/test.cpp](https://github.com/Hexmagic/ONNX-yolov5/blob/master/src/test.cpp)\\n- [https://github.com/doleron/yolov5-opencv-cpp-python](https://github.com/doleron/yolov5-opencv-cpp-python)\\n\\nYOLOv5 OpenVINO C++ inference examples:\\n\\n- [https://github.com/dacquaviva/yolov5-openvino-cpp-python](https://github.com/dacquaviva/yolov5-openvino-cpp-python)\\n- [https://github.com/UNeedCryDear/yolov5-seg-opencv-dnn-cpp](https://github.com/UNeedCryDear/yolov5-seg-opencv-dnn-cpp)\\n\\n## TensorFlow.js Web Browser Inference\\n\\n- [https://aukerul-shuvo.github.io/YOLOv5_TensorFlow-JS/](https://aukerul-shuvo.github.io/YOLOv5_TensorFlow-JS/)\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.', metadata={'file_path': 'docs/yolov5/tutorials/model_export.md', 'file_name': 'model_export.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n📚 This guide explains how to apply **pruning** to YOLOv5 🚀 models.  \\nUPDATED 25 September 2022.\\n\\n## Before You Start\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n## Test Normally\\n\\nBefore pruning we want to establish a baseline performance to compare to. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. `yolov5x.pt` is the largest and most accurate model available. Other options are `yolov5s.pt`, `yolov5m.pt` and `yolov5l.pt`, or you own checkpoint from training a custom dataset `./weights/best.pt`. For details on all available models please see our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints).\\n```bash\\npython val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\\n```\\n\\nOutput:\\n```shell\\nval: data=/content/yolov5/data/coco.yaml, weights=[\\'yolov5x.pt\\'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\\nYOLOv5 🚀 v6.0-224-g4c40933 torch 1.10.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\\n\\nFusing layers... \\nModel Summary: 444 layers, 86705005 parameters, 0 gradients\\nval: Scanning \\'/content/datasets/coco/val2017.cache\\' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupt: 100% 5000/5000 [00:00<?, ?it/s]\\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:12<00:00,  2.16it/s]\\n                 all       5000      36335      0.732      0.628      0.683      0.496\\nSpeed: 0.1ms pre-process, 5.2ms inference, 1.7ms NMS per image at shape (32, 3, 640, 640)  # <--- base speed\\n\\nEvaluating pycocotools mAP... saving runs/val/exp2/yolov5x_predictions.json...\\n...\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.507  # <--- base mAP\\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.689\\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.552\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.345\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.559\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.652\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.381\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.630\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.682\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.526\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.731\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.829\\nResults saved to runs/val/exp\\n```\\n\\n## Test YOLOv5x on COCO (0.30 sparsity)\\n\\nWe repeat the above test with a pruned model by using the `torch_utils.prune()` command. We update `val.py` to prune YOLOv5x to 0.3 sparsity:\\n\\n<img width=\"894\" alt=\"Screenshot 2022-02-02 at 22 54 18\" src=\"https://user-images.githubusercontent.com/26833433/152243799-b0ac2777-b1a8-47b1-801a-2e4c93c06ead.png\">\\n\\n30% pruned output:\\n```bash\\nval: data=/content/yolov5/data/coco.yaml, weights=[\\'yolov5x.pt\\'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\\nYOLOv5 🚀 v6.0-224-g4c40933 torch 1.10.0+cu111 CUDA:0 (Tesla V100-SXM2-16GB, 16160MiB)\\n\\nFusing layers... \\nModel Summary: 444 layers, 86705005 parameters, 0 gradients\\nPruning model...  0.3 global sparsity\\nval: Scanning \\'/content/datasets/coco/val2017.cache\\' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupt: 100% 5000/5000 [00:00<?, ?it/s]\\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [01:11<00:00,  2.19it/s]\\n                 all       5000      36335      0.724      0.614      0.671      0.478\\nSpeed: 0.1ms pre-process, 5.2ms inference, 1.7ms NMS per image at shape (32, 3, 640, 640)  # <--- prune mAP\\n\\nEvaluating pycocotools mAP... saving runs/val/exp3/yolov5x_predictions.json...\\n...\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489  # <--- prune mAP\\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.677\\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.537\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.334\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.542\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.635\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.370\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.612\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.664\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.496\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.722\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.803\\nResults saved to runs/val/exp3\\n```\\n\\nIn the results we can observe that we have achieved a **sparsity of 30%** in our model after pruning, which means that 30% of the model\\'s weight parameters in `nn.Conv2d` layers are equal to 0. **Inference time is essentially unchanged**, while the model\\'s **AP and AR scores a slightly reduced**.\\n\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.', metadata={'file_path': 'docs/yolov5/tutorials/model_pruning_and_sparsity.md', 'file_name': 'model_pruning_and_sparsity.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n📚 This guide explains how to properly use **multiple** GPUs to train a dataset with YOLOv5 🚀 on single or multiple machine(s).  \\nUPDATED 25 December 2022.\\n\\n## Before You Start\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n💡 ProTip! **Docker Image** is recommended for all Multi-GPU trainings. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n💡 ProTip! `torch.distributed.run` replaces `torch.distributed.launch` in **PyTorch>=1.9**. See [docs](https://pytorch.org/docs/stable/distributed.html) for details.\\n\\n## Training\\n\\nSelect a pretrained model to start training from. Here we select [YOLOv5s](https://github.com/ultralytics/yolov5/blob/master/models/yolov5s.yaml), the smallest and fastest model available. See our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) for a full comparison of all models.  We will train this model with Multi-GPU on the [COCO](https://github.com/ultralytics/yolov5/blob/master/data/scripts/get_coco.sh) dataset.\\n\\n<p align=\"center\"><img width=\"700\" alt=\"YOLOv5 Models\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png\"></p>\\n\\n\\n### Single GPU\\n\\n```bash\\npython train.py  --batch 64 --data coco.yaml --weights yolov5s.pt --device 0\\n```\\n\\n### Multi-GPU [DataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.DataParallel) Mode (⚠️ not recommended)\\n\\nYou can increase the `device` to use Multiple GPUs in DataParallel mode.\\n```bash\\npython train.py  --batch 64 --data coco.yaml --weights yolov5s.pt --device 0,1\\n```\\n\\nThis method is slow and barely speeds up training compared to using just 1 GPU.\\n\\n### Multi-GPU [DistributedDataParallel](https://pytorch.org/docs/stable/nn.html#torch.nn.parallel.DistributedDataParallel) Mode (✅ recommended)\\n\\nYou will have to pass `python -m torch.distributed.run --nproc_per_node`, followed by the usual arguments.\\n\\n```bash\\npython -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --weights yolov5s.pt --device 0,1\\n```\\n\\n`--nproc_per_node` specifies how many GPUs you would like to use. In the example above, it is 2.\\n`--batch ` is the total batch-size. It will be divided evenly to each GPU. In the example above, it is 64/2=32 per GPU.\\n\\nThe code above will use GPUs `0... (N-1)`.\\n\\n<details markdown>\\n  <summary>Use specific GPUs (click to expand)</summary>\\n\\nYou can do so by simply passing `--device` followed by your specific GPUs. For example, in the code below, we will use GPUs `2,3`.\\n\\n```bash\\npython -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights \\'\\' --device 2,3\\n```\\n\\n</details>\\n\\n<details markdown>\\n  <summary>Use SyncBatchNorm (click to expand)</summary>\\n\\n[SyncBatchNorm](https://pytorch.org/docs/master/generated/torch.nn.SyncBatchNorm.html) could increase accuracy for multiple gpu training, however, it will slow down training by a significant factor. It is **only** available for Multiple GPU DistributedDataParallel training. \\n\\nIt is best used when the batch-size on **each** GPU is small (<= 8).\\n\\nTo use SyncBatchNorm, simple pass `--sync-bn` to the command like below, \\n\\n```bash\\npython -m torch.distributed.run --nproc_per_node 2 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights \\'\\' --sync-bn\\n```\\n</details>\\n\\n<details markdown>\\n  <summary>Use Multiple machines (click to expand)</summary>\\n\\nThis is **only** available for Multiple GPU DistributedDataParallel training. \\n\\nBefore we continue, make sure the files on all machines are the same, dataset, codebase, etc. Afterwards, make sure the machines can communicate to each other.\\n\\nYou will have to choose a master machine(the machine that the others will talk to). Note down its address(`master_addr`) and choose a port(`master_port`). I will use `master_addr = 192.168.1.1` and `master_port = 1234` for the example below.\\n\\nTo use it, you can do as the following,\\n\\n```bash\\n# On master machine 0\\npython -m torch.distributed.run --nproc_per_node G --nnodes N --node_rank 0 --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights \\'\\'\\n```\\n```bash\\n# On machine R\\npython -m torch.distributed.run --nproc_per_node G --nnodes N --node_rank R --master_addr \"192.168.1.1\" --master_port 1234 train.py --batch 64 --data coco.yaml --cfg yolov5s.yaml --weights \\'\\'\\n```\\nwhere `G` is number of GPU per machine, `N` is the number of machines, and `R` is the machine number from `0...(N-1)`. \\nLet\\'s say I have two machines with two GPUs each, it would be `G = 2` , `N = 2`, and `R = 1` for the above.\\n\\nTraining will not start until <b>all </b> `N` machines are connected. Output will only be shown on master machine!\\n\\n</details>\\n\\n\\n### Notes\\n\\n- Windows support is untested, Linux is recommended.\\n- `--batch ` must be a multiple of the number of GPUs.\\n- GPU 0 will take slightly more memory than the other GPUs as it maintains EMA and is responsible for checkpointing etc.\\n- If you get `RuntimeError: Address already in use`, it could be because you are running multiple trainings at a time. To fix this, simply use a different port number by adding `--master_port` like below,\\n\\n```bash\\npython -m torch.distributed.run --master_port 1234 --nproc_per_node 2 ...\\n```\\n\\n## Results\\n\\nDDP profiling results on an [AWS EC2 P4d instance](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/) with 8x A100 SXM4-40GB for YOLOv5l for 1 COCO epoch.\\n\\n<details markdown>\\n  <summary>Profiling code</summary>\\n\\n```bash\\n# prepare\\nt=ultralytics/yolov5:latest && sudo docker pull $t && sudo docker run -it --ipc=host --gpus all -v \"$(pwd)\"/coco:/usr/src/coco $t\\npip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\\ncd .. && rm -rf app && git clone https://github.com/ultralytics/yolov5 -b master app && cd app\\ncp data/coco.yaml data/coco_profile.yaml\\n\\n# profile\\npython train.py --batch-size 16 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0 \\npython -m torch.distributed.run --nproc_per_node 2 train.py --batch-size 32 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1   \\npython -m torch.distributed.run --nproc_per_node 4 train.py --batch-size 64 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1,2,3  \\npython -m torch.distributed.run --nproc_per_node 8 train.py --batch-size 128 --data coco_profile.yaml --weights yolov5l.pt --epochs 1 --device 0,1,2,3,4,5,6,7\\n```\\n\\n</details>\\n\\n| GPUs<br>A100 | batch-size | CUDA_mem<br><sup>device0 (G) | COCO<br><sup>train | COCO<br><sup>val |\\n|--------------|------------|------------------------------|--------------------|------------------|\\n| 1x           | 16         | 26GB                         | 20:39              | 0:55             |\\n| 2x           | 32         | 26GB                         | 11:43              | 0:57             |\\n| 4x           | 64         | 26GB                         | 5:57               | 0:55             |\\n| 8x           | 128        | 26GB                         | 3:09               | 0:57             |\\n\\n## FAQ\\n\\nIf an error occurs, please read the checklist below first! (It could save your time)\\n\\n<details markdown>\\n  <summary>Checklist (click to expand) </summary>\\n\\n<ul>\\n    <li>Have you properly read this post?  </li>\\n    <li>Have you tried to reclone the codebase? The code changes <b>daily</b>.</li>\\n    <li>Have you tried to search for your error? Someone may have already encountered it in this repo or in another and have the solution. </li>\\n    <li>Have you installed all the requirements listed on top (including the correct Python and Pytorch versions)? </li>\\n    <li>Have you tried in other environments listed in the \"Environments\" section below? </li>\\n    <li>Have you tried with another dataset like coco128 or coco2017? It will make it easier to find the root cause. </li>\\n</ul>\\n\\nIf you went through all the above, feel free to raise an Issue by giving as much detail as possible following the template.\\n\\n</details>\\n\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\\n\\n\\n## Credits\\n\\nI would like to thank @MagicFrogSJTU, who did all the heavy lifting, and @glenn-jocher for guiding us along the way.', metadata={'file_path': 'docs/yolov5/tutorials/multi_gpu_training.md', 'file_name': 'multi_gpu_training.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n<!--\\nCopyright (c) 2021 - present / Neuralmagic, Inc. All Rights Reserved.\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\");\\nyou may not use this file except in compliance with the License.\\nYou may obtain a copy of the License at\\n\\n   http://www.apache.org/licenses/LICENSE-2.0\\n\\nUnless required by applicable law or agreed to in writing,\\nsoftware distributed under the License is distributed on an \"AS IS\" BASIS,\\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\\nSee the License for the specific language governing permissions and\\nlimitations under the License.\\n-->\\n\\nWelcome to software-delivered AI.\\n\\nThis guide explains how to deploy YOLOv5 with Neural Magic\\'s DeepSparse.\\n\\nDeepSparse is an inference runtime with exceptional performance on CPUs. For instance, compared to the ONNX Runtime baseline, DeepSparse offers a 5.8x speed-up for YOLOv5s, running on the same machine!\\n\\n<p align=\"center\">\\n  <img width=\"60%\" src=\"https://github.com/neuralmagic/deepsparse/raw/main/examples/ultralytics-yolo/ultralytics-readmes/performance-chart-5.8x.png\">\\n</p>\\n\\nFor the first time, your deep learning workloads can meet the performance demands of production without the complexity and costs of hardware accelerators.\\nPut simply, DeepSparse gives you the performance of GPUs and the simplicity of software:\\n- **Flexible Deployments**: Run consistently across cloud, data center, and edge with any hardware provider from Intel to AMD to ARM\\n- **Infinite Scalability**: Scale vertically to 100s of cores, out with standard Kubernetes, or fully-abstracted with Serverless\\n- **Easy Integration**: Clean APIs for integrating your model into an application and monitoring it in production\\n\\n### How Does DeepSparse Achieve GPU-Class Performance?\\n\\nDeepSparse takes advantage of model sparsity to gain its performance speedup. \\n\\nSparsification through pruning and quantization is a broadly studied technique, allowing order-of-magnitude reductions in the size and compute needed to \\nexecute a network, while maintaining high accuracy. DeepSparse is sparsity-aware, meaning it skips the zeroed out parameters, shrinking amount of compute\\nin a forward pass. Since the sparse computation is now memory bound, DeepSparse executes the network depth-wise, breaking the problem into Tensor Columns, \\nvertical stripes of computation that fit in cache.\\n\\n<p align=\"center\">\\n  <img width=\"60%\" src=\"https://github.com/neuralmagic/deepsparse/raw/main/examples/ultralytics-yolo/ultralytics-readmes/tensor-columns.png\">\\n</p>\\n\\nSparse networks with compressed computation, executed depth-wise in cache, allows DeepSparse to deliver GPU-class performance on CPUs!\\n\\n### How Do I Create A Sparse Version of YOLOv5 Trained on My Data?\\n\\nNeural Magic\\'s open-source model repository, SparseZoo, contains pre-sparsified checkpoints of each YOLOv5 model. Using SparseML, which is integrated with Ultralytics, you can fine-tune a sparse checkpoint onto your data with a single CLI command.\\n\\n[Checkout Neural Magic\\'s YOLOv5 documentation for more details](https://docs.neuralmagic.com/use-cases/object-detection/sparsifying).\\n\\n## DeepSparse Usage\\n\\nWe will walk through an example benchmarking and deploying a sparse version of YOLOv5s with DeepSparse.\\n\\n### Install DeepSparse\\n\\nRun the following to install DeepSparse. We recommend you use a virtual environment with Python.\\n\\n```bash\\npip install deepsparse[server,yolo,onnxruntime]\\n```\\n\\n### Collect an ONNX File\\n\\nDeepSparse accepts a model in the ONNX format, passed either as:\\n- A SparseZoo stub which identifies an ONNX file in the SparseZoo\\n- A local path to an ONNX model in a filesystem\\n\\nThe examples below use the standard dense and pruned-quantized YOLOv5s checkpoints, identified by the following SparseZoo stubs:\\n```bash\\nzoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none\\nzoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\\n```\\n\\n### Deploy a Model\\n\\nDeepSparse offers convenient APIs for integrating your model into an application.  \\n\\nTo try the deployment examples below, pull down a sample image and save it as `basilica.jpg` with the following:\\n```bash\\nwget -O basilica.jpg https://raw.githubusercontent.com/neuralmagic/deepsparse/main/src/deepsparse/yolo/sample_images/basilica.jpg\\n```\\n\\n#### Python API\\n  \\n`Pipelines` wrap pre-processing and output post-processing around the runtime, providing a clean interface for adding DeepSparse to an application. \\nThe DeepSparse-Ultralytics integration includes an out-of-the-box `Pipeline` that accepts raw images and outputs the bounding boxes.\\n\\nCreate a `Pipeline` and run inference:\\n\\n```python\\nfrom deepsparse import Pipeline\\n\\n# list of images in local filesystem\\nimages = [\"basilica.jpg\"]\\n\\n# create Pipeline\\nmodel_stub = \"zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\"\\nyolo_pipeline = Pipeline.create(\\n    task=\"yolo\",\\n    model_path=model_stub,\\n)\\n\\n# run inference on images, receive bounding boxes + classes\\npipeline_outputs = yolo_pipeline(images=images, iou_thres=0.6, conf_thres=0.001)\\nprint(pipeline_outputs)\\n```\\n\\nIf you are running in the cloud, you may get an error that open-cv cannot find `libGL.so.1`. Running the following on Ubuntu installs it:\\n\\n```\\napt-get install libgl1-mesa-glx\\n```\\n\\n#### HTTP Server\\n  \\nDeepSparse Server runs on top of the popular FastAPI web framework and Uvicorn web server. With just a single CLI command, you can easily setup a model \\nservice endpoint with DeepSparse. The Server supports any Pipeline from DeepSparse, including object detection with YOLOv5, enabling you to send raw \\nimages to the endpoint and receive the bounding boxes.\\n\\nSpin up the Server with the pruned-quantized YOLOv5s:\\n\\n```bash\\ndeepsparse.server \\\\\\n    --task yolo \\\\\\n    --model_path zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\\n```\\n\\nAn example request, using Python\\'s `requests` package:\\n```python\\nimport requests, json\\n\\n# list of images for inference (local files on client side)\\npath = [\\'basilica.jpg\\'] \\nfiles = [(\\'request\\', open(img, \\'rb\\')) for img in path]\\n\\n# send request over HTTP to /predict/from_files endpoint\\nurl = \\'http://0.0.0.0:5543/predict/from_files\\'\\nresp = requests.post(url=url, files=files)\\n\\n# response is returned in JSON\\nannotations = json.loads(resp.text) # dictionary of annotation results\\nbounding_boxes = annotations[\"boxes\"]\\nlabels = annotations[\"labels\"]\\n```\\n\\n#### Annotate CLI\\nYou can also use the annotate command to have the engine save an annotated photo on disk. Try --source 0 to annotate your live webcam feed!\\n```bash\\ndeepsparse.object_detection.annotate --model_filepath zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none --source basilica.jpg\\n```\\n\\nRunning the above command will create an `annotation-results` folder and save the annotated image inside.\\n\\n<p align = \"center\">\\n<img src=\"https://github.com/neuralmagic/deepsparse/raw/d31f02596ebff2ec62761d0bc9ca14c4663e8858/src/deepsparse/yolo/sample_images/basilica-annotated.jpg\" alt=\"annotated\" width=\"60%\"/>\\n</p>\\n\\n## Benchmarking Performance\\n\\nWe will compare DeepSparse\\'s throughput to ONNX Runtime\\'s throughput on YOLOv5s, using DeepSparse\\'s benchmarking script.\\n\\nThe benchmarks were run on an AWS `c6i.8xlarge` instance (16 cores). \\n\\n### Batch 32 Performance Comparison\\n\\n#### ONNX Runtime Baseline\\n\\nAt batch 32, ONNX Runtime achieves 42 images/sec with the standard dense YOLOv5s:\\n\\n```bash\\ndeepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none -s sync -b 32 -nstreams 1 -e onnxruntime\\n\\n> Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none\\n> Batch Size: 32\\n> Scenario: sync\\n> Throughput (items/sec): 41.9025\\n```\\n\\n#### DeepSparse Dense Performance\\n\\nWhile DeepSparse offers its best performance with optimized sparse models, it also performs well with the standard dense YOLOv5s. \\n\\nAt batch 32, DeepSparse achieves 70 images/sec with the standard dense YOLOv5s, a **1.7x performance improvement over ORT**!\\n\\n```bash\\ndeepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none -s sync -b 32 -nstreams 1\\n\\n> Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none\\n> Batch Size: 32\\n> Scenario: sync\\n> Throughput (items/sec): 69.5546\\n```\\n#### DeepSparse Sparse Performance\\n\\nWhen sparsity is applied to the model, DeepSparse\\'s performance gains over ONNX Runtime is even stronger.\\n\\nAt batch 32, DeepSparse achieves 241 images/sec with the pruned-quantized YOLOv5s, a **5.8x performance improvement over ORT**!\\n\\n```bash\\ndeepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none -s sync -b 32 -nstreams 1\\n\\n> Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\\n> Batch Size: 32\\n> Scenario: sync\\n> Throughput (items/sec): 241.2452\\n```\\n\\n### Batch 1 Performance Comparison\\n\\nDeepSparse is also able to gain a speed-up over ONNX Runtime for the latency-sensitive, batch 1 scenario.\\n\\n#### ONNX Runtime Baseline\\nAt batch 1, ONNX Runtime achieves 48 images/sec with the standard, dense YOLOv5s.\\n\\n```bash\\ndeepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none -s sync -b 1 -nstreams 1 -e onnxruntime\\n\\n> Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/base-none\\n> Batch Size: 1\\n> Scenario: sync\\n> Throughput (items/sec): 48.0921\\n```\\n\\n#### DeepSparse Sparse Performance\\n\\nAt batch 1, DeepSparse achieves 135 items/sec with a pruned-quantized YOLOv5s, **a 2.8x performance gain over ONNX Runtime!**\\n\\n```bash\\ndeepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none -s sync -b 1 -nstreams 1\\n\\n> Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned65_quant-none\\n> Batch Size: 1\\n> Scenario: sync\\n> Throughput (items/sec): 134.9468\\n```\\n\\nSince `c6i.8xlarge` instances have VNNI instructions, DeepSparse\\'s throughput can be pushed further if weights are pruned in blocks of 4. \\n\\nAt batch 1, DeepSparse achieves 180 items/sec with a 4-block pruned-quantized YOLOv5s, a **3.7x performance gain over ONNX Runtime!**\\n\\n```bash\\ndeepsparse.benchmark zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned35_quant-none-vnni -s sync -b 1 -nstreams 1\\n\\n> Original Model Path: zoo:cv/detection/yolov5-s/pytorch/ultralytics/coco/pruned35_quant-none-vnni\\n> Batch Size: 1\\n> Scenario: sync\\n> Throughput (items/sec): 179.7375\\n```\\n\\n## Get Started With DeepSparse\\n\\n**Research or Testing?** DeepSparse Community is free for research and testing. Get started with our [Documentation](https://docs.neuralmagic.com/).\\n', metadata={'file_path': 'docs/yolov5/tutorials/neural_magic_pruning_quantization.md', 'file_name': 'neural_magic_pruning_quantization.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n📚 This guide explains how to load YOLOv5 🚀 from PyTorch Hub  at [https://pytorch.org/hub/ultralytics_yolov5](https://pytorch.org/hub/ultralytics_yolov5).  \\nUPDATED 26 March 2023.\\n\\n## Before You Start\\n\\nInstall [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\npip install -r https://raw.githubusercontent.com/ultralytics/yolov5/master/requirements.txt\\n```\\n\\n💡 ProTip: Cloning [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5) is **not** required 😃\\n\\n## Load YOLOv5 with PyTorch Hub\\n\\n### Simple Example\\n\\nThis example loads a pretrained YOLOv5s model from PyTorch Hub as `model` and passes an image for inference. `\\'yolov5s\\'` is the lightest and fastest YOLOv5 model. For details on all available models please see the [README](https://github.com/ultralytics/yolov5#pretrained-checkpoints).\\n```python\\nimport torch\\n\\n# Model\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\')\\n\\n# Image\\nim = \\'https://ultralytics.com/images/zidane.jpg\\'\\n\\n# Inference\\nresults = model(im)\\n\\nresults.pandas().xyxy[0]\\n#      xmin    ymin    xmax   ymax  confidence  class    name\\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\\n```\\n\\n\\n### Detailed Example\\n\\nThis example shows **batched inference** with **PIL** and **OpenCV** image sources. `results` can be **printed** to console, **saved** to `runs/hub`, **showed** to screen on supported environments, and returned as **tensors** or **pandas** dataframes.\\n```python\\nimport cv2\\nimport torch\\nfrom PIL import Image\\n\\n# Model\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\')\\n\\n# Images\\nfor f in \\'zidane.jpg\\', \\'bus.jpg\\':\\n    torch.hub.download_url_to_file(\\'https://ultralytics.com/images/\\' + f, f)  # download 2 images\\nim1 = Image.open(\\'zidane.jpg\\')  # PIL image\\nim2 = cv2.imread(\\'bus.jpg\\')[..., ::-1]  # OpenCV image (BGR to RGB)\\n\\n# Inference\\nresults = model([im1, im2], size=640) # batch of images\\n\\n# Results\\nresults.print()  \\nresults.save()  # or .show()\\n\\nresults.xyxy[0]  # im1 predictions (tensor)\\nresults.pandas().xyxy[0]  # im1 predictions (pandas)\\n#      xmin    ymin    xmax   ymax  confidence  class    name\\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\\n```\\n<img src=\"https://user-images.githubusercontent.com/26833433/124915064-62a49e00-dff1-11eb-86b3-a85b97061afb.jpg\" width=\"500\">  <img src=\"https://user-images.githubusercontent.com/26833433/124915055-60424400-dff1-11eb-9055-24585b375a29.jpg\" width=\"300\">\\n\\nFor all inference options see YOLOv5 `AutoShape()` forward [method](https://github.com/ultralytics/yolov5/blob/30e4c4f09297b67afedf8b2bcd851833ddc9dead/models/common.py#L243-L252).\\n\\n### Inference Settings\\nYOLOv5 models contain various inference attributes such as **confidence threshold**, **IoU threshold**, etc. which can be set by:\\n```python\\nmodel.conf = 0.25  # NMS confidence threshold\\n      iou = 0.45  # NMS IoU threshold\\n      agnostic = False  # NMS class-agnostic\\n      multi_label = False  # NMS multiple labels per box\\n      classes = None  # (optional list) filter by class, i.e. = [0, 15, 16] for COCO persons, cats and dogs\\n      max_det = 1000  # maximum number of detections per image\\n      amp = False  # Automatic Mixed Precision (AMP) inference\\n\\nresults = model(im, size=320)  # custom inference size\\n```\\n\\n\\n### Device\\nModels can be transferred to any device after creation:\\n```python\\nmodel.cpu()  # CPU\\nmodel.cuda()  # GPU\\nmodel.to(device)  # i.e. device=torch.device(0)\\n```\\n\\nModels can also be created directly on any `device`:\\n```python\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', device=\\'cpu\\')  # load on CPU\\n```\\n\\n💡 ProTip: Input images are automatically transferred to the correct model device before inference.\\n\\n### Silence Outputs\\nModels can be loaded silently with `_verbose=False`:\\n```python\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', _verbose=False)  # load silently\\n```\\n\\n### Input Channels\\nTo load a pretrained YOLOv5s model with 4 input channels rather than the default 3:\\n```python\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', channels=4)\\n```\\nIn this case the model will be composed of pretrained weights **except for** the very first input layer, which is no longer the same shape as the pretrained input layer. The input layer will remain initialized by random weights.\\n\\n### Number of Classes\\nTo load a pretrained YOLOv5s model with 10 output classes rather than the default 80:\\n```python\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', classes=10)\\n```\\nIn this case the model will be composed of pretrained weights **except for** the output layers, which are no longer the same shape as the pretrained output layers. The output layers will remain initialized by random weights.\\n\\n### Force Reload\\nIf you run into problems with the above steps, setting `force_reload=True` may help by discarding the existing cache and force a fresh download of the latest YOLOv5 version from PyTorch Hub.\\n```python\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', force_reload=True)  # force reload\\n```\\n\\n### Screenshot Inference\\nTo run inference on your desktop screen:\\n```python\\nimport torch\\nfrom PIL import ImageGrab\\n\\n# Model\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\')\\n\\n# Image\\nim = ImageGrab.grab()  # take a screenshot\\n\\n# Inference\\nresults = model(im)\\n```\\n\\n### Multi-GPU Inference\\n\\nYOLOv5 models can be loaded to multiple GPUs in parallel with threaded inference:\\n\\n```python\\nimport torch\\nimport threading\\n\\ndef run(model, im):\\n  results = model(im)\\n  results.save()\\n\\n# Models\\nmodel0 = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', device=0)\\nmodel1 = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', device=1)\\n\\n# Inference\\nthreading.Thread(target=run, args=[model0, \\'https://ultralytics.com/images/zidane.jpg\\'], daemon=True).start()\\nthreading.Thread(target=run, args=[model1, \\'https://ultralytics.com/images/bus.jpg\\'], daemon=True).start()\\n```\\n\\n### Training\\nTo load a YOLOv5 model for training rather than inference, set `autoshape=False`. To load a model with randomly initialized weights (to train from scratch) use `pretrained=False`. You must provide your own training script in this case. Alternatively see  our YOLOv5 [Train Custom Data Tutorial](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data) for model training.\\n```python\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', autoshape=False)  # load pretrained\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\', autoshape=False, pretrained=False)  # load scratch\\n```\\n\\n### Base64 Results\\nFor use with API services. See https://github.com/ultralytics/yolov5/pull/2291 and [Flask REST API](https://github.com/ultralytics/yolov5/tree/master/utils/flask_rest_api) example for details.\\n```python\\nresults = model(im)  # inference\\n\\nresults.ims # array of original images (as np array) passed to model for inference\\nresults.render()  # updates results.ims with boxes and labels\\nfor im in results.ims:\\n    buffered = BytesIO()\\n    im_base64 = Image.fromarray(im)\\n    im_base64.save(buffered, format=\"JPEG\")\\n    print(base64.b64encode(buffered.getvalue()).decode(\\'utf-8\\'))  # base64 encoded image with results\\n```\\n\\n### Cropped Results\\nResults can be returned and saved as detection crops:\\n```python\\nresults = model(im)  # inference\\ncrops = results.crop(save=True)  # cropped detections dictionary\\n```\\n\\n### Pandas Results\\nResults can be returned as [Pandas DataFrames](https://pandas.pydata.org/):\\n```python\\nresults = model(im)  # inference\\nresults.pandas().xyxy[0]  # Pandas DataFrame\\n```\\n<details markdown>\\n  <summary>Pandas Output (click to expand)</summary>\\n\\n```python\\nprint(results.pandas().xyxy[0])\\n#      xmin    ymin    xmax   ymax  confidence  class    name\\n# 0  749.50   43.50  1148.0  704.5    0.874023      0  person\\n# 1  433.50  433.50   517.5  714.5    0.687988     27     tie\\n# 2  114.75  195.75  1095.0  708.0    0.624512      0  person\\n# 3  986.00  304.00  1028.0  420.0    0.286865     27     tie\\n```\\n</details>\\n\\n### Sorted Results\\nResults can be sorted by column, i.e. to sort license plate digit detection left-to-right (x-axis):\\n```python\\nresults = model(im)  # inference\\nresults.pandas().xyxy[0].sort_values(\\'xmin\\')  # sorted left-right\\n```\\n\\n### Box-Cropped Results\\nResults can be returned and saved as detection crops:\\n```python\\nresults = model(im)  # inference\\ncrops = results.crop(save=True)  # cropped detections dictionary\\n```\\n\\n### JSON Results\\nResults can be returned in JSON format once converted to `.pandas()` dataframes using the `.to_json()` method. The JSON format can be modified using the `orient` argument. See pandas `.to_json()` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html) for details.\\n```python\\nresults = model(ims)  # inference\\nresults.pandas().xyxy[0].to_json(orient=\"records\")  # JSON img1 predictions\\n```\\n\\n<details markdown>\\n  <summary>JSON Output (click to expand)</summary>\\n\\n```json\\n[\\n{\"xmin\":749.5,\"ymin\":43.5,\"xmax\":1148.0,\"ymax\":704.5,\"confidence\":0.8740234375,\"class\":0,\"name\":\"person\"},\\n{\"xmin\":433.5,\"ymin\":433.5,\"xmax\":517.5,\"ymax\":714.5,\"confidence\":0.6879882812,\"class\":27,\"name\":\"tie\"},\\n{\"xmin\":115.25,\"ymin\":195.75,\"xmax\":1096.0,\"ymax\":708.0,\"confidence\":0.6254882812,\"class\":0,\"name\":\"person\"},\\n{\"xmin\":986.0,\"ymin\":304.0,\"xmax\":1028.0,\"ymax\":420.0,\"confidence\":0.2873535156,\"class\":27,\"name\":\"tie\"}\\n]\\n```\\n\\n</details>\\n\\n## Custom Models\\nThis example loads a custom 20-class [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml)-trained YOLOv5s model `\\'best.pt\\'` with PyTorch Hub.\\n```python\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'custom\\', path=\\'path/to/best.pt\\')  # local model\\nmodel = torch.hub.load(\\'path/to/yolov5\\', \\'custom\\', path=\\'path/to/best.pt\\', source=\\'local\\')  # local repo\\n```\\n\\n## TensorRT, ONNX and OpenVINO Models\\n\\nPyTorch Hub supports inference on most YOLOv5 export formats, including custom trained models. See [TFLite, ONNX, CoreML, TensorRT Export tutorial](https://docs.ultralytics.com/yolov5/tutorials/model_export) for details on exporting models.\\n\\n💡 ProTip: **TensorRT** may be up to 2-5X faster than PyTorch on [**GPU benchmarks**](https://github.com/ultralytics/yolov5/pull/6963)  \\n💡 ProTip: **ONNX** and **OpenVINO** may be up to 2-3X faster than PyTorch on [**CPU benchmarks**](https://github.com/ultralytics/yolov5/pull/6613)\\n\\n```python\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'custom\\', path=\\'yolov5s.pt\\')  # PyTorch\\n                                                            \\'yolov5s.torchscript\\')  # TorchScript\\n                                                            \\'yolov5s.onnx\\')  # ONNX\\n                                                            \\'yolov5s_openvino_model/\\')  # OpenVINO\\n                                                            \\'yolov5s.engine\\')  # TensorRT\\n                                                            \\'yolov5s.mlmodel\\')  # CoreML (macOS-only)\\n                                                            \\'yolov5s.tflite\\')  # TFLite\\n                                                            \\'yolov5s_paddle_model/\\')  # PaddlePaddle\\n```\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.', metadata={'file_path': 'docs/yolov5/tutorials/pytorch_hub_model_loading.md', 'file_name': 'pytorch_hub_model_loading.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Roboflow Datasets\\n\\nYou can now use Roboflow to organize, label, prepare, version, and host your datasets for training YOLOv5 🚀 models. Roboflow is free to use with YOLOv5 if you make your workspace public.  \\nUPDATED 30 September 2021.\\n\\n## Upload\\nYou can upload your data to Roboflow via [web UI](https://docs.roboflow.com/adding-data), [rest API](https://docs.roboflow.com/adding-data/upload-api), or [python](https://docs.roboflow.com/python).\\n\\n## Labeling\\nAfter uploading data to Roboflow, you can label your data and review previous labels.\\n\\n[![Roboflow Annotate](https://roboflow-darknet.s3.us-east-2.amazonaws.com/roboflow-annotate.gif)](https://roboflow.com/annotate)\\n\\n## Versioning\\nYou can make versions of your dataset with different preprocessing and offline augmentation options. YOLOv5 does online augmentations natively, so be intentional when layering Roboflow\\'s offline augs on top.\\n\\n![Roboflow Preprocessing](https://roboflow-darknet.s3.us-east-2.amazonaws.com/robolfow-preprocessing.png)\\n\\n## Exporting Data\\nYou can download your data in YOLOv5 format to quickly begin training.\\n\\n```\\nfrom roboflow import Roboflow\\nrf = Roboflow(api_key=\"YOUR API KEY HERE\")\\nproject = rf.workspace().project(\"YOUR PROJECT\")\\ndataset = project.version(\"YOUR VERSION\").download(\"yolov5\")\\n```\\n\\n## Custom Training\\nWe have released a custom training tutorial demonstrating all of the above capabilities. You can access the code here:\\n\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/yolov5-custom-training-tutorial/blob/main/yolov5-custom-training.ipynb)\\n\\n## Active Learning\\nThe real world is messy and your model will invariably encounter situations your dataset didn\\'t anticipate. Using [active learning](https://blog.roboflow.com/what-is-active-learning/) is an important strategy to iteratively improve your dataset and model. With the Roboflow and YOLOv5 integration, you can quickly make improvements on your model deployments by using a battle tested machine learning pipeline.\\n\\n<p align=\"\"><a href=\"https://roboflow.com/?ref=ultralytics\"><img width=\"1000\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png\"/></a></p>\\n', metadata={'file_path': 'docs/yolov5/tutorials/roboflow_datasets_integration.md', 'file_name': 'roboflow_datasets_integration.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Deploy on NVIDIA Jetson using TensorRT and DeepStream SDK\\n\\n📚 This guide explains how to deploy a trained model into NVIDIA Jetson Platform and perform inference using TensorRT and DeepStream SDK. Here we use TensorRT to maximize the inference performance on the Jetson platform.  \\nUPDATED 18 November 2022. \\n\\n## Hardware Verification\\n\\nWe have tested and verified this guide on the following Jetson devices\\n\\n- [Seeed reComputer J1010 built with Jetson Nano module](https://www.seeedstudio.com/Jetson-10-1-A0-p-5336.html)\\n- [Seeed reComputer J2021 built with Jetson Xavier NX module](https://www.seeedstudio.com/reComputer-J2021-p-5438.html)\\n\\n## Before You Start\\n\\nMake sure you have properly installed **JetPack SDK** with all the **SDK Components** and **DeepStream SDK** on the Jetson device as this includes CUDA, TensorRT and DeepStream SDK which are needed for this guide.\\n\\nJetPack SDK provides a full development environment for hardware-accelerated AI-at-the-edge development. All Jetson modules and developer kits are supported by JetPack SDK.\\n\\nThere are two major installation methods including,\\n\\n1. SD Card Image Method\\n2. NVIDIA SDK Manager Method\\n\\nYou can find a very detailed installation guide from NVIDIA [official website](https://developer.nvidia.com/jetpack-sdk-461). You can also find guides corresponding to the above-mentioned [reComputer J1010](https://wiki.seeedstudio.com/reComputer_J1010_J101_Flash_Jetpack) and [reComputer J2021](https://wiki.seeedstudio.com/reComputer_J2021_J202_Flash_Jetpack).\\n\\n\\n## Install Necessary Packages \\n\\n- **Step 1.** Access the terminal of Jetson device, install pip and upgrade it\\n\\n```sh\\nsudo apt update\\nsudo apt install -y python3-pip\\npip3 install --upgrade pip\\n```\\n\\n- **Step 2.** Clone the following repo\\n\\n```sh\\ngit clone https://github.com/ultralytics/yolov5\\n```\\n\\n- **Step 3.** Open **requirements.txt**\\n\\n```sh\\ncd yolov5\\nvi requirements.txt\\n```\\n\\n- **Step 5.** Edit the following lines. Here you need to press **i** first to enter editing mode. Press **ESC**, then type **:wq** to save and quit\\n\\n```sh\\n# torch>=1.7.0\\n# torchvision>=0.8.1\\n```\\n\\n**Note:** torch and torchvision are excluded for now because they will be installed later.\\n\\n- **Step 6.** install the below dependency\\n\\n```sh\\nsudo apt install -y libfreetype6-dev\\n```\\n\\n- **Step 7.** Install the necessary packages\\n\\n```sh\\npip3 install -r requirements.txt\\n```\\n\\n## Install PyTorch and Torchvision\\n\\nWe cannot install PyTorch and Torchvision from pip because they are not compatible to run on Jetson platform which is based on **ARM aarch64 architecture**. Therefore, we need to manually install pre-built PyTorch pip wheel and compile/ install Torchvision from source.\\n\\nVisit [this page](https://forums.developer.nvidia.com/t/pytorch-for-jetson) to access all the PyTorch and Torchvision links.\\n\\nHere are some of the versions supported by JetPack 4.6 and above.\\n\\n**PyTorch v1.10.0**\\n\\nSupported by JetPack 4.4 (L4T R32.4.3) / JetPack 4.4.1 (L4T R32.4.4) / JetPack 4.5 (L4T R32.5.0) / JetPack 4.5.1 (L4T R32.5.1) / JetPack 4.6 (L4T R32.6.1) with Python 3.6\\n\\n**file_name:** torch-1.10.0-cp36-cp36m-linux_aarch64.whl\\n**URL:** [https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl](https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl)\\n\\n**PyTorch v1.12.0**\\n\\nSupported by JetPack 5.0 (L4T R34.1.0) / JetPack 5.0.1 (L4T R34.1.1) / JetPack 5.0.2 (L4T R35.1.0) with Python 3.8 \\n\\n**file_name:** torch-1.12.0a0+2c916ef.nv22.3-cp38-cp38-linux_aarch64.whl\\n**URL:** [https://developer.download.nvidia.com/compute/redist/jp/v50/pytorch/torch-1.12.0a0+2c916ef.nv22.3-cp38-cp38-linux_aarch64.whl](https://developer.download.nvidia.com/compute/redist/jp/v50/pytorch/torch-1.12.0a0+2c916ef.nv22.3-cp38-cp38-linux_aarch64.whl)\\n\\n- **Step 1.** Install torch according to your JetPack version in the following format\\n\\n```sh\\nwget <URL> -O <file_name>\\npip3 install <file_name>\\n```\\n\\nFor example, here we are running **JP4.6.1**, and therefore we choose **PyTorch v1.10.0**\\n\\n```sh\\ncd ~\\nsudo apt-get install -y libopenblas-base libopenmpi-dev\\nwget https://nvidia.box.com/shared/static/fjtbno0vpo676a25cgvuqc1wty0fkkg6.whl -O torch-1.10.0-cp36-cp36m-linux_aarch64.whl\\npip3 install torch-1.10.0-cp36-cp36m-linux_aarch64.whl\\n```\\n\\n- **Step 2.** Install torchvision depending on the version of PyTorch that you have installed. For example, we chose **PyTorch v1.10.0**, which means, we need to choose **Torchvision v0.11.1**\\n\\n```sh\\nsudo apt install -y libjpeg-dev zlib1g-dev\\ngit clone --branch v0.11.1 https://github.com/pytorch/vision torchvision\\ncd torchvision\\nsudo python3 setup.py install \\n```\\n\\nHere a list of the corresponding torchvision version that you need to install according to the PyTorch version:\\n\\n- PyTorch v1.10 - torchvision v0.11.1\\n- PyTorch v1.12 - torchvision v0.13.0\\n\\n## DeepStream Configuration for YOLOv5\\n\\n- **Step 1.** Clone the following repo\\n\\n```sh\\ncd ~\\ngit clone https://github.com/marcoslucianops/DeepStream-Yolo\\n```\\n\\n- **Step 2.** Copy **gen_wts_yoloV5.py** from **DeepStream-Yolo/utils** into **yolov5** directory \\n\\n```sh\\ncp DeepStream-Yolo/utils/gen_wts_yoloV5.py yolov5\\n```\\n\\n- **Step 3.** Inside the yolov5 repo, download **pt file** from YOLOv5 releases (example for YOLOv5s 6.1)\\n\\n```sh\\ncd yolov5\\nwget https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt\\n```\\n\\n- **Step 4.** Generate the **cfg** and **wts** files\\n\\n```sh\\npython3 gen_wts_yoloV5.py -w yolov5s.pt\\n```\\n\\n**Note**: To change the inference size (default: 640)\\n\\n```sh\\n-s SIZE\\n--size SIZE\\n-s HEIGHT WIDTH\\n--size HEIGHT WIDTH\\n\\nExample for 1280:\\n\\n-s 1280\\nor\\n-s 1280 1280\\n```\\n\\n- **Step 5.** Copy the generated **cfg** and **wts** files into the **DeepStream-Yolo** folder\\n\\n```sh\\ncp yolov5s.cfg ~/DeepStream-Yolo\\ncp yolov5s.wts ~/DeepStream-Yolo\\n```\\n\\n- **Step 6.** Open the **DeepStream-Yolo** folder and compile the library\\n\\n```sh\\ncd ~/DeepStream-Yolo\\nCUDA_VER=11.4 make -C nvdsinfer_custom_impl_Yolo  # for DeepStream 6.1\\nCUDA_VER=10.2 make -C nvdsinfer_custom_impl_Yolo  # for DeepStream 6.0.1 / 6.0\\n```\\n\\n- **Step 7.** Edit the **config_infer_primary_yoloV5.txt** file according to your model\\n\\n```sh\\n[property]\\n...\\ncustom-network-config=yolov5s.cfg\\nmodel-file=yolov5s.wts\\n...\\n```\\n\\n- **Step 8.** Edit the **deepstream_app_config** file\\n\\n```sh\\n...\\n[primary-gie]\\n...\\nconfig-file=config_infer_primary_yoloV5.txt\\n```\\n\\n- **Step 9.** Change the video source in **deepstream_app_config** file. Here a default video file is loaded as you can see below\\n\\n```sh\\n...\\n[source0]\\n...\\nuri=file:///opt/nvidia/deepstream/deepstream/samples/streams/sample_1080p_h264.mp4\\n```\\n\\n## Run the Inference\\n\\n```sh\\ndeepstream-app -c deepstream_app_config.txt\\n```\\n\\n<div align=center><img width=1000 src=\"https://files.seeedstudio.com/wiki/YOLOV5/FP32-yolov5s.gif\"/></div>\\n\\nThe above result is running on **Jetson Xavier NX** with **FP32** and **YOLOv5s 640x640**. We can see that the **FPS** is around **30**.\\n\\n## INT8 Calibration\\n\\nIf you want to use INT8 precision for inference, you need to follow the steps below \\n\\n- **Step 1.** Install OpenCV\\n\\n```sh\\nsudo apt-get install libopencv-dev\\n```\\n\\n- **Step 2.** Compile/recompile the **nvdsinfer_custom_impl_Yolo** library with OpenCV support\\n\\n```sh\\ncd ~/DeepStream-Yolo\\nCUDA_VER=11.4 OPENCV=1 make -C nvdsinfer_custom_impl_Yolo  # for DeepStream 6.1\\nCUDA_VER=10.2 OPENCV=1 make -C nvdsinfer_custom_impl_Yolo  # for DeepStream 6.0.1 / 6.0\\n```\\n\\n- **Step 3.** For COCO dataset, download the [val2017](https://drive.google.com/file/d/1gbvfn7mcsGDRZ_luJwtITL-ru2kK99aK/view?usp=sharing), extract, and move to **DeepStream-Yolo** folder\\n\\n- **Step 4.** Make a new directory for calibration images\\n\\n```sh\\nmkdir calibration\\n```\\n\\n- **Step 5.** Run the following to select 1000 random images from COCO dataset to run calibration\\n\\n```sh\\nfor jpg in $(ls -1 val2017/*.jpg | sort -R | head -1000); do \\\\\\n    cp ${jpg} calibration/; \\\\\\ndone\\n```\\n\\n**Note:** NVIDIA recommends at least 500 images to get a good accuracy. On this example, 1000 images are chosen to get better accuracy (more images = more accuracy). Higher INT8_CALIB_BATCH_SIZE values will result in more accuracy and faster calibration speed. Set it according to you GPU memory. You can set it from **head -1000**. For example, for 2000 images, **head -2000**. This process can take a long time. \\n\\n- **Step 6.** Create the **calibration.txt** file with all selected images\\n\\n```sh\\nrealpath calibration/*jpg > calibration.txt\\n```\\n\\n- **Step 7.** Set environment variables\\n\\n```sh\\nexport INT8_CALIB_IMG_PATH=calibration.txt\\nexport INT8_CALIB_BATCH_SIZE=1\\n```\\n\\n- **Step 8.** Update the **config_infer_primary_yoloV5.txt** file\\n\\nFrom\\n\\n```sh\\n...\\nmodel-engine-file=model_b1_gpu0_fp32.engine\\n#int8-calib-file=calib.table\\n...\\nnetwork-mode=0\\n...\\n```\\n\\nTo\\n\\n```sh\\n...\\nmodel-engine-file=model_b1_gpu0_int8.engine\\nint8-calib-file=calib.table\\n...\\nnetwork-mode=1\\n...\\n```\\n\\n- **Step 9.** Run the inference\\n\\n```sh\\ndeepstream-app -c deepstream_app_config.txt\\n```\\n\\n<div align=center><img width=1000  src=\"https://files.seeedstudio.com/wiki/YOLOV5/INT8-yolov5s.gif\"/></div>\\n\\nThe above result is running on **Jetson Xavier NX** with **INT8** and **YOLOv5s 640x640**. We can see that the **FPS** is around **60**.\\n\\n## Benchmark results\\n\\nThe following table summarizes how different models perform on **Jetson Xavier NX**. \\n\\n| Model Name | Precision | Inference Size | Inference Time (ms) | FPS |\\n|------------|-----------|----------------|---------------------|-----|\\n| YOLOv5s    | FP32      | 320x320        | 16.66               | 60  |                    \\n|            | FP32      | 640x640        | 33.33               | 30  |                    \\n|            | INT8      | 640x640        | 16.66               | 60  |                    \\n| YOLOv5n    | FP32      | 640x640        | 16.66               | 60  |                    \\n\\n\\n### Additional\\n\\nThis tutorial is written by our friends at seeed @lakshanthad and Elaine\\n', metadata={'file_path': 'docs/yolov5/tutorials/running_on_jetson_nano.md', 'file_name': 'running_on_jetson_nano.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n# Test-Time Augmentation (TTA)\\n\\n📚 This guide explains how to use Test Time Augmentation (TTA) during testing and inference for improved mAP and Recall with YOLOv5 🚀.  \\nUPDATED 25 September 2022.\\n\\n## Before You Start\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n## Test Normally\\n\\nBefore trying TTA we want to establish a baseline performance to compare to. This command tests YOLOv5x on COCO val2017 at image size 640 pixels. `yolov5x.pt` is the largest and most accurate model available. Other options are `yolov5s.pt`, `yolov5m.pt` and `yolov5l.pt`, or you own checkpoint from training a custom dataset `./weights/best.pt`. For details on all available models please see our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints).\\n```bash\\npython val.py --weights yolov5x.pt --data coco.yaml --img 640 --half\\n```\\n\\nOutput:\\n```shell\\nval: data=./data/coco.yaml, weights=[\\'yolov5x.pt\\'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\\nYOLOv5 🚀 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\\n\\nFusing layers... \\nModel Summary: 476 layers, 87730285 parameters, 0 gradients\\n\\nval: Scanning \\'../datasets/coco/val2017\\' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 2846.03it/s]\\nval: New cache created: ../datasets/coco/val2017.cache\\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [02:30<00:00,  1.05it/s]\\n                 all       5000      36335      0.746      0.626       0.68       0.49\\nSpeed: 0.1ms pre-process, 22.4ms inference, 1.4ms NMS per image at shape (32, 3, 640, 640)  # <--- baseline speed\\n\\nEvaluating pycocotools mAP... saving runs/val/exp/yolov5x_predictions.json...\\n...\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504  # <--- baseline mAP\\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.688\\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.546\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.351\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.551\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.644\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.382\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.628\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.681  # <--- baseline mAR\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.524\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.735\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.826\\n```\\n\\n## Test with TTA\\nAppend `--augment` to any existing `val.py` command to enable TTA, and increase the image size by about 30% for improved results. Note that inference with TTA enabled will typically take about 2-3X the time of normal inference as the images are being left-right flipped and processed at 3 different resolutions, with the outputs merged before NMS. Part of the speed decrease is simply due to larger image sizes (832 vs 640), while part is due to the actual TTA operations.\\n```bash\\npython val.py --weights yolov5x.pt --data coco.yaml --img 832 --augment --half\\n```\\n\\nOutput:\\n```shell\\nval: data=./data/coco.yaml, weights=[\\'yolov5x.pt\\'], batch_size=32, imgsz=832, conf_thres=0.001, iou_thres=0.6, task=val, device=, single_cls=False, augment=True, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True\\nYOLOv5 🚀 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\\n\\nFusing layers... \\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\\n  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\\nModel Summary: 476 layers, 87730285 parameters, 0 gradients\\nval: Scanning \\'../datasets/coco/val2017\\' images and labels...4952 found, 48 missing, 0 empty, 0 corrupted: 100% 5000/5000 [00:01<00:00, 2885.61it/s]\\nval: New cache created: ../datasets/coco/val2017.cache\\n               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 157/157 [07:29<00:00,  2.86s/it]\\n                 all       5000      36335      0.718      0.656      0.695      0.503\\nSpeed: 0.2ms pre-process, 80.6ms inference, 2.7ms NMS per image at shape (32, 3, 832, 832)  # <--- TTA speed\\n\\nEvaluating pycocotools mAP... saving runs/val/exp2/yolov5x_predictions.json...\\n...\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.516  # <--- TTA mAP\\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.701\\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.562\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.361\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.564\\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.656\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.388\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.640\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.696  # <--- TTA mAR\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.553\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.744\\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.833\\n```\\n\\n## Inference with TTA\\n\\n`detect.py` TTA inference operates identically to `val.py` TTA: simply append `--augment` to any existing `detect.py` command:\\n```bash\\npython detect.py --weights yolov5s.pt --img 832 --source data/images --augment\\n```\\n\\nOutput:\\n```bash\\ndetect: weights=[\\'yolov5s.pt\\'], source=data/images, imgsz=832, conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=True, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False\\nYOLOv5 🚀 v5.0-267-g6a3ee7c torch 1.9.0+cu102 CUDA:0 (Tesla P100-PCIE-16GB, 16280.875MB)\\n\\nDownloading https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt to yolov5s.pt...\\n100% 14.1M/14.1M [00:00<00:00, 81.9MB/s]\\n\\nFusing layers... \\nModel Summary: 224 layers, 7266973 parameters, 0 gradients\\nimage 1/2 /content/yolov5/data/images/bus.jpg: 832x640 4 persons, 1 bus, 1 fire hydrant, Done. (0.029s)\\nimage 2/2 /content/yolov5/data/images/zidane.jpg: 480x832 3 persons, 3 ties, Done. (0.024s)\\nResults saved to runs/detect/exp\\nDone. (0.156s)\\n```\\n\\n<img src=\"https://user-images.githubusercontent.com/26833433/124491703-dbb6b200-ddb3-11eb-8b57-ed0d58d0d8b4.jpg\" width=\"500\">\\n\\n\\n### PyTorch Hub TTA\\n\\nTTA is automatically integrated into all [YOLOv5 PyTorch Hub](https://pytorch.org/hub/ultralytics_yolov5) models, and can be accessed by passing `augment=True` at inference time.\\n```python\\nimport torch\\n\\n# Model\\nmodel = torch.hub.load(\\'ultralytics/yolov5\\', \\'yolov5s\\')  # or yolov5m, yolov5x, custom\\n\\n# Images\\nimg = \\'https://ultralytics.com/images/zidane.jpg\\'  # or file, PIL, OpenCV, numpy, multiple\\n\\n# Inference\\nresults = model(img, augment=True)  # <--- TTA inference\\n\\n# Results\\nresults.print()  # or .show(), .save(), .crop(), .pandas(), etc.\\n```\\n\\n### Customize \\n\\nYou can customize the TTA ops applied in the YOLOv5 `forward_augment()` method [here](https://github.com/ultralytics/yolov5/blob/8c6f9e15bfc0000d18b976a95b9d7c17d407ec91/models/yolo.py#L125-L137).\\n\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.', metadata={'file_path': 'docs/yolov5/tutorials/test_time_augmentation.md', 'file_name': 'test_time_augmentation.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n📚 This guide explains how to produce the best mAP and training results with YOLOv5 🚀.  \\nUPDATED 25 May 2022.\\n\\nMost of the time good results can be obtained with no changes to the models or training settings, **provided your dataset is sufficiently large and well labelled**. If at first you don\\'t get good results, there are steps you might be able to take to improve, but we always recommend users **first train with all default settings** before considering any changes. This helps establish a performance baseline and spot areas for improvement.\\n\\nIf you have questions about your training results **we recommend you provide the maximum amount of information possible** if you expect a helpful response, including results plots (train losses, val losses, P, R, mAP), PR curve, confusion matrix, training mosaics, test results and dataset statistics images such as labels.png. All of these are located in your `project/name` directory, typically `yolov5/runs/train/exp`.\\n\\nWe\\'ve put together a full guide for users looking to get the best results on their YOLOv5 trainings below.\\n\\n## Dataset\\n\\n- **Images per class.** ≥ 1500 images per class recommended\\n- **Instances per class.** ≥ 10000 instances (labeled objects) per class recommended\\n- **Image variety.** Must be representative of deployed environment. For real-world use cases we recommend images from different times of day, different seasons, different weather, different lighting, different angles, different sources (scraped online, collected locally, different cameras) etc.\\n- **Label consistency.** All instances of all classes in all images must be labelled. Partial labelling will not work.\\n- **Label accuracy.** Labels must closely enclose each object. No space should exist between an object and it\\'s bounding box. No objects should be missing a label.\\n- **Label verification.** View `train_batch*.jpg` on train start to verify your labels appear correct, i.e. see [example](https://docs.ultralytics.com/yolov5/tutorials/train_custom_data#local-logging) mosaic.\\n- **Background images.** Background images are images with no objects that are added to a dataset to reduce False Positives (FP). We recommend about 0-10% background images to help reduce FPs (COCO has 1000 background images for reference, 1% of the total). No labels are required for background images.\\n\\n<a href=\"https://arxiv.org/abs/1405.0312\"><img width=\"800\" src=\"https://user-images.githubusercontent.com/26833433/109398377-82b0ac00-78f1-11eb-9c76-cc7820669d0d.png\" alt=\"COCO Analysis\"></a>\\n\\n\\n## Model Selection\\n\\nLarger models like YOLOv5x and [YOLOv5x6](https://github.com/ultralytics/yolov5/releases/tag/v5.0) will produce better results in nearly all cases, but have more parameters, require more CUDA memory to train, and are slower to run. For **mobile** deployments we recommend YOLOv5s/m, for **cloud** deployments we recommend YOLOv5l/x. See our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) for a full comparison of all models. \\n\\n<p align=\"center\"><img width=\"700\" alt=\"YOLOv5 Models\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png\"></p>\\n\\n- **Start from Pretrained weights.** Recommended for small to medium-sized datasets (i.e. [VOC](https://github.com/ultralytics/yolov5/blob/master/data/VOC.yaml), [VisDrone](https://github.com/ultralytics/yolov5/blob/master/data/VisDrone.yaml), [GlobalWheat](https://github.com/ultralytics/yolov5/blob/master/data/GlobalWheat2020.yaml)). Pass the name of the model to the `--weights` argument. Models download automatically from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases).\\n```shell\\npython train.py --data custom.yaml --weights yolov5s.pt\\n                                             yolov5m.pt\\n                                             yolov5l.pt\\n                                             yolov5x.pt\\n                                             custom_pretrained.pt\\n```\\n- **Start from Scratch.** Recommended for large datasets (i.e. [COCO](https://github.com/ultralytics/yolov5/blob/master/data/coco.yaml), [Objects365](https://github.com/ultralytics/yolov5/blob/master/data/Objects365.yaml), [OIv6](https://storage.googleapis.com/openimages/web/index.html)). Pass the model architecture yaml you are interested in, along with an empty `--weights \\'\\'` argument:\\n```bash\\npython train.py --data custom.yaml --weights \\'\\' --cfg yolov5s.yaml\\n                                                      yolov5m.yaml\\n                                                      yolov5l.yaml\\n                                                      yolov5x.yaml\\n```\\n\\n\\n## Training Settings\\n\\nBefore modifying anything, **first train with default settings to establish a performance baseline**. A full list of train.py settings can be found in the [train.py](https://github.com/ultralytics/yolov5/blob/master/train.py) argparser.\\n\\n- **Epochs.** Start with 300 epochs. If this overfits early then you can reduce epochs. If overfitting does not occur after 300 epochs, train longer, i.e. 600, 1200 etc epochs.\\n- **Image size.** COCO trains at native resolution of `--img 640`, though due to the high amount of small objects in the dataset it can benefit from training at higher resolutions such as `--img 1280`. If there are many small objects then custom datasets will benefit from training at native or higher resolution. Best inference results are obtained at the same `--img` as the training was run at, i.e. if you train at `--img 1280` you should also test and detect at `--img 1280`.\\n- **Batch size.** Use the largest `--batch-size` that your hardware allows for. Small batch sizes produce poor batchnorm statistics and should be avoided.\\n- **Hyperparameters.** Default hyperparameters are in [hyp.scratch-low.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch-low.yaml). We recommend you train with default hyperparameters first before thinking of modifying any. In general, increasing augmentation hyperparameters will reduce and delay overfitting, allowing for longer trainings and higher final mAP. Reduction in loss component gain hyperparameters like `hyp[\\'obj\\']` will help reduce overfitting in those specific loss components. For an automated method of optimizing these hyperparameters, see our [Hyperparameter Evolution Tutorial](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution).\\n\\n## Further Reading\\n\\nIf you\\'d like to know more, a good place to start is Karpathy\\'s \\'Recipe for Training Neural Networks\\', which has great ideas for training that apply broadly across all ML domains: [http://karpathy.github.io/2019/04/25/recipe/](http://karpathy.github.io/2019/04/25/recipe/)\\n\\nGood luck 🍀 and let us know if you have any other questions!', metadata={'file_path': 'docs/yolov5/tutorials/tips_for_best_training_results.md', 'file_name': 'tips_for_best_training_results.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n📚 This guide explains how to train your own **custom dataset** with [YOLOv5](https://github.com/ultralytics/yolov5) 🚀.  \\nUPDATED 26 March 2023.\\n\\n## Before You Start\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n## Train On Custom Data\\n\\n<a href=\"https://bit.ly/ultralytics_hub\" target=\"_blank\">\\n<img width=\"100%\" src=\"https://github.com/ultralytics/assets/raw/main/im/integrations-loop.png\"></a>\\n<br>\\n<br>\\n\\nCreating a custom model to detect your objects is an iterative process of collecting and organizing images, labeling your objects of interest, training a model, deploying it into the wild to make predictions, and then using that deployed model to collect examples of edge cases to repeat and improve.\\n\\n### 1. Create Dataset\\n\\nYOLOv5 models must be trained on labelled data in order to learn classes of objects in that data. There are two options for creating your dataset before you start training:\\n\\n<details markdown>\\n<summary>Use <a href=\"https://roboflow.com/?ref=ultralytics\">Roboflow</a> to create your dataset in YOLO format</summary>\\n\\n### 1.1 Collect Images\\n\\nYour model will learn by example. Training on images similar to the ones it will see in the wild is of the utmost importance. Ideally, you will collect a wide variety of images from the same configuration (camera, angle, lighting, etc.) as you will ultimately deploy your project.\\n\\nIf this is not possible, you can start from [a public dataset](https://universe.roboflow.com/?ref=ultralytics) to train your initial model and then [sample images from the wild during inference](https://blog.roboflow.com/computer-vision-active-learning-tips/?ref=ultralytics) to improve your dataset and model iteratively.\\n\\n### 1.2 Create Labels\\n\\nOnce you have collected images, you will need to annotate the objects of interest to create a ground truth for your model to learn from.\\n\\n<p align=\"center\"><a href=\"https://app.roboflow.com/?model=yolov5&ref=ultralytics\" title=\"Create a Free Roboflow Account\"><img width=\"450\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a275ad4b4ac20cd2e21a_roboflow-annotate.gif\" /></a></p>\\n\\n[Roboflow Annotate](https://roboflow.com/annotate?ref=ultralytics) is a simple\\nweb-based tool for managing and labeling your images with your team and exporting\\nthem in [YOLOv5\\'s annotation format](https://roboflow.com/formats/yolov5-pytorch-txt?ref=ultralytics).\\n\\n### 1.3 Prepare Dataset for YOLOv5\\n\\nWhether you [label your images with Roboflow](https://roboflow.com/annotate?ref=ultralytics) or not, you can use it to convert your dataset into YOLO format, create a YOLOv5 YAML configuration file, and host it for importing into your training script.\\n\\n[Create a free Roboflow account](https://app.roboflow.com/?model=yolov5&ref=ultralytics)\\nand upload your dataset to a `Public` workspace, label any unannotated images,\\nthen generate and export a version of your dataset in `YOLOv5 Pytorch` format.\\n\\nNote: YOLOv5 does online augmentation during training, so we do not recommend\\napplying any augmentation steps in Roboflow for training with YOLOv5. But we\\nrecommend applying the following preprocessing steps:\\n\\n<p align=\"center\"><img width=\"450\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a273477fccf42a0fd3d6_roboflow-preprocessing.png\" title=\"Recommended Preprocessing Steps\" /></p>\\n\\n* **Auto-Orient** - to strip EXIF orientation from your images.\\n* **Resize (Stretch)** - to the square input size of your model (640x640 is the YOLOv5 default).\\n\\nGenerating a version will give you a point in time snapshot of your dataset so\\nyou can always go back and compare your future model training runs against it,\\neven if you add more images or change its configuration later.\\n\\n<p align=\"center\"><img width=\"450\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a2733fd1da943619934e_roboflow-export.png\" title=\"Export in YOLOv5 Format\" /></p>\\n\\nExport in `YOLOv5 Pytorch` format, then copy the snippet into your training\\nscript or notebook to download your dataset.\\n\\n<p align=\"center\"><img width=\"450\" src=\"https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/6152a273a92e4f5cb72594df_roboflow-snippet.png\" title=\"Roboflow dataset download snippet\" /></p>\\n\\nNow continue with `2. Select a Model`.\\n</details>\\n\\n<details open markdown>\\n<summary>Or manually prepare your dataset</summary>\\n\\n### 1.1 Create dataset.yaml\\n\\n[COCO128](https://www.kaggle.com/ultralytics/coco128) is an example small tutorial dataset composed of the first 128 images in [COCO](http://cocodataset.org/#home) train2017. These same 128 images are used for both training and validation to verify our training pipeline is capable of overfitting. [data/coco128.yaml](https://github.com/ultralytics/yolov5/blob/master/data/coco128.yaml), shown below, is the dataset config file that defines 1) the dataset root directory `path` and relative paths to `train` / `val` / `test` image directories (or *.txt files with image paths) and 2) a class `names` dictionary:\\n```yaml\\n# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]\\npath: ../datasets/coco128  # dataset root dir\\ntrain: images/train2017  # train images (relative to \\'path\\') 128 images\\nval: images/train2017  # val images (relative to \\'path\\') 128 images\\ntest:  # test images (optional)\\n\\n# Classes (80 COCO classes)\\nnames:\\n  0: person\\n  1: bicycle\\n  2: car\\n  ...\\n  77: teddy bear\\n  78: hair drier\\n  79: toothbrush\\n```\\n\\n\\n### 1.2 Create Labels\\n\\nAfter using an annotation tool to label your images, export your labels to **YOLO format**, with one `*.txt` file per image (if no objects in image, no `*.txt` file is required). The `*.txt` file specifications are:\\n\\n- One row per object\\n- Each row is `class x_center y_center width height` format.\\n- Box coordinates must be in **normalized xywh** format (from 0 - 1). If your boxes are in pixels, divide `x_center` and `width` by image width, and `y_center` and `height` by image height.\\n- Class numbers are zero-indexed (start from 0).\\n\\n<p align=\"center\"><img width=\"750\" src=\"https://user-images.githubusercontent.com/26833433/91506361-c7965000-e886-11ea-8291-c72b98c25eec.jpg\"></p>\\n\\nThe label file corresponding to the above image contains 2 persons (class `0`) and a tie (class `27`):\\n\\n<p align=\"center\"><img width=\"428\" src=\"https://user-images.githubusercontent.com/26833433/112467037-d2568c00-8d66-11eb-8796-55402ac0d62f.png\"></p>\\n\\n\\n### 1.3 Organize Directories\\n\\nOrganize your train and val images and labels according to the example below. YOLOv5 assumes  `/coco128` is inside a `/datasets` directory **next to** the `/yolov5` directory. **YOLOv5 locates labels automatically for each image** by replacing the last instance of `/images/` in each image path with `/labels/`. For example:\\n```bash\\n../datasets/coco128/images/im0.jpg  # image\\n../datasets/coco128/labels/im0.txt  # label\\n```\\n\\n<p align=\"center\"><img width=\"700\" src=\"https://user-images.githubusercontent.com/26833433/134436012-65111ad1-9541-4853-81a6-f19a3468b75f.png\"></p>\\n</details>\\n\\n\\n### 2. Select a Model\\n\\nSelect a pretrained model to start training from. Here we select [YOLOv5s](https://github.com/ultralytics/yolov5/blob/master/models/yolov5s.yaml), the second-smallest and fastest model available. See our README [table](https://github.com/ultralytics/yolov5#pretrained-checkpoints) for a full comparison of all models.\\n\\n<p align=\"center\"><img width=\"800\" alt=\"YOLOv5 Models\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/model_comparison.png\"></p>\\n\\n### 3. Train\\n\\nTrain a YOLOv5s model on COCO128 by specifying dataset, batch-size, image size and either pretrained `--weights yolov5s.pt` (recommended), or randomly initialized `--weights \\'\\' --cfg yolov5s.yaml` (not recommended). Pretrained weights are auto-downloaded from the [latest YOLOv5 release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\npython train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt\\n```\\n!!! tip \"Tip\"\\n\\n    💡 Add `--cache ram` or `--cache disk` to speed up training (requires significant RAM/disk resources).  \\n\\n!!! tip \"Tip\"\\n\\n    💡 Always train from a local dataset. Mounted or network drives like Google Drive will be very slow. \\n\\nAll training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc. For more details see the Training section of our tutorial notebook. <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n\\n### 4. Visualize\\n\\n#### Comet Logging and Visualization 🌟 NEW\\n\\n[Comet](https://bit.ly/yolov5-readme-comet) is now fully integrated with YOLOv5. Track and visualize model metrics in real time, save your hyperparameters, datasets, and model checkpoints, and visualize your model predictions with [Comet Custom Panels](https://bit.ly/yolov5-colab-comet-panels)! Comet makes sure you never lose track of your work and makes it easy to share results and collaborate across teams of all sizes! \\n\\nGetting started is easy:\\n```shell\\npip install comet_ml  # 1. install\\nexport COMET_API_KEY=<Your API Key>  # 2. paste API key\\npython train.py --img 640 --epochs 3 --data coco128.yaml --weights yolov5s.pt  # 3. train\\n```\\n\\nTo learn more about all the supported Comet features for this integration, check out the [Comet Tutorial](https://docs.ultralytics.com/yolov5/tutorials/comet_logging_integration). If you\\'d like to learn more about Comet, head over to our [documentation](https://bit.ly/yolov5-colab-comet-docs). Get started by trying out the Comet Colab Notebook:\\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1RG0WOQyxlDlo5Km8GogJpIEJlg_5lyYO?usp=sharing)\\n\\n<img width=\"1920\" alt=\"yolo-ui\" src=\"https://user-images.githubusercontent.com/26833433/202851203-164e94e1-2238-46dd-91f8-de020e9d6b41.png\">\\n\\n#### ClearML Logging and Automation 🌟 NEW\\n\\n[ClearML](https://cutt.ly/yolov5-notebook-clearml) is completely integrated into YOLOv5 to track your experimentation, manage dataset versions and even remotely execute training runs. To enable ClearML:\\n\\n- `pip install clearml`\\n- run `clearml-init` to connect to a ClearML server (**deploy your own open-source server [here](https://github.com/allegroai/clearml-server)**, or use our free hosted server [here](https://cutt.ly/yolov5-notebook-clearml))\\n\\nYou\\'ll get all the great expected features from an experiment manager: live updates, model upload, experiment comparison etc. but ClearML also tracks uncommitted changes and installed packages for example. Thanks to that ClearML Tasks (which is what we call experiments) are also reproducible on different machines! With only 1 extra line, we can schedule a YOLOv5 training task on a queue to be executed by any number of ClearML Agents (workers).\\n\\nYou can use ClearML Data to version your dataset and then pass it to YOLOv5 simply using its unique ID. This will help you keep track of your data without adding extra hassle. Explore the [ClearML Tutorial](https://docs.ultralytics.com/yolov5/tutorials/clearml_logging_integration) for details!\\n\\n<a href=\"https://cutt.ly/yolov5-notebook-clearml\">\\n<img alt=\"ClearML Experiment Management UI\" src=\"https://github.com/thepycoder/clearml_screenshots/raw/main/scalars.jpg\" width=\"1280\"/></a>\\n\\n\\n#### Local Logging\\n\\nTraining results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\\n\\nThis directory contains train and val statistics, mosaics, labels, predictions and augmented mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices. \\n\\n<img alt=\"Local logging results\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/image-local_logging.jpg\" width=\"1280\"/>\\n\\nResults file `results.csv` is updated after each epoch, and then plotted as `results.png` (below) after training completes. You can also plot any `results.csv` file manually:\\n\\n```python\\nfrom utils.plots import plot_results\\nplot_results(\\'path/to/results.csv\\')  # plot \\'results.csv\\' as \\'results.png\\'\\n```\\n\\n<p align=\"center\"><img width=\"800\" alt=\"results.png\" src=\"https://github.com/ultralytics/yolov5/releases/download/v1.0/results.png\"></p>\\n\\n\\n\\n## Next Steps\\n\\nOnce your model is trained you can use your best checkpoint `best.pt` to:\\n* Run [CLI](https://github.com/ultralytics/yolov5#quick-start-examples) or [Python](https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading) inference on new images and videos\\n* [Validate](https://github.com/ultralytics/yolov5/blob/master/val.py) accuracy on train, val and test splits\\n* [Export](https://docs.ultralytics.com/yolov5/tutorials/model_export) to TensorFlow, Keras, ONNX, TFlite, TF.js, CoreML and TensorRT formats\\n* [Evolve](https://docs.ultralytics.com/yolov5/tutorials/hyperparameter_evolution) hyperparameters to improve performance\\n* [Improve](https://docs.roboflow.com/adding-data/upload-api?ref=ultralytics) your model by sampling real-world images and adding them to your dataset\\n\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.\\n', metadata={'file_path': 'docs/yolov5/tutorials/train_custom_data.md', 'file_name': 'train_custom_data.md', 'file_type': '.md'}), Document(page_content='---\\ncomments: true\\n---\\n\\n📚 This guide explains how to **freeze** YOLOv5 🚀 layers when **transfer learning**. Transfer learning is a useful way to quickly retrain a model on new data without having to retrain the entire network. Instead, part of the initial weights are frozen in place, and the rest of the weights are used to compute loss and are updated by the optimizer. This requires less resources than normal training and allows for faster training times, though it may also result in reductions to final trained accuracy.  \\nUPDATED 25 September 2022.\\n\\n\\n## Before You Start\\n\\nClone repo and install [requirements.txt](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) in a [**Python>=3.7.0**](https://www.python.org/) environment, including [**PyTorch>=1.7**](https://pytorch.org/get-started/locally/). [Models](https://github.com/ultralytics/yolov5/tree/master/models) and [datasets](https://github.com/ultralytics/yolov5/tree/master/data) download automatically from the latest YOLOv5 [release](https://github.com/ultralytics/yolov5/releases).\\n\\n```bash\\ngit clone https://github.com/ultralytics/yolov5  # clone\\ncd yolov5\\npip install -r requirements.txt  # install\\n```\\n\\n## Freeze Backbone\\n\\nAll layers that match the train.py `freeze` list in train.py will be frozen by setting their gradients to zero before training starts.\\n```python\\n # Freeze \\n freeze = [f\\'model.{x}.\\' for x in range(freeze)]  # layers to freeze \\n for k, v in model.named_parameters(): \\n     v.requires_grad = True  # train all layers \\n     if any(x in k for x in freeze): \\n         print(f\\'freezing {k}\\') \\n         v.requires_grad = False \\n```\\n\\nTo see a list of module names:\\n```python\\nfor k, v in model.named_parameters():\\n    print(k)\\n\\n# Output\\nmodel.0.conv.conv.weight\\nmodel.0.conv.bn.weight\\nmodel.0.conv.bn.bias\\nmodel.1.conv.weight\\nmodel.1.bn.weight\\nmodel.1.bn.bias\\nmodel.2.cv1.conv.weight\\nmodel.2.cv1.bn.weight\\n...\\nmodel.23.m.0.cv2.bn.weight\\nmodel.23.m.0.cv2.bn.bias\\nmodel.24.m.0.weight\\nmodel.24.m.0.bias\\nmodel.24.m.1.weight\\nmodel.24.m.1.bias\\nmodel.24.m.2.weight\\nmodel.24.m.2.bias\\n```\\n\\nLooking at the model architecture we can see that the model backbone is layers 0-9:\\n```yaml\\n# YOLOv5 backbone \\n backbone: \\n   # [from, number, module, args] \\n   [[-1, 1, Focus, [64, 3]],  # 0-P1/2 \\n    [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4 \\n    [-1, 3, BottleneckCSP, [128]], \\n    [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8 \\n    [-1, 9, BottleneckCSP, [256]], \\n    [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16 \\n    [-1, 9, BottleneckCSP, [512]], \\n    [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32 \\n    [-1, 1, SPP, [1024, [5, 9, 13]]], \\n    [-1, 3, BottleneckCSP, [1024, False]],  # 9 \\n   ] \\n  \\n # YOLOv5 head \\n head: \\n   [[-1, 1, Conv, [512, 1, 1]], \\n    [-1, 1, nn.Upsample, [None, 2, \\'nearest\\']], \\n    [[-1, 6], 1, Concat, [1]],  # cat backbone P4 \\n    [-1, 3, BottleneckCSP, [512, False]],  # 13 \\n  \\n    [-1, 1, Conv, [256, 1, 1]], \\n    [-1, 1, nn.Upsample, [None, 2, \\'nearest\\']], \\n    [[-1, 4], 1, Concat, [1]],  # cat backbone P3 \\n    [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small) \\n  \\n    [-1, 1, Conv, [256, 3, 2]], \\n    [[-1, 14], 1, Concat, [1]],  # cat head P4 \\n    [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium) \\n  \\n    [-1, 1, Conv, [512, 3, 2]], \\n    [[-1, 10], 1, Concat, [1]],  # cat head P5 \\n    [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large) \\n  \\n    [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5) \\n   ] \\n```\\n\\nso we can define the freeze list to contain all modules with \\'model.0.\\' - \\'model.9.\\' in their names:\\n```bash\\npython train.py --freeze 10\\n```\\n\\n## Freeze All Layers\\n\\nTo freeze the full model except for the final output convolution layers in Detect(), we set freeze list to contain all modules with \\'model.0.\\' - \\'model.23.\\' in their names:\\n```bash\\npython train.py --freeze 24\\n```\\n\\n## Results\\n\\nWe train YOLOv5m on VOC on both of the above scenarios, along with a default model (no freezing), starting from the official COCO pretrained `--weights yolov5m.pt`:\\n```python\\ntrain.py --batch 48 --weights yolov5m.pt --data voc.yaml --epochs 50 --cache --img 512 --hyp hyp.finetune.yaml\\n```\\n\\n### Accuracy Comparison\\n\\nThe results show that freezing speeds up training, but reduces final accuracy slightly.\\n\\n![](https://user-images.githubusercontent.com/26833433/98394454-11579f80-205b-11eb-8e57-d8318e1cc2f8.png)\\n\\n![](https://user-images.githubusercontent.com/26833433/98394459-13216300-205b-11eb-871b-49e20691a423.png)\\n\\n<img width=\"922\" alt=\"Screenshot 2020-11-06 at 18 08 13\" src=\"https://user-images.githubusercontent.com/26833433/98394485-22081580-205b-11eb-9e37-1f9869fe91d8.png\">\\n\\n### GPU Utilization Comparison\\n\\nInterestingly, the more modules are frozen the less GPU memory is required to train, and the lower GPU utilization. This indicates that larger models, or models trained at larger --image-size may benefit from freezing in order to train faster.\\n\\n![](https://user-images.githubusercontent.com/26833433/98394920-c2f6d080-205b-11eb-9611-fd68522b4e0e.png)\\n\\n![](https://user-images.githubusercontent.com/26833433/98394918-bf634980-205b-11eb-948d-311036ef9325.png)\\n\\n\\n## Environments\\n\\nYOLOv5 may be run in any of the following up-to-date verified environments (with all dependencies including [CUDA](https://developer.nvidia.com/cuda)/[CUDNN](https://developer.nvidia.com/cudnn), [Python](https://www.python.org/) and [PyTorch](https://pytorch.org/) preinstalled):\\n\\n- **Notebooks** with free GPU: <a href=\"https://bit.ly/yolov5-paperspace-notebook\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"></a> <a href=\"https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a> <a href=\"https://www.kaggle.com/ultralytics/yolov5\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\\n- **Google Cloud** Deep Learning VM. See [GCP Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/google_cloud_quickstart_tutorial/)\\n- **Amazon** Deep Learning AMI. See [AWS Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/aws_quickstart_tutorial/)\\n- **Docker Image**. See [Docker Quickstart Guide](https://docs.ultralytics.com/yolov5/environments/docker_image_quickstart_tutorial/) <a href=\"https://hub.docker.com/r/ultralytics/yolov5\"><img src=\"https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker\" alt=\"Docker Pulls\"></a>\\n\\n\\n## Status\\n\\n<a href=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml\"><img src=\"https://github.com/ultralytics/yolov5/actions/workflows/ci-testing.yml/badge.svg\" alt=\"YOLOv5 CI\"></a>\\n\\nIf this badge is green, all [YOLOv5 GitHub Actions](https://github.com/ultralytics/yolov5/actions) Continuous Integration (CI) tests are currently passing. CI tests verify correct operation of YOLOv5 [training](https://github.com/ultralytics/yolov5/blob/master/train.py), [validation](https://github.com/ultralytics/yolov5/blob/master/val.py), [inference](https://github.com/ultralytics/yolov5/blob/master/detect.py), [export](https://github.com/ultralytics/yolov5/blob/master/export.py) and [benchmarks](https://github.com/ultralytics/yolov5/blob/master/benchmarks.py) on macOS, Windows, and Ubuntu every 24 hours and on every commit.', metadata={'file_path': 'docs/yolov5/tutorials/transfer_learning_with_frozen_layers.md', 'file_name': 'transfer_learning_with_frozen_layers.md', 'file_type': '.md'}), Document(page_content='# TrackState\\n---\\n:::ultralytics.tracker.trackers.basetrack.TrackState\\n<br><br>\\n\\n# BaseTrack\\n---\\n:::ultralytics.tracker.trackers.basetrack.BaseTrack\\n<br><br>\\n', metadata={'file_path': 'docs/reference/tracker/trackers/basetrack.md', 'file_name': 'basetrack.md', 'file_type': '.md'}), Document(page_content='# BOTrack\\n---\\n:::ultralytics.tracker.trackers.bot_sort.BOTrack\\n<br><br>\\n\\n# BOTSORT\\n---\\n:::ultralytics.tracker.trackers.bot_sort.BOTSORT\\n<br><br>\\n', metadata={'file_path': 'docs/reference/tracker/trackers/bot_sort.md', 'file_name': 'bot_sort.md', 'file_type': '.md'}), Document(page_content='# STrack\\n---\\n:::ultralytics.tracker.trackers.byte_tracker.STrack\\n<br><br>\\n\\n# BYTETracker\\n---\\n:::ultralytics.tracker.trackers.byte_tracker.BYTETracker\\n<br><br>\\n', metadata={'file_path': 'docs/reference/tracker/trackers/byte_tracker.md', 'file_name': 'byte_tracker.md', 'file_type': '.md'}), Document(page_content='# GMC\\n---\\n:::ultralytics.tracker.utils.gmc.GMC\\n<br><br>\\n', metadata={'file_path': 'docs/reference/tracker/utils/gmc.md', 'file_name': 'gmc.md', 'file_type': '.md'}), Document(page_content='# KalmanFilterXYAH\\n---\\n:::ultralytics.tracker.utils.kalman_filter.KalmanFilterXYAH\\n<br><br>\\n\\n# KalmanFilterXYWH\\n---\\n:::ultralytics.tracker.utils.kalman_filter.KalmanFilterXYWH\\n<br><br>\\n', metadata={'file_path': 'docs/reference/tracker/utils/kalman_filter.md', 'file_name': 'kalman_filter.md', 'file_type': '.md'}), Document(page_content='# merge_matches\\n---\\n:::ultralytics.tracker.utils.matching.merge_matches\\n<br><br>\\n\\n# _indices_to_matches\\n---\\n:::ultralytics.tracker.utils.matching._indices_to_matches\\n<br><br>\\n\\n# linear_assignment\\n---\\n:::ultralytics.tracker.utils.matching.linear_assignment\\n<br><br>\\n\\n# ious\\n---\\n:::ultralytics.tracker.utils.matching.ious\\n<br><br>\\n\\n# iou_distance\\n---\\n:::ultralytics.tracker.utils.matching.iou_distance\\n<br><br>\\n\\n# v_iou_distance\\n---\\n:::ultralytics.tracker.utils.matching.v_iou_distance\\n<br><br>\\n\\n# embedding_distance\\n---\\n:::ultralytics.tracker.utils.matching.embedding_distance\\n<br><br>\\n\\n# gate_cost_matrix\\n---\\n:::ultralytics.tracker.utils.matching.gate_cost_matrix\\n<br><br>\\n\\n# fuse_motion\\n---\\n:::ultralytics.tracker.utils.matching.fuse_motion\\n<br><br>\\n\\n# fuse_iou\\n---\\n:::ultralytics.tracker.utils.matching.fuse_iou\\n<br><br>\\n\\n# fuse_score\\n---\\n:::ultralytics.tracker.utils.matching.fuse_score\\n<br><br>\\n\\n# bbox_ious\\n---\\n:::ultralytics.tracker.utils.matching.bbox_ious\\n<br><br>\\n', metadata={'file_path': 'docs/reference/tracker/utils/matching.md', 'file_name': 'matching.md', 'file_type': '.md'}), Document(page_content='# auto_annotate\\n---\\n:::ultralytics.yolo.data.annotator.auto_annotate\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/annotator.md', 'file_name': 'annotator.md', 'file_type': '.md'}), Document(page_content='# BaseTransform\\n---\\n:::ultralytics.yolo.data.augment.BaseTransform\\n<br><br>\\n\\n# Compose\\n---\\n:::ultralytics.yolo.data.augment.Compose\\n<br><br>\\n\\n# BaseMixTransform\\n---\\n:::ultralytics.yolo.data.augment.BaseMixTransform\\n<br><br>\\n\\n# Mosaic\\n---\\n:::ultralytics.yolo.data.augment.Mosaic\\n<br><br>\\n\\n# MixUp\\n---\\n:::ultralytics.yolo.data.augment.MixUp\\n<br><br>\\n\\n# RandomPerspective\\n---\\n:::ultralytics.yolo.data.augment.RandomPerspective\\n<br><br>\\n\\n# RandomHSV\\n---\\n:::ultralytics.yolo.data.augment.RandomHSV\\n<br><br>\\n\\n# RandomFlip\\n---\\n:::ultralytics.yolo.data.augment.RandomFlip\\n<br><br>\\n\\n# LetterBox\\n---\\n:::ultralytics.yolo.data.augment.LetterBox\\n<br><br>\\n\\n# CopyPaste\\n---\\n:::ultralytics.yolo.data.augment.CopyPaste\\n<br><br>\\n\\n# Albumentations\\n---\\n:::ultralytics.yolo.data.augment.Albumentations\\n<br><br>\\n\\n# Format\\n---\\n:::ultralytics.yolo.data.augment.Format\\n<br><br>\\n\\n# ClassifyLetterBox\\n---\\n:::ultralytics.yolo.data.augment.ClassifyLetterBox\\n<br><br>\\n\\n# CenterCrop\\n---\\n:::ultralytics.yolo.data.augment.CenterCrop\\n<br><br>\\n\\n# ToTensor\\n---\\n:::ultralytics.yolo.data.augment.ToTensor\\n<br><br>\\n\\n# v8_transforms\\n---\\n:::ultralytics.yolo.data.augment.v8_transforms\\n<br><br>\\n\\n# classify_transforms\\n---\\n:::ultralytics.yolo.data.augment.classify_transforms\\n<br><br>\\n\\n# classify_albumentations\\n---\\n:::ultralytics.yolo.data.augment.classify_albumentations\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/augment.md', 'file_name': 'augment.md', 'file_type': '.md'}), Document(page_content='# BaseDataset\\n---\\n:::ultralytics.yolo.data.base.BaseDataset\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/base.md', 'file_name': 'base.md', 'file_type': '.md'}), Document(page_content='# InfiniteDataLoader\\n---\\n:::ultralytics.yolo.data.build.InfiniteDataLoader\\n<br><br>\\n\\n# _RepeatSampler\\n---\\n:::ultralytics.yolo.data.build._RepeatSampler\\n<br><br>\\n\\n# seed_worker\\n---\\n:::ultralytics.yolo.data.build.seed_worker\\n<br><br>\\n\\n# build_yolo_dataset\\n---\\n:::ultralytics.yolo.data.build.build_yolo_dataset\\n<br><br>\\n\\n# build_dataloader\\n---\\n:::ultralytics.yolo.data.build.build_dataloader\\n<br><br>\\n\\n# check_source\\n---\\n:::ultralytics.yolo.data.build.check_source\\n<br><br>\\n\\n# load_inference_source\\n---\\n:::ultralytics.yolo.data.build.load_inference_source\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/build.md', 'file_name': 'build.md', 'file_type': '.md'}), Document(page_content='# YOLODataset\\n---\\n:::ultralytics.yolo.data.dataset.YOLODataset\\n<br><br>\\n\\n# ClassificationDataset\\n---\\n:::ultralytics.yolo.data.dataset.ClassificationDataset\\n<br><br>\\n\\n# SemanticDataset\\n---\\n:::ultralytics.yolo.data.dataset.SemanticDataset\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/dataset.md', 'file_name': 'dataset.md', 'file_type': '.md'}), Document(page_content='# MixAndRectDataset\\n---\\n:::ultralytics.yolo.data.dataset_wrappers.MixAndRectDataset\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/dataset_wrappers.md', 'file_name': 'dataset_wrappers.md', 'file_type': '.md'}), Document(page_content='# HUBDatasetStats\\n---\\n:::ultralytics.yolo.data.utils.HUBDatasetStats\\n<br><br>\\n\\n# img2label_paths\\n---\\n:::ultralytics.yolo.data.utils.img2label_paths\\n<br><br>\\n\\n# get_hash\\n---\\n:::ultralytics.yolo.data.utils.get_hash\\n<br><br>\\n\\n# exif_size\\n---\\n:::ultralytics.yolo.data.utils.exif_size\\n<br><br>\\n\\n# verify_image_label\\n---\\n:::ultralytics.yolo.data.utils.verify_image_label\\n<br><br>\\n\\n# polygon2mask\\n---\\n:::ultralytics.yolo.data.utils.polygon2mask\\n<br><br>\\n\\n# polygons2masks\\n---\\n:::ultralytics.yolo.data.utils.polygons2masks\\n<br><br>\\n\\n# polygons2masks_overlap\\n---\\n:::ultralytics.yolo.data.utils.polygons2masks_overlap\\n<br><br>\\n\\n# check_det_dataset\\n---\\n:::ultralytics.yolo.data.utils.check_det_dataset\\n<br><br>\\n\\n# check_cls_dataset\\n---\\n:::ultralytics.yolo.data.utils.check_cls_dataset\\n<br><br>\\n\\n# compress_one_image\\n---\\n:::ultralytics.yolo.data.utils.compress_one_image\\n<br><br>\\n\\n# delete_dsstore\\n---\\n:::ultralytics.yolo.data.utils.delete_dsstore\\n<br><br>\\n\\n# zip_directory\\n---\\n:::ultralytics.yolo.data.utils.zip_directory\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/utils.md', 'file_name': 'utils.md', 'file_type': '.md'}), Document(page_content='# Exporter\\n---\\n:::ultralytics.yolo.engine.exporter.Exporter\\n<br><br>\\n\\n# iOSDetectModel\\n---\\n:::ultralytics.yolo.engine.exporter.iOSDetectModel\\n<br><br>\\n\\n# export_formats\\n---\\n:::ultralytics.yolo.engine.exporter.export_formats\\n<br><br>\\n\\n# gd_outputs\\n---\\n:::ultralytics.yolo.engine.exporter.gd_outputs\\n<br><br>\\n\\n# try_export\\n---\\n:::ultralytics.yolo.engine.exporter.try_export\\n<br><br>\\n\\n# export\\n---\\n:::ultralytics.yolo.engine.exporter.export\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/engine/exporter.md', 'file_name': 'exporter.md', 'file_type': '.md'}), Document(page_content='# YOLO\\n---\\n:::ultralytics.yolo.engine.model.YOLO\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/engine/model.md', 'file_name': 'model.md', 'file_type': '.md'}), Document(page_content='# BasePredictor\\n---\\n:::ultralytics.yolo.engine.predictor.BasePredictor\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/engine/predictor.md', 'file_name': 'predictor.md', 'file_type': '.md'}), Document(page_content='# BaseTensor\\n---\\n:::ultralytics.yolo.engine.results.BaseTensor\\n<br><br>\\n\\n# Results\\n---\\n:::ultralytics.yolo.engine.results.Results\\n<br><br>\\n\\n# Boxes\\n---\\n:::ultralytics.yolo.engine.results.Boxes\\n<br><br>\\n\\n# Masks\\n---\\n:::ultralytics.yolo.engine.results.Masks\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/engine/results.md', 'file_name': 'results.md', 'file_type': '.md'}), Document(page_content='# BaseTrainer\\n---\\n:::ultralytics.yolo.engine.trainer.BaseTrainer\\n<br><br>\\n\\n# check_amp\\n---\\n:::ultralytics.yolo.engine.trainer.check_amp\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/engine/trainer.md', 'file_name': 'trainer.md', 'file_type': '.md'}), Document(page_content='# BaseValidator\\n---\\n:::ultralytics.yolo.engine.validator.BaseValidator\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/engine/validator.md', 'file_name': 'validator.md', 'file_type': '.md'}), Document(page_content='# check_train_batch_size\\n---\\n:::ultralytics.yolo.utils.autobatch.check_train_batch_size\\n<br><br>\\n\\n# autobatch\\n---\\n:::ultralytics.yolo.utils.autobatch.autobatch\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/autobatch.md', 'file_name': 'autobatch.md', 'file_type': '.md'}), Document(page_content='# benchmark\\n---\\n:::ultralytics.yolo.utils.benchmarks.benchmark\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/benchmarks.md', 'file_name': 'benchmarks.md', 'file_type': '.md'}), Document(page_content='# is_ascii\\n---\\n:::ultralytics.yolo.utils.checks.is_ascii\\n<br><br>\\n\\n# check_imgsz\\n---\\n:::ultralytics.yolo.utils.checks.check_imgsz\\n<br><br>\\n\\n# check_version\\n---\\n:::ultralytics.yolo.utils.checks.check_version\\n<br><br>\\n\\n# check_latest_pypi_version\\n---\\n:::ultralytics.yolo.utils.checks.check_latest_pypi_version\\n<br><br>\\n\\n# check_pip_update_available\\n---\\n:::ultralytics.yolo.utils.checks.check_pip_update_available\\n<br><br>\\n\\n# check_font\\n---\\n:::ultralytics.yolo.utils.checks.check_font\\n<br><br>\\n\\n# check_python\\n---\\n:::ultralytics.yolo.utils.checks.check_python\\n<br><br>\\n\\n# check_requirements\\n---\\n:::ultralytics.yolo.utils.checks.check_requirements\\n<br><br>\\n\\n# check_suffix\\n---\\n:::ultralytics.yolo.utils.checks.check_suffix\\n<br><br>\\n\\n# check_yolov5u_filename\\n---\\n:::ultralytics.yolo.utils.checks.check_yolov5u_filename\\n<br><br>\\n\\n# check_file\\n---\\n:::ultralytics.yolo.utils.checks.check_file\\n<br><br>\\n\\n# check_yaml\\n---\\n:::ultralytics.yolo.utils.checks.check_yaml\\n<br><br>\\n\\n# check_imshow\\n---\\n:::ultralytics.yolo.utils.checks.check_imshow\\n<br><br>\\n\\n# check_yolo\\n---\\n:::ultralytics.yolo.utils.checks.check_yolo\\n<br><br>\\n\\n# git_describe\\n---\\n:::ultralytics.yolo.utils.checks.git_describe\\n<br><br>\\n\\n# print_args\\n---\\n:::ultralytics.yolo.utils.checks.print_args\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/checks.md', 'file_name': 'checks.md', 'file_type': '.md'}), Document(page_content='# find_free_network_port\\n---\\n:::ultralytics.yolo.utils.dist.find_free_network_port\\n<br><br>\\n\\n# generate_ddp_file\\n---\\n:::ultralytics.yolo.utils.dist.generate_ddp_file\\n<br><br>\\n\\n# generate_ddp_command\\n---\\n:::ultralytics.yolo.utils.dist.generate_ddp_command\\n<br><br>\\n\\n# ddp_cleanup\\n---\\n:::ultralytics.yolo.utils.dist.ddp_cleanup\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/dist.md', 'file_name': 'dist.md', 'file_type': '.md'}), Document(page_content='# is_url\\n---\\n:::ultralytics.yolo.utils.downloads.is_url\\n<br><br>\\n\\n# unzip_file\\n---\\n:::ultralytics.yolo.utils.downloads.unzip_file\\n<br><br>\\n\\n# check_disk_space\\n---\\n:::ultralytics.yolo.utils.downloads.check_disk_space\\n<br><br>\\n\\n# safe_download\\n---\\n:::ultralytics.yolo.utils.downloads.safe_download\\n<br><br>\\n\\n# attempt_download_asset\\n---\\n:::ultralytics.yolo.utils.downloads.attempt_download_asset\\n<br><br>\\n\\n# download\\n---\\n:::ultralytics.yolo.utils.downloads.download\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/downloads.md', 'file_name': 'downloads.md', 'file_type': '.md'}), Document(page_content='# HUBModelError\\n---\\n:::ultralytics.yolo.utils.errors.HUBModelError\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/errors.md', 'file_name': 'errors.md', 'file_type': '.md'}), Document(page_content='# WorkingDirectory\\n---\\n:::ultralytics.yolo.utils.files.WorkingDirectory\\n<br><br>\\n\\n# increment_path\\n---\\n:::ultralytics.yolo.utils.files.increment_path\\n<br><br>\\n\\n# file_age\\n---\\n:::ultralytics.yolo.utils.files.file_age\\n<br><br>\\n\\n# file_date\\n---\\n:::ultralytics.yolo.utils.files.file_date\\n<br><br>\\n\\n# file_size\\n---\\n:::ultralytics.yolo.utils.files.file_size\\n<br><br>\\n\\n# get_latest_run\\n---\\n:::ultralytics.yolo.utils.files.get_latest_run\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/files.md', 'file_name': 'files.md', 'file_type': '.md'}), Document(page_content='# Bboxes\\n---\\n:::ultralytics.yolo.utils.instance.Bboxes\\n<br><br>\\n\\n# Instances\\n---\\n:::ultralytics.yolo.utils.instance.Instances\\n<br><br>\\n\\n# _ntuple\\n---\\n:::ultralytics.yolo.utils.instance._ntuple\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/instance.md', 'file_name': 'instance.md', 'file_type': '.md'}), Document(page_content='# VarifocalLoss\\n---\\n:::ultralytics.yolo.utils.loss.VarifocalLoss\\n<br><br>\\n\\n# BboxLoss\\n---\\n:::ultralytics.yolo.utils.loss.BboxLoss\\n<br><br>\\n\\n# KeypointLoss\\n---\\n:::ultralytics.yolo.utils.loss.KeypointLoss\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/loss.md', 'file_name': 'loss.md', 'file_type': '.md'}), Document(page_content='# FocalLoss\\n---\\n:::ultralytics.yolo.utils.metrics.FocalLoss\\n<br><br>\\n\\n# ConfusionMatrix\\n---\\n:::ultralytics.yolo.utils.metrics.ConfusionMatrix\\n<br><br>\\n\\n# Metric\\n---\\n:::ultralytics.yolo.utils.metrics.Metric\\n<br><br>\\n\\n# DetMetrics\\n---\\n:::ultralytics.yolo.utils.metrics.DetMetrics\\n<br><br>\\n\\n# SegmentMetrics\\n---\\n:::ultralytics.yolo.utils.metrics.SegmentMetrics\\n<br><br>\\n\\n# PoseMetrics\\n---\\n:::ultralytics.yolo.utils.metrics.PoseMetrics\\n<br><br>\\n\\n# ClassifyMetrics\\n---\\n:::ultralytics.yolo.utils.metrics.ClassifyMetrics\\n<br><br>\\n\\n# box_area\\n---\\n:::ultralytics.yolo.utils.metrics.box_area\\n<br><br>\\n\\n# bbox_ioa\\n---\\n:::ultralytics.yolo.utils.metrics.bbox_ioa\\n<br><br>\\n\\n# box_iou\\n---\\n:::ultralytics.yolo.utils.metrics.box_iou\\n<br><br>\\n\\n# bbox_iou\\n---\\n:::ultralytics.yolo.utils.metrics.bbox_iou\\n<br><br>\\n\\n# mask_iou\\n---\\n:::ultralytics.yolo.utils.metrics.mask_iou\\n<br><br>\\n\\n# kpt_iou\\n---\\n:::ultralytics.yolo.utils.metrics.kpt_iou\\n<br><br>\\n\\n# smooth_BCE\\n---\\n:::ultralytics.yolo.utils.metrics.smooth_BCE\\n<br><br>\\n\\n# smooth\\n---\\n:::ultralytics.yolo.utils.metrics.smooth\\n<br><br>\\n\\n# plot_pr_curve\\n---\\n:::ultralytics.yolo.utils.metrics.plot_pr_curve\\n<br><br>\\n\\n# plot_mc_curve\\n---\\n:::ultralytics.yolo.utils.metrics.plot_mc_curve\\n<br><br>\\n\\n# compute_ap\\n---\\n:::ultralytics.yolo.utils.metrics.compute_ap\\n<br><br>\\n\\n# ap_per_class\\n---\\n:::ultralytics.yolo.utils.metrics.ap_per_class\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/metrics.md', 'file_name': 'metrics.md', 'file_type': '.md'}), Document(page_content='# Profile\\n---\\n:::ultralytics.yolo.utils.ops.Profile\\n<br><br>\\n\\n# coco80_to_coco91_class\\n---\\n:::ultralytics.yolo.utils.ops.coco80_to_coco91_class\\n<br><br>\\n\\n# segment2box\\n---\\n:::ultralytics.yolo.utils.ops.segment2box\\n<br><br>\\n\\n# scale_boxes\\n---\\n:::ultralytics.yolo.utils.ops.scale_boxes\\n<br><br>\\n\\n# make_divisible\\n---\\n:::ultralytics.yolo.utils.ops.make_divisible\\n<br><br>\\n\\n# non_max_suppression\\n---\\n:::ultralytics.yolo.utils.ops.non_max_suppression\\n<br><br>\\n\\n# clip_boxes\\n---\\n:::ultralytics.yolo.utils.ops.clip_boxes\\n<br><br>\\n\\n# clip_coords\\n---\\n:::ultralytics.yolo.utils.ops.clip_coords\\n<br><br>\\n\\n# scale_image\\n---\\n:::ultralytics.yolo.utils.ops.scale_image\\n<br><br>\\n\\n# xyxy2xywh\\n---\\n:::ultralytics.yolo.utils.ops.xyxy2xywh\\n<br><br>\\n\\n# xywh2xyxy\\n---\\n:::ultralytics.yolo.utils.ops.xywh2xyxy\\n<br><br>\\n\\n# xywhn2xyxy\\n---\\n:::ultralytics.yolo.utils.ops.xywhn2xyxy\\n<br><br>\\n\\n# xyxy2xywhn\\n---\\n:::ultralytics.yolo.utils.ops.xyxy2xywhn\\n<br><br>\\n\\n# xyn2xy\\n---\\n:::ultralytics.yolo.utils.ops.xyn2xy\\n<br><br>\\n\\n# xywh2ltwh\\n---\\n:::ultralytics.yolo.utils.ops.xywh2ltwh\\n<br><br>\\n\\n# xyxy2ltwh\\n---\\n:::ultralytics.yolo.utils.ops.xyxy2ltwh\\n<br><br>\\n\\n# ltwh2xywh\\n---\\n:::ultralytics.yolo.utils.ops.ltwh2xywh\\n<br><br>\\n\\n# ltwh2xyxy\\n---\\n:::ultralytics.yolo.utils.ops.ltwh2xyxy\\n<br><br>\\n\\n# segments2boxes\\n---\\n:::ultralytics.yolo.utils.ops.segments2boxes\\n<br><br>\\n\\n# resample_segments\\n---\\n:::ultralytics.yolo.utils.ops.resample_segments\\n<br><br>\\n\\n# crop_mask\\n---\\n:::ultralytics.yolo.utils.ops.crop_mask\\n<br><br>\\n\\n# process_mask_upsample\\n---\\n:::ultralytics.yolo.utils.ops.process_mask_upsample\\n<br><br>\\n\\n# process_mask\\n---\\n:::ultralytics.yolo.utils.ops.process_mask\\n<br><br>\\n\\n# process_mask_native\\n---\\n:::ultralytics.yolo.utils.ops.process_mask_native\\n<br><br>\\n\\n# scale_coords\\n---\\n:::ultralytics.yolo.utils.ops.scale_coords\\n<br><br>\\n\\n# masks2segments\\n---\\n:::ultralytics.yolo.utils.ops.masks2segments\\n<br><br>\\n\\n# clean_str\\n---\\n:::ultralytics.yolo.utils.ops.clean_str\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/ops.md', 'file_name': 'ops.md', 'file_type': '.md'}), Document(page_content='# Colors\\n---\\n:::ultralytics.yolo.utils.plotting.Colors\\n<br><br>\\n\\n# Annotator\\n---\\n:::ultralytics.yolo.utils.plotting.Annotator\\n<br><br>\\n\\n# plot_labels\\n---\\n:::ultralytics.yolo.utils.plotting.plot_labels\\n<br><br>\\n\\n# save_one_box\\n---\\n:::ultralytics.yolo.utils.plotting.save_one_box\\n<br><br>\\n\\n# plot_images\\n---\\n:::ultralytics.yolo.utils.plotting.plot_images\\n<br><br>\\n\\n# plot_results\\n---\\n:::ultralytics.yolo.utils.plotting.plot_results\\n<br><br>\\n\\n# output_to_target\\n---\\n:::ultralytics.yolo.utils.plotting.output_to_target\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/plotting.md', 'file_name': 'plotting.md', 'file_type': '.md'}), Document(page_content='# TaskAlignedAssigner\\n---\\n:::ultralytics.yolo.utils.tal.TaskAlignedAssigner\\n<br><br>\\n\\n# select_candidates_in_gts\\n---\\n:::ultralytics.yolo.utils.tal.select_candidates_in_gts\\n<br><br>\\n\\n# select_highest_overlaps\\n---\\n:::ultralytics.yolo.utils.tal.select_highest_overlaps\\n<br><br>\\n\\n# make_anchors\\n---\\n:::ultralytics.yolo.utils.tal.make_anchors\\n<br><br>\\n\\n# dist2bbox\\n---\\n:::ultralytics.yolo.utils.tal.dist2bbox\\n<br><br>\\n\\n# bbox2dist\\n---\\n:::ultralytics.yolo.utils.tal.bbox2dist\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/tal.md', 'file_name': 'tal.md', 'file_type': '.md'}), Document(page_content='# ModelEMA\\n---\\n:::ultralytics.yolo.utils.torch_utils.ModelEMA\\n<br><br>\\n\\n# EarlyStopping\\n---\\n:::ultralytics.yolo.utils.torch_utils.EarlyStopping\\n<br><br>\\n\\n# torch_distributed_zero_first\\n---\\n:::ultralytics.yolo.utils.torch_utils.torch_distributed_zero_first\\n<br><br>\\n\\n# smart_inference_mode\\n---\\n:::ultralytics.yolo.utils.torch_utils.smart_inference_mode\\n<br><br>\\n\\n# select_device\\n---\\n:::ultralytics.yolo.utils.torch_utils.select_device\\n<br><br>\\n\\n# time_sync\\n---\\n:::ultralytics.yolo.utils.torch_utils.time_sync\\n<br><br>\\n\\n# fuse_conv_and_bn\\n---\\n:::ultralytics.yolo.utils.torch_utils.fuse_conv_and_bn\\n<br><br>\\n\\n# fuse_deconv_and_bn\\n---\\n:::ultralytics.yolo.utils.torch_utils.fuse_deconv_and_bn\\n<br><br>\\n\\n# model_info\\n---\\n:::ultralytics.yolo.utils.torch_utils.model_info\\n<br><br>\\n\\n# get_num_params\\n---\\n:::ultralytics.yolo.utils.torch_utils.get_num_params\\n<br><br>\\n\\n# get_num_gradients\\n---\\n:::ultralytics.yolo.utils.torch_utils.get_num_gradients\\n<br><br>\\n\\n# get_flops\\n---\\n:::ultralytics.yolo.utils.torch_utils.get_flops\\n<br><br>\\n\\n# initialize_weights\\n---\\n:::ultralytics.yolo.utils.torch_utils.initialize_weights\\n<br><br>\\n\\n# scale_img\\n---\\n:::ultralytics.yolo.utils.torch_utils.scale_img\\n<br><br>\\n\\n# make_divisible\\n---\\n:::ultralytics.yolo.utils.torch_utils.make_divisible\\n<br><br>\\n\\n# copy_attr\\n---\\n:::ultralytics.yolo.utils.torch_utils.copy_attr\\n<br><br>\\n\\n# get_latest_opset\\n---\\n:::ultralytics.yolo.utils.torch_utils.get_latest_opset\\n<br><br>\\n\\n# intersect_dicts\\n---\\n:::ultralytics.yolo.utils.torch_utils.intersect_dicts\\n<br><br>\\n\\n# is_parallel\\n---\\n:::ultralytics.yolo.utils.torch_utils.is_parallel\\n<br><br>\\n\\n# de_parallel\\n---\\n:::ultralytics.yolo.utils.torch_utils.de_parallel\\n<br><br>\\n\\n# one_cycle\\n---\\n:::ultralytics.yolo.utils.torch_utils.one_cycle\\n<br><br>\\n\\n# init_seeds\\n---\\n:::ultralytics.yolo.utils.torch_utils.init_seeds\\n<br><br>\\n\\n# strip_optimizer\\n---\\n:::ultralytics.yolo.utils.torch_utils.strip_optimizer\\n<br><br>\\n\\n# profile\\n---\\n:::ultralytics.yolo.utils.torch_utils.profile\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/torch_utils.md', 'file_name': 'torch_utils.md', 'file_type': '.md'}), Document(page_content='# SourceTypes\\n---\\n:::ultralytics.yolo.data.dataloaders.stream_loaders.SourceTypes\\n<br><br>\\n\\n# LoadStreams\\n---\\n:::ultralytics.yolo.data.dataloaders.stream_loaders.LoadStreams\\n<br><br>\\n\\n# LoadScreenshots\\n---\\n:::ultralytics.yolo.data.dataloaders.stream_loaders.LoadScreenshots\\n<br><br>\\n\\n# LoadImages\\n---\\n:::ultralytics.yolo.data.dataloaders.stream_loaders.LoadImages\\n<br><br>\\n\\n# LoadPilAndNumpy\\n---\\n:::ultralytics.yolo.data.dataloaders.stream_loaders.LoadPilAndNumpy\\n<br><br>\\n\\n# LoadTensor\\n---\\n:::ultralytics.yolo.data.dataloaders.stream_loaders.LoadTensor\\n<br><br>\\n\\n# autocast_list\\n---\\n:::ultralytics.yolo.data.dataloaders.stream_loaders.autocast_list\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/dataloaders/stream_loaders.md', 'file_name': 'stream_loaders.md', 'file_type': '.md'}), Document(page_content='# Albumentations\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.Albumentations\\n<br><br>\\n\\n# LetterBox\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.LetterBox\\n<br><br>\\n\\n# CenterCrop\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.CenterCrop\\n<br><br>\\n\\n# ToTensor\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.ToTensor\\n<br><br>\\n\\n# normalize\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.normalize\\n<br><br>\\n\\n# denormalize\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.denormalize\\n<br><br>\\n\\n# augment_hsv\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.augment_hsv\\n<br><br>\\n\\n# hist_equalize\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.hist_equalize\\n<br><br>\\n\\n# replicate\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.replicate\\n<br><br>\\n\\n# letterbox\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.letterbox\\n<br><br>\\n\\n# random_perspective\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.random_perspective\\n<br><br>\\n\\n# copy_paste\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.copy_paste\\n<br><br>\\n\\n# cutout\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.cutout\\n<br><br>\\n\\n# mixup\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.mixup\\n<br><br>\\n\\n# box_candidates\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.box_candidates\\n<br><br>\\n\\n# classify_albumentations\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.classify_albumentations\\n<br><br>\\n\\n# classify_transforms\\n---\\n:::ultralytics.yolo.data.dataloaders.v5augmentations.classify_transforms\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/dataloaders/v5augmentations.md', 'file_name': 'v5augmentations.md', 'file_type': '.md'}), Document(page_content='# InfiniteDataLoader\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.InfiniteDataLoader\\n<br><br>\\n\\n# _RepeatSampler\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader._RepeatSampler\\n<br><br>\\n\\n# LoadScreenshots\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.LoadScreenshots\\n<br><br>\\n\\n# LoadImages\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.LoadImages\\n<br><br>\\n\\n# LoadStreams\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.LoadStreams\\n<br><br>\\n\\n# LoadImagesAndLabels\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.LoadImagesAndLabels\\n<br><br>\\n\\n# ClassificationDataset\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.ClassificationDataset\\n<br><br>\\n\\n# get_hash\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.get_hash\\n<br><br>\\n\\n# exif_size\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.exif_size\\n<br><br>\\n\\n# exif_transpose\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.exif_transpose\\n<br><br>\\n\\n# seed_worker\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.seed_worker\\n<br><br>\\n\\n# create_dataloader\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.create_dataloader\\n<br><br>\\n\\n# img2label_paths\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.img2label_paths\\n<br><br>\\n\\n# flatten_recursive\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.flatten_recursive\\n<br><br>\\n\\n# extract_boxes\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.extract_boxes\\n<br><br>\\n\\n# autosplit\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.autosplit\\n<br><br>\\n\\n# verify_image_label\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.verify_image_label\\n<br><br>\\n\\n# create_classification_dataloader\\n---\\n:::ultralytics.yolo.data.dataloaders.v5loader.create_classification_dataloader\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/data/dataloaders/v5loader.md', 'file_name': 'v5loader.md', 'file_type': '.md'}), Document(page_content='# on_pretrain_routine_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_pretrain_routine_start\\n<br><br>\\n\\n# on_pretrain_routine_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_pretrain_routine_end\\n<br><br>\\n\\n# on_train_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_train_start\\n<br><br>\\n\\n# on_train_epoch_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_train_epoch_start\\n<br><br>\\n\\n# on_train_batch_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_train_batch_start\\n<br><br>\\n\\n# optimizer_step\\n---\\n:::ultralytics.yolo.utils.callbacks.base.optimizer_step\\n<br><br>\\n\\n# on_before_zero_grad\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_before_zero_grad\\n<br><br>\\n\\n# on_train_batch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_train_batch_end\\n<br><br>\\n\\n# on_train_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_train_epoch_end\\n<br><br>\\n\\n# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_fit_epoch_end\\n<br><br>\\n\\n# on_model_save\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_model_save\\n<br><br>\\n\\n# on_train_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_train_end\\n<br><br>\\n\\n# on_params_update\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_params_update\\n<br><br>\\n\\n# teardown\\n---\\n:::ultralytics.yolo.utils.callbacks.base.teardown\\n<br><br>\\n\\n# on_val_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_val_start\\n<br><br>\\n\\n# on_val_batch_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_val_batch_start\\n<br><br>\\n\\n# on_val_batch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_val_batch_end\\n<br><br>\\n\\n# on_val_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_val_end\\n<br><br>\\n\\n# on_predict_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_predict_start\\n<br><br>\\n\\n# on_predict_batch_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_predict_batch_start\\n<br><br>\\n\\n# on_predict_batch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_predict_batch_end\\n<br><br>\\n\\n# on_predict_postprocess_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_predict_postprocess_end\\n<br><br>\\n\\n# on_predict_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_predict_end\\n<br><br>\\n\\n# on_export_start\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_export_start\\n<br><br>\\n\\n# on_export_end\\n---\\n:::ultralytics.yolo.utils.callbacks.base.on_export_end\\n<br><br>\\n\\n# get_default_callbacks\\n---\\n:::ultralytics.yolo.utils.callbacks.base.get_default_callbacks\\n<br><br>\\n\\n# add_integration_callbacks\\n---\\n:::ultralytics.yolo.utils.callbacks.base.add_integration_callbacks\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/base.md', 'file_name': 'base.md', 'file_type': '.md'}), Document(page_content='# _log_debug_samples\\n---\\n:::ultralytics.yolo.utils.callbacks.clearml._log_debug_samples\\n<br><br>\\n\\n# _log_plot\\n---\\n:::ultralytics.yolo.utils.callbacks.clearml._log_plot\\n<br><br>\\n\\n# on_pretrain_routine_start\\n---\\n:::ultralytics.yolo.utils.callbacks.clearml.on_pretrain_routine_start\\n<br><br>\\n\\n# on_train_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.clearml.on_train_epoch_end\\n<br><br>\\n\\n# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.clearml.on_fit_epoch_end\\n<br><br>\\n\\n# on_val_end\\n---\\n:::ultralytics.yolo.utils.callbacks.clearml.on_val_end\\n<br><br>\\n\\n# on_train_end\\n---\\n:::ultralytics.yolo.utils.callbacks.clearml.on_train_end\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/clearml.md', 'file_name': 'clearml.md', 'file_type': '.md'}), Document(page_content='# _get_comet_mode\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._get_comet_mode\\n<br><br>\\n\\n# _get_comet_model_name\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._get_comet_model_name\\n<br><br>\\n\\n# _get_eval_batch_logging_interval\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._get_eval_batch_logging_interval\\n<br><br>\\n\\n# _get_max_image_predictions_to_log\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._get_max_image_predictions_to_log\\n<br><br>\\n\\n# _scale_confidence_score\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._scale_confidence_score\\n<br><br>\\n\\n# _should_log_confusion_matrix\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._should_log_confusion_matrix\\n<br><br>\\n\\n# _should_log_image_predictions\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._should_log_image_predictions\\n<br><br>\\n\\n# _get_experiment_type\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._get_experiment_type\\n<br><br>\\n\\n# _create_experiment\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._create_experiment\\n<br><br>\\n\\n# _fetch_trainer_metadata\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._fetch_trainer_metadata\\n<br><br>\\n\\n# _scale_bounding_box_to_original_image_shape\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._scale_bounding_box_to_original_image_shape\\n<br><br>\\n\\n# _format_ground_truth_annotations_for_detection\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._format_ground_truth_annotations_for_detection\\n<br><br>\\n\\n# _format_prediction_annotations_for_detection\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._format_prediction_annotations_for_detection\\n<br><br>\\n\\n# _fetch_annotations\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._fetch_annotations\\n<br><br>\\n\\n# _create_prediction_metadata_map\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._create_prediction_metadata_map\\n<br><br>\\n\\n# _log_confusion_matrix\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._log_confusion_matrix\\n<br><br>\\n\\n# _log_images\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._log_images\\n<br><br>\\n\\n# _log_image_predictions\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._log_image_predictions\\n<br><br>\\n\\n# _log_plots\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._log_plots\\n<br><br>\\n\\n# _log_model\\n---\\n:::ultralytics.yolo.utils.callbacks.comet._log_model\\n<br><br>\\n\\n# on_pretrain_routine_start\\n---\\n:::ultralytics.yolo.utils.callbacks.comet.on_pretrain_routine_start\\n<br><br>\\n\\n# on_train_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.comet.on_train_epoch_end\\n<br><br>\\n\\n# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.comet.on_fit_epoch_end\\n<br><br>\\n\\n# on_train_end\\n---\\n:::ultralytics.yolo.utils.callbacks.comet.on_train_end\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/comet.md', 'file_name': 'comet.md', 'file_type': '.md'}), Document(page_content='# on_pretrain_routine_end\\n---\\n:::ultralytics.yolo.utils.callbacks.hub.on_pretrain_routine_end\\n<br><br>\\n\\n# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.hub.on_fit_epoch_end\\n<br><br>\\n\\n# on_model_save\\n---\\n:::ultralytics.yolo.utils.callbacks.hub.on_model_save\\n<br><br>\\n\\n# on_train_end\\n---\\n:::ultralytics.yolo.utils.callbacks.hub.on_train_end\\n<br><br>\\n\\n# on_train_start\\n---\\n:::ultralytics.yolo.utils.callbacks.hub.on_train_start\\n<br><br>\\n\\n# on_val_start\\n---\\n:::ultralytics.yolo.utils.callbacks.hub.on_val_start\\n<br><br>\\n\\n# on_predict_start\\n---\\n:::ultralytics.yolo.utils.callbacks.hub.on_predict_start\\n<br><br>\\n\\n# on_export_start\\n---\\n:::ultralytics.yolo.utils.callbacks.hub.on_export_start\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/hub.md', 'file_name': 'hub.md', 'file_type': '.md'}), Document(page_content='# on_pretrain_routine_end\\n---\\n:::ultralytics.yolo.utils.callbacks.mlflow.on_pretrain_routine_end\\n<br><br>\\n\\n# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.mlflow.on_fit_epoch_end\\n<br><br>\\n\\n# on_train_end\\n---\\n:::ultralytics.yolo.utils.callbacks.mlflow.on_train_end\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/mlflow.md', 'file_name': 'mlflow.md', 'file_type': '.md'}), Document(page_content='# _log_scalars\\n---\\n:::ultralytics.yolo.utils.callbacks.neptune._log_scalars\\n<br><br>\\n\\n# _log_images\\n---\\n:::ultralytics.yolo.utils.callbacks.neptune._log_images\\n<br><br>\\n\\n# _log_plot\\n---\\n:::ultralytics.yolo.utils.callbacks.neptune._log_plot\\n<br><br>\\n\\n# on_pretrain_routine_start\\n---\\n:::ultralytics.yolo.utils.callbacks.neptune.on_pretrain_routine_start\\n<br><br>\\n\\n# on_train_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.neptune.on_train_epoch_end\\n<br><br>\\n\\n# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.neptune.on_fit_epoch_end\\n<br><br>\\n\\n# on_val_end\\n---\\n:::ultralytics.yolo.utils.callbacks.neptune.on_val_end\\n<br><br>\\n\\n# on_train_end\\n---\\n:::ultralytics.yolo.utils.callbacks.neptune.on_train_end\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/neptune.md', 'file_name': 'neptune.md', 'file_type': '.md'}), Document(page_content='# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.raytune.on_fit_epoch_end\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/raytune.md', 'file_name': 'raytune.md', 'file_type': '.md'}), Document(page_content='# _log_scalars\\n---\\n:::ultralytics.yolo.utils.callbacks.tensorboard._log_scalars\\n<br><br>\\n\\n# on_pretrain_routine_start\\n---\\n:::ultralytics.yolo.utils.callbacks.tensorboard.on_pretrain_routine_start\\n<br><br>\\n\\n# on_batch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.tensorboard.on_batch_end\\n<br><br>\\n\\n# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.tensorboard.on_fit_epoch_end\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/tensorboard.md', 'file_name': 'tensorboard.md', 'file_type': '.md'}), Document(page_content='# on_pretrain_routine_start\\n---\\n:::ultralytics.yolo.utils.callbacks.wb.on_pretrain_routine_start\\n<br><br>\\n\\n# on_fit_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.wb.on_fit_epoch_end\\n<br><br>\\n\\n# on_train_epoch_end\\n---\\n:::ultralytics.yolo.utils.callbacks.wb.on_train_epoch_end\\n<br><br>\\n\\n# on_train_end\\n---\\n:::ultralytics.yolo.utils.callbacks.wb.on_train_end\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/utils/callbacks/wb.md', 'file_name': 'wb.md', 'file_type': '.md'}), Document(page_content='# ClassificationPredictor\\n---\\n:::ultralytics.yolo.v8.classify.predict.ClassificationPredictor\\n<br><br>\\n\\n# predict\\n---\\n:::ultralytics.yolo.v8.classify.predict.predict\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/classify/predict.md', 'file_name': 'predict.md', 'file_type': '.md'}), Document(page_content='# ClassificationTrainer\\n---\\n:::ultralytics.yolo.v8.classify.train.ClassificationTrainer\\n<br><br>\\n\\n# train\\n---\\n:::ultralytics.yolo.v8.classify.train.train\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/classify/train.md', 'file_name': 'train.md', 'file_type': '.md'}), Document(page_content='# ClassificationValidator\\n---\\n:::ultralytics.yolo.v8.classify.val.ClassificationValidator\\n<br><br>\\n\\n# val\\n---\\n:::ultralytics.yolo.v8.classify.val.val\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/classify/val.md', 'file_name': 'val.md', 'file_type': '.md'}), Document(page_content='# DetectionPredictor\\n---\\n:::ultralytics.yolo.v8.detect.predict.DetectionPredictor\\n<br><br>\\n\\n# predict\\n---\\n:::ultralytics.yolo.v8.detect.predict.predict\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/detect/predict.md', 'file_name': 'predict.md', 'file_type': '.md'}), Document(page_content='# DetectionTrainer\\n---\\n:::ultralytics.yolo.v8.detect.train.DetectionTrainer\\n<br><br>\\n\\n# Loss\\n---\\n:::ultralytics.yolo.v8.detect.train.Loss\\n<br><br>\\n\\n# train\\n---\\n:::ultralytics.yolo.v8.detect.train.train\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/detect/train.md', 'file_name': 'train.md', 'file_type': '.md'}), Document(page_content='# DetectionValidator\\n---\\n:::ultralytics.yolo.v8.detect.val.DetectionValidator\\n<br><br>\\n\\n# val\\n---\\n:::ultralytics.yolo.v8.detect.val.val\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/detect/val.md', 'file_name': 'val.md', 'file_type': '.md'}), Document(page_content='# PosePredictor\\n---\\n:::ultralytics.yolo.v8.pose.predict.PosePredictor\\n<br><br>\\n\\n# predict\\n---\\n:::ultralytics.yolo.v8.pose.predict.predict\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/pose/predict.md', 'file_name': 'predict.md', 'file_type': '.md'}), Document(page_content='# PoseTrainer\\n---\\n:::ultralytics.yolo.v8.pose.train.PoseTrainer\\n<br><br>\\n\\n# PoseLoss\\n---\\n:::ultralytics.yolo.v8.pose.train.PoseLoss\\n<br><br>\\n\\n# train\\n---\\n:::ultralytics.yolo.v8.pose.train.train\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/pose/train.md', 'file_name': 'train.md', 'file_type': '.md'}), Document(page_content='# PoseValidator\\n---\\n:::ultralytics.yolo.v8.pose.val.PoseValidator\\n<br><br>\\n\\n# val\\n---\\n:::ultralytics.yolo.v8.pose.val.val\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/pose/val.md', 'file_name': 'val.md', 'file_type': '.md'}), Document(page_content='# SegmentationPredictor\\n---\\n:::ultralytics.yolo.v8.segment.predict.SegmentationPredictor\\n<br><br>\\n\\n# predict\\n---\\n:::ultralytics.yolo.v8.segment.predict.predict\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/segment/predict.md', 'file_name': 'predict.md', 'file_type': '.md'}), Document(page_content='# SegmentationTrainer\\n---\\n:::ultralytics.yolo.v8.segment.train.SegmentationTrainer\\n<br><br>\\n\\n# SegLoss\\n---\\n:::ultralytics.yolo.v8.segment.train.SegLoss\\n<br><br>\\n\\n# train\\n---\\n:::ultralytics.yolo.v8.segment.train.train\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/segment/train.md', 'file_name': 'train.md', 'file_type': '.md'}), Document(page_content='# SegmentationValidator\\n---\\n:::ultralytics.yolo.v8.segment.val.SegmentationValidator\\n<br><br>\\n\\n# val\\n---\\n:::ultralytics.yolo.v8.segment.val.val\\n<br><br>\\n', metadata={'file_path': 'docs/reference/yolo/v8/segment/val.md', 'file_name': 'val.md', 'file_type': '.md'})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional (Filtering out the required data)"
      ],
      "metadata": {
        "id": "phIsLD3nSQiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "def getonlyrelevant(data,address=[\"docs/modes/\"]):\n",
        "  docid=[]\n",
        "  for path in address:\n",
        "    for i in range(len(data)):\n",
        "      # print(data[i].metadata['file_path'])\n",
        "      cur_path=data[i].metadata['file_path']\n",
        "      if path in cur_path:\n",
        "        print(\"exists\")\n",
        "        docid.append(i)\n",
        "  return docid\n",
        "\n"
      ],
      "metadata": {
        "id": "xhnrZqI1Cv0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docid"
      ],
      "metadata": {
        "id": "xyNYb6B7TpKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "required_paths=[\"docs/modes/pre\"]\n",
        "docids=getonlyrelevant(data,required_paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftinBLWGDF-a",
        "outputId": "aaa13834-af9f-448c-8fb0-5c13700768d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1gbX5Y2FWQn",
        "outputId": "f13e1bfd-cdac-44d7-9e76-71d5e5280150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[25]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplication of the data (Optional)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kMa2TMSGEJI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_duplicate(num):\n",
        "  duplicate=[]\n",
        "  for i in num:\n",
        "    duplicate.append(data[i])\n",
        "\n",
        "  return duplicate\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "UcLsdwIXJKBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=create_duplicate(docids)"
      ],
      "metadata": {
        "id": "hBy8MmswJ0zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9MYaUyMJ6Gs",
        "outputId": "32827dd1-f41c-4026-a8ff-9e99be6d35ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "cFQhjz2IKVtM",
        "outputId": "75b9c61b-a6df-452e-eada-497b5384d283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'---\\ncomments: true\\n---\\n\\n## Callbacks\\n\\nUltralytics framework supports callbacks as entry points in strategic stages of train, val, export, and predict modes.\\nEach callback accepts a `Trainer`, `Validator`, or `Predictor` object depending on the operation type. All properties of\\nthese objects can be found in Reference section of the docs.\\n\\n## Examples\\n\\n### Returning additional information with Prediction\\n\\nIn this example, we want to return the original frame with each result object. Here\\'s how we can do that\\n\\n```python\\ndef on_predict_batch_end(predictor):\\n    # Retrieve the batch data\\n    _, im0s, _, _ = predictor.batch\\n    \\n    # Ensure that im0s is a list\\n    im0s = im0s if isinstance(im0s, list) else [im0s]\\n    \\n    # Combine the prediction results with the corresponding frames\\n    predictor.results = zip(predictor.results, im0s)\\n\\n# Create a YOLO model instance\\nmodel = YOLO(f\\'yolov8n.pt\\')\\n\\n# Add the custom callback to the model\\nmodel.add_callback(\"on_predict_batch_end\", on_predict_batch_end)\\n\\n# Iterate through the results and frames\\nfor (result, frame) in model.track/predict():\\n    pass\\n```\\n\\n## All callbacks\\n\\nHere are all supported callbacks. See callbacks [source code](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/utils/callbacks/base.py) for additional details.\\n\\n\\n### Trainer Callbacks\\n\\n| Callback                    | Description                                             |\\n|-----------------------------|---------------------------------------------------------|\\n| `on_pretrain_routine_start` | Triggered at the beginning of pre-training routine      |\\n| `on_pretrain_routine_end`   | Triggered at the end of pre-training routine            |\\n| `on_train_start`            | Triggered when the training starts                      |\\n| `on_train_epoch_start`      | Triggered at the start of each training epoch           |\\n| `on_train_batch_start`      | Triggered at the start of each training batch           |\\n| `optimizer_step`            | Triggered during the optimizer step                     |\\n| `on_before_zero_grad`       | Triggered before gradients are zeroed                   |\\n| `on_train_batch_end`        | Triggered at the end of each training batch             |\\n| `on_train_epoch_end`        | Triggered at the end of each training epoch             |\\n| `on_fit_epoch_end`          | Triggered at the end of each fit epoch                  |\\n| `on_model_save`             | Triggered when the model is saved                       |\\n| `on_train_end`              | Triggered when the training process ends                |\\n| `on_params_update`          | Triggered when model parameters are updated             |\\n| `teardown`                  | Triggered when the training process is being cleaned up |\\n\\n\\n### Validator Callbacks\\n\\n| Callback             | Description                                     |\\n|----------------------|-------------------------------------------------|\\n| `on_val_start`       | Triggered when the validation starts            |\\n| `on_val_batch_start` | Triggered at the start of each validation batch |\\n| `on_val_batch_end`   | Triggered at the end of each validation batch   |\\n| `on_val_end`         | Triggered when the validation ends              |\\n\\n\\n### Predictor Callbacks\\n\\n| Callback                     | Description                                       |\\n|------------------------------|---------------------------------------------------|\\n| `on_predict_start`           | Triggered when the prediction process starts      |\\n| `on_predict_batch_start`     | Triggered at the start of each prediction batch   |\\n| `on_predict_postprocess_end` | Triggered at the end of prediction postprocessing |\\n| `on_predict_batch_end`       | Triggered at the end of each prediction batch     |\\n| `on_predict_end`             | Triggered when the prediction process ends        |\\n\\n### Exporter Callbacks\\n\\n| Callback          | Description                              |\\n|-------------------|------------------------------------------|\\n| `on_export_start` | Triggered when the export process starts |\\n| `on_export_end`   | Triggered when the export process ends   |\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(test)):\n",
        "  print(test[i].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2PaH-9rKBFr",
        "outputId": "cb87c5f9-ff3a-4ed0-b03d-780e4cc2e1cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "## Callbacks\n",
            "\n",
            "Ultralytics framework supports callbacks as entry points in strategic stages of train, val, export, and predict modes.\n",
            "Each callback accepts a `Trainer`, `Validator`, or `Predictor` object depending on the operation type. All properties of\n",
            "these objects can be found in Reference section of the docs.\n",
            "\n",
            "## Examples\n",
            "\n",
            "### Returning additional information with Prediction\n",
            "\n",
            "In this example, we want to return the original frame with each result object. Here's how we can do that\n",
            "\n",
            "```python\n",
            "def on_predict_batch_end(predictor):\n",
            "    # Retrieve the batch data\n",
            "    _, im0s, _, _ = predictor.batch\n",
            "    \n",
            "    # Ensure that im0s is a list\n",
            "    im0s = im0s if isinstance(im0s, list) else [im0s]\n",
            "    \n",
            "    # Combine the prediction results with the corresponding frames\n",
            "    predictor.results = zip(predictor.results, im0s)\n",
            "\n",
            "# Create a YOLO model instance\n",
            "model = YOLO(f'yolov8n.pt')\n",
            "\n",
            "# Add the custom callback to the model\n",
            "model.add_callback(\"on_predict_batch_end\", on_predict_batch_end)\n",
            "\n",
            "# Iterate through the results and frames\n",
            "for (result, frame) in model.track/predict():\n",
            "    pass\n",
            "```\n",
            "\n",
            "## All callbacks\n",
            "\n",
            "Here are all supported callbacks. See callbacks [source code](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/utils/callbacks/base.py) for additional details.\n",
            "\n",
            "\n",
            "### Trainer Callbacks\n",
            "\n",
            "| Callback                    | Description                                             |\n",
            "|-----------------------------|---------------------------------------------------------|\n",
            "| `on_pretrain_routine_start` | Triggered at the beginning of pre-training routine      |\n",
            "| `on_pretrain_routine_end`   | Triggered at the end of pre-training routine            |\n",
            "| `on_train_start`            | Triggered when the training starts                      |\n",
            "| `on_train_epoch_start`      | Triggered at the start of each training epoch           |\n",
            "| `on_train_batch_start`      | Triggered at the start of each training batch           |\n",
            "| `optimizer_step`            | Triggered during the optimizer step                     |\n",
            "| `on_before_zero_grad`       | Triggered before gradients are zeroed                   |\n",
            "| `on_train_batch_end`        | Triggered at the end of each training batch             |\n",
            "| `on_train_epoch_end`        | Triggered at the end of each training epoch             |\n",
            "| `on_fit_epoch_end`          | Triggered at the end of each fit epoch                  |\n",
            "| `on_model_save`             | Triggered when the model is saved                       |\n",
            "| `on_train_end`              | Triggered when the training process ends                |\n",
            "| `on_params_update`          | Triggered when model parameters are updated             |\n",
            "| `teardown`                  | Triggered when the training process is being cleaned up |\n",
            "\n",
            "\n",
            "### Validator Callbacks\n",
            "\n",
            "| Callback             | Description                                     |\n",
            "|----------------------|-------------------------------------------------|\n",
            "| `on_val_start`       | Triggered when the validation starts            |\n",
            "| `on_val_batch_start` | Triggered at the start of each validation batch |\n",
            "| `on_val_batch_end`   | Triggered at the end of each validation batch   |\n",
            "| `on_val_end`         | Triggered when the validation ends              |\n",
            "\n",
            "\n",
            "### Predictor Callbacks\n",
            "\n",
            "| Callback                     | Description                                       |\n",
            "|------------------------------|---------------------------------------------------|\n",
            "| `on_predict_start`           | Triggered when the prediction process starts      |\n",
            "| `on_predict_batch_start`     | Triggered at the start of each prediction batch   |\n",
            "| `on_predict_postprocess_end` | Triggered at the end of prediction postprocessing |\n",
            "| `on_predict_batch_end`       | Triggered at the end of each prediction batch     |\n",
            "| `on_predict_end`             | Triggered when the prediction process ends        |\n",
            "\n",
            "### Exporter Callbacks\n",
            "\n",
            "| Callback          | Description                              |\n",
            "|-------------------|------------------------------------------|\n",
            "| `on_export_start` | Triggered when the export process starts |\n",
            "| `on_export_end`   | Triggered when the export process ends   |\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "YOLO settings and hyperparameters play a critical role in the model's performance, speed, and accuracy. These settings\n",
            "and hyperparameters can affect the model's behavior at various stages of the model development process, including\n",
            "training, validation, and prediction.\n",
            "\n",
            "YOLOv8 'yolo' CLI commands use the following syntax:\n",
            "\n",
            "!!! example \"\"\n",
            "\n",
            "    === \"CLI\"\n",
            "    \n",
            "        ```bash\n",
            "        yolo TASK MODE ARGS\n",
            "        ```\n",
            "\n",
            "    === \"Python\"\n",
            "    \n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "        \n",
            "        # Load a YOLOv8 model from a pre-trained weights file\n",
            "        model = YOLO('yolov8n.pt')\n",
            "         \n",
            "        # Run MODE mode using the custom arguments ARGS (guess TASK)\n",
            "        model.MODE(ARGS)\n",
            "        ```\n",
            "\n",
            "Where:\n",
            "\n",
            "- `TASK` (optional) is one of `[detect, segment, classify, pose]`. If it is not passed explicitly YOLOv8 will try to\n",
            "  guess\n",
            "  the `TASK` from the model type.\n",
            "- `MODE` (required) is one of `[train, val, predict, export, track, benchmark]`\n",
            "- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults.\n",
            "  For a full list of available `ARGS` see the [Configuration](cfg.md) page and `defaults.yaml`\n",
            "  GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/cfg/default.yaml).\n",
            "\n",
            "#### Tasks\n",
            "\n",
            "YOLO models can be used for a variety of tasks, including detection, segmentation, classification and pose. These tasks\n",
            "differ in the type of output they produce and the specific problem they are designed to solve.\n",
            "\n",
            "**Detect**: For identifying and localizing objects or regions of interest in an image or video.  \n",
            "**Segment**: For dividing an image or video into regions or pixels that correspond to different objects or classes.  \n",
            "**Classify**: For predicting the class label of an input image.  \n",
            "**Pose**: For identifying objects and estimating their keypoints in an image or video.\n",
            "\n",
            "| Key    | Value      | Description                                     |\n",
            "|--------|------------|-------------------------------------------------|\n",
            "| `task` | `'detect'` | YOLO task, i.e. detect, segment, classify, pose |\n",
            "\n",
            "[Tasks Guide](../tasks/index.md){ .md-button .md-button--primary}\n",
            "\n",
            "#### Modes\n",
            "\n",
            "YOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes\n",
            "include:\n",
            "\n",
            "**Train**: For training a YOLOv8 model on a custom dataset.  \n",
            "**Val**: For validating a YOLOv8 model after it has been trained.  \n",
            "**Predict**: For making predictions using a trained YOLOv8 model on new images or videos.  \n",
            "**Export**: For exporting a YOLOv8 model to a format that can be used for deployment.  \n",
            "**Track**: For tracking objects in real-time using a YOLOv8 model.  \n",
            "**Benchmark**: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.\n",
            "\n",
            "| Key    | Value     | Description                                                   |\n",
            "|--------|-----------|---------------------------------------------------------------|\n",
            "| `mode` | `'train'` | YOLO mode, i.e. train, val, predict, export, track, benchmark |\n",
            "\n",
            "[Modes Guide](../modes/index.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Train\n",
            "\n",
            "The training settings for YOLO models encompass various hyperparameters and configurations used during the training process. These settings influence the model's performance, speed, and accuracy. Key training settings include batch size, learning rate, momentum, and weight decay. Additionally, the choice of optimizer, loss function, and training dataset composition can impact the training process. Careful tuning and experimentation with these settings are crucial for optimizing performance.\n",
            "\n",
            "| Key               | Value    | Description                                                                 |\n",
            "|-------------------|----------|-----------------------------------------------------------------------------|\n",
            "| `model`           | `None`   | path to model file, i.e. yolov8n.pt, yolov8n.yaml                           |\n",
            "| `data`            | `None`   | path to data file, i.e. coco128.yaml                                        |\n",
            "| `epochs`          | `100`    | number of epochs to train for                                               |\n",
            "| `patience`        | `50`     | epochs to wait for no observable improvement for early stopping of training |\n",
            "| `batch`           | `16`     | number of images per batch (-1 for AutoBatch)                               |\n",
            "| `imgsz`           | `640`    | size of input images as integer or w,h                                      |\n",
            "| `save`            | `True`   | save train checkpoints and predict results                                  |\n",
            "| `save_period`     | `-1`     | Save checkpoint every x epochs (disabled if < 1)                            |\n",
            "| `cache`           | `False`  | True/ram, disk or False. Use cache for data loading                         |\n",
            "| `device`          | `None`   | device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu        |\n",
            "| `workers`         | `8`      | number of worker threads for data loading (per RANK if DDP)                 |\n",
            "| `project`         | `None`   | project name                                                                |\n",
            "| `name`            | `None`   | experiment name                                                             |\n",
            "| `exist_ok`        | `False`  | whether to overwrite existing experiment                                    |\n",
            "| `pretrained`      | `False`  | whether to use a pretrained model                                           |\n",
            "| `optimizer`       | `'SGD'`  | optimizer to use, choices=['SGD', 'Adam', 'AdamW', 'RMSProp']               |\n",
            "| `verbose`         | `False`  | whether to print verbose output                                             |\n",
            "| `seed`            | `0`      | random seed for reproducibility                                             |\n",
            "| `deterministic`   | `True`   | whether to enable deterministic mode                                        |\n",
            "| `single_cls`      | `False`  | train multi-class data as single-class                                      |\n",
            "| `rect`            | `False`  | rectangular training with each batch collated for minimum padding           |\n",
            "| `cos_lr`          | `False`  | use cosine learning rate scheduler                                          |\n",
            "| `close_mosaic`    | `0`      | (int) disable mosaic augmentation for final epochs                          |\n",
            "| `resume`          | `False`  | resume training from last checkpoint                                        |\n",
            "| `amp`             | `True`   | Automatic Mixed Precision (AMP) training, choices=[True, False]             |\n",
            "| `lr0`             | `0.01`   | initial learning rate (i.e. SGD=1E-2, Adam=1E-3)                            |\n",
            "| `lrf`             | `0.01`   | final learning rate (lr0 * lrf)                                             |\n",
            "| `momentum`        | `0.937`  | SGD momentum/Adam beta1                                                     |\n",
            "| `weight_decay`    | `0.0005` | optimizer weight decay 5e-4                                                 |\n",
            "| `warmup_epochs`   | `3.0`    | warmup epochs (fractions ok)                                                |\n",
            "| `warmup_momentum` | `0.8`    | warmup initial momentum                                                     |\n",
            "| `warmup_bias_lr`  | `0.1`    | warmup initial bias lr                                                      |\n",
            "| `box`             | `7.5`    | box loss gain                                                               |\n",
            "| `cls`             | `0.5`    | cls loss gain (scale with pixels)                                           |\n",
            "| `dfl`             | `1.5`    | dfl loss gain                                                               |\n",
            "| `pose`            | `12.0`   | pose loss gain (pose-only)                                                  |\n",
            "| `kobj`            | `2.0`    | keypoint obj loss gain (pose-only)                                          |\n",
            "| `label_smoothing` | `0.0`    | label smoothing (fraction)                                                  |\n",
            "| `nbs`             | `64`     | nominal batch size                                                          |\n",
            "| `overlap_mask`    | `True`   | masks should overlap during training (segment train only)                   |\n",
            "| `mask_ratio`      | `4`      | mask downsample ratio (segment train only)                                  |\n",
            "| `dropout`         | `0.0`    | use dropout regularization (classify train only)                            |\n",
            "| `val`             | `True`   | validate/test during training                                               |\n",
            "\n",
            "[Train Guide](../modes/train.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Predict\n",
            "\n",
            "The prediction settings for YOLO models encompass a range of hyperparameters and configurations that influence the model's performance, speed, and accuracy during inference on new data. Careful tuning and experimentation with these settings are essential to achieve optimal performance for a specific task. Key settings include the confidence threshold, Non-Maximum Suppression (NMS) threshold, and the number of classes considered. Additional factors affecting the prediction process are input data size and format, the presence of supplementary features such as masks or multiple labels per box, and the particular task the model is employed for.\n",
            "\n",
            "| Key              | Value                  | Description                                              |\n",
            "|------------------|------------------------|----------------------------------------------------------|\n",
            "| `source`         | `'ultralytics/assets'` | source directory for images or videos                    |\n",
            "| `conf`           | `0.25`                 | object confidence threshold for detection                |\n",
            "| `iou`            | `0.7`                  | intersection over union (IoU) threshold for NMS          |\n",
            "| `half`           | `False`                | use half precision (FP16)                                |\n",
            "| `device`         | `None`                 | device to run on, i.e. cuda device=0/1/2/3 or device=cpu |\n",
            "| `show`           | `False`                | show results if possible                                 |\n",
            "| `save`           | `False`                | save images with results                                 |\n",
            "| `save_txt`       | `False`                | save results as .txt file                                |\n",
            "| `save_conf`      | `False`                | save results with confidence scores                      |\n",
            "| `save_crop`      | `False`                | save cropped images with results                         |\n",
            "| `show_labels`    | `True`                 | show object labels in plots                              |\n",
            "| `show_conf`      | `True`                 | show object confidence scores in plots                   |\n",
            "| `max_det`        | `300`                  | maximum number of detections per image                   |\n",
            "| `vid_stride`     | `False`                | video frame-rate stride                                  |\n",
            "| `line_thickness` | `3`                    | bounding box thickness (pixels)                          |\n",
            "| `visualize`      | `False`                | visualize model features                                 |\n",
            "| `augment`        | `False`                | apply image augmentation to prediction sources           |\n",
            "| `agnostic_nms`   | `False`                | class-agnostic NMS                                       |\n",
            "| `retina_masks`   | `False`                | use high-resolution segmentation masks                   |\n",
            "| `classes`        | `None`                 | filter results by class, i.e. class=0, or class=[0,2,3]  |\n",
            "| `boxes`          | `True`                 | Show boxes in segmentation predictions                   |\n",
            "\n",
            "[Predict Guide](../modes/predict.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Val\n",
            "\n",
            "The val (validation) settings for YOLO models involve various hyperparameters and configurations used to evaluate the model's performance on a validation dataset. These settings influence the model's performance, speed, and accuracy. Common YOLO validation settings include batch size, validation frequency during training, and performance evaluation metrics. Other factors affecting the validation process include the validation dataset's size and composition, as well as the specific task the model is employed for. Careful tuning and experimentation with these settings are crucial to ensure optimal performance on the validation dataset and detect and prevent overfitting.\n",
            "\n",
            "| Key           | Value   | Description                                                        |\n",
            "|---------------|---------|--------------------------------------------------------------------|\n",
            "| `save_json`   | `False` | save results to JSON file                                          |\n",
            "| `save_hybrid` | `False` | save hybrid version of labels (labels + additional predictions)    |\n",
            "| `conf`        | `0.001` | object confidence threshold for detection                          |\n",
            "| `iou`         | `0.6`   | intersection over union (IoU) threshold for NMS                    |\n",
            "| `max_det`     | `300`   | maximum number of detections per image                             |\n",
            "| `half`        | `True`  | use half precision (FP16)                                          |\n",
            "| `device`      | `None`  | device to run on, i.e. cuda device=0/1/2/3 or device=cpu           |\n",
            "| `dnn`         | `False` | use OpenCV DNN for ONNX inference                                  |\n",
            "| `plots`       | `False` | show plots during training                                         |\n",
            "| `rect`        | `False` | rectangular val with each batch collated for minimum padding       |\n",
            "| `split`       | `val`   | dataset split to use for validation, i.e. 'val', 'test' or 'train' |\n",
            "\n",
            "[Val Guide](../modes/val.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Export\n",
            "\n",
            "Export settings for YOLO models encompass configurations and options related to saving or exporting the model for use in different environments or platforms. These settings can impact the model's performance, size, and compatibility with various systems. Key export settings include the exported model file format (e.g., ONNX, TensorFlow SavedModel), the target device (e.g., CPU, GPU), and additional features such as masks or multiple labels per box. The export process may also be affected by the model's specific task and the requirements or constraints of the destination environment or platform. It is crucial to thoughtfully configure these settings to ensure the exported model is optimized for the intended use case and functions effectively in the target environment.\n",
            "\n",
            "| Key         | Value           | Description                                          |\n",
            "|-------------|-----------------|------------------------------------------------------|\n",
            "| `format`    | `'torchscript'` | format to export to                                  |\n",
            "| `imgsz`     | `640`           | image size as scalar or (h, w) list, i.e. (640, 480) |\n",
            "| `keras`     | `False`         | use Keras for TF SavedModel export                   |\n",
            "| `optimize`  | `False`         | TorchScript: optimize for mobile                     |\n",
            "| `half`      | `False`         | FP16 quantization                                    |\n",
            "| `int8`      | `False`         | INT8 quantization                                    |\n",
            "| `dynamic`   | `False`         | ONNX/TF/TensorRT: dynamic axes                       |\n",
            "| `simplify`  | `False`         | ONNX: simplify model                                 |\n",
            "| `opset`     | `None`          | ONNX: opset version (optional, defaults to latest)   |\n",
            "| `workspace` | `4`             | TensorRT: workspace size (GB)                        |\n",
            "| `nms`       | `False`         | CoreML: add NMS                                      |\n",
            "\n",
            "[Export Guide](../modes/export.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Augmentation\n",
            "\n",
            "Augmentation settings for YOLO models refer to the various transformations and modifications\n",
            "applied to the training data to increase the diversity and size of the dataset. These settings can affect the model's\n",
            "performance, speed, and accuracy. Some common YOLO augmentation settings include the type and intensity of the\n",
            "transformations applied (e.g. random flips, rotations, cropping, color changes), the probability with which each\n",
            "transformation is applied, and the presence of additional features such as masks or multiple labels per box. Other\n",
            "factors that may affect the augmentation process include the size and composition of the original dataset and the\n",
            "specific task the model is being used for. It is important to carefully tune and experiment with these settings to\n",
            "ensure that the augmented dataset is diverse and representative enough to train a high-performing model.\n",
            "\n",
            "| Key           | Value | Description                                     |\n",
            "|---------------|-------|-------------------------------------------------|\n",
            "| `hsv_h`       | 0.015 | image HSV-Hue augmentation (fraction)           |\n",
            "| `hsv_s`       | 0.7   | image HSV-Saturation augmentation (fraction)    |\n",
            "| `hsv_v`       | 0.4   | image HSV-Value augmentation (fraction)         |\n",
            "| `degrees`     | 0.0   | image rotation (+/- deg)                        |\n",
            "| `translate`   | 0.1   | image translation (+/- fraction)                |\n",
            "| `scale`       | 0.5   | image scale (+/- gain)                          |\n",
            "| `shear`       | 0.0   | image shear (+/- deg)                           |\n",
            "| `perspective` | 0.0   | image perspective (+/- fraction), range 0-0.001 |\n",
            "| `flipud`      | 0.0   | image flip up-down (probability)                |\n",
            "| `fliplr`      | 0.5   | image flip left-right (probability)             |\n",
            "| `mosaic`      | 1.0   | image mosaic (probability)                      |\n",
            "| `mixup`       | 0.0   | image mixup (probability)                       |\n",
            "| `copy_paste`  | 0.0   | segment copy-paste (probability)                |\n",
            "\n",
            "## Logging, checkpoints, plotting and file management\n",
            "\n",
            "Logging, checkpoints, plotting, and file management are important considerations when training a YOLO model.\n",
            "\n",
            "- Logging: It is often helpful to log various metrics and statistics during training to track the model's progress and\n",
            "  diagnose any issues that may arise. This can be done using a logging library such as TensorBoard or by writing log\n",
            "  messages to a file.\n",
            "- Checkpoints: It is a good practice to save checkpoints of the model at regular intervals during training. This allows\n",
            "  you to resume training from a previous point if the training process is interrupted or if you want to experiment with\n",
            "  different training configurations.\n",
            "- Plotting: Visualizing the model's performance and training progress can be helpful for understanding how the model is\n",
            "  behaving and identifying potential issues. This can be done using a plotting library such as matplotlib or by\n",
            "  generating plots using a logging library such as TensorBoard.\n",
            "- File management: Managing the various files generated during the training process, such as model checkpoints, log\n",
            "  files, and plots, can be challenging. It is important to have a clear and organized file structure to keep track of\n",
            "  these files and make it easy to access and analyze them as needed.\n",
            "\n",
            "Effective logging, checkpointing, plotting, and file management can help you keep track of the model's progress and make\n",
            "it easier to debug and optimize the training process.\n",
            "\n",
            "| Key        | Value    | Description                                                                                    |\n",
            "|------------|----------|------------------------------------------------------------------------------------------------|\n",
            "| `project`  | `'runs'` | project name                                                                                   |\n",
            "| `name`     | `'exp'`  | experiment name. `exp` gets automatically incremented if not specified, i.e, `exp`, `exp2` ... |\n",
            "| `exist_ok` | `False`  | whether to overwrite existing experiment                                                       |\n",
            "| `plots`    | `False`  | save plots during train/val                                                                    |\n",
            "| `save`     | `False`  | save train checkpoints and predict results                                                     |\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Command Line Interface Usage\n",
            "\n",
            "The YOLO command line interface (CLI) allows for simple single-line commands without the need for a Python environment.\n",
            "CLI requires no customization or Python code. You can simply run all tasks from the terminal with the `yolo` command.\n",
            "\n",
            "!!! example\n",
            "\n",
            "    === \"Syntax\"\n",
            "\n",
            "        Ultralytics `yolo` commands use the following syntax:\n",
            "        ```bash\n",
            "        yolo TASK MODE ARGS\n",
            "\n",
            "        Where   TASK (optional) is one of [detect, segment, classify]\n",
            "                MODE (required) is one of [train, val, predict, export, track]\n",
            "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
            "        ```\n",
            "        See all ARGS in the full [Configuration Guide](./cfg.md) or with `yolo cfg`\n",
            "\n",
            "    === \"Train\"\n",
            "\n",
            "        Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
            "        ```bash\n",
            "        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n",
            "        ```\n",
            "\n",
            "    === \"Predict\"\n",
            "\n",
            "        Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
            "        ```bash\n",
            "        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n",
            "        ```\n",
            "\n",
            "    === \"Val\"\n",
            "\n",
            "        Val a pretrained detection model at batch-size 1 and image size 640:\n",
            "        ```bash\n",
            "        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n",
            "        ```\n",
            "\n",
            "    === \"Export\"\n",
            "\n",
            "        Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
            "        ```bash\n",
            "        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n",
            "        ```\n",
            "\n",
            "    === \"Special\"\n",
            "\n",
            "        Run special commands to see version, view settings, run checks and more:\n",
            "        ```bash\n",
            "        yolo help\n",
            "        yolo checks\n",
            "        yolo version\n",
            "        yolo settings\n",
            "        yolo copy-cfg\n",
            "        yolo cfg\n",
            "        ```\n",
            "\n",
            "Where:\n",
            "\n",
            "- `TASK` (optional) is one of `[detect, segment, classify]`. If it is not passed explicitly YOLOv8 will try to guess\n",
            "  the `TASK` from the model type.\n",
            "- `MODE` (required) is one of `[train, val, predict, export, track]`\n",
            "- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults.\n",
            "  For a full list of available `ARGS` see the [Configuration](cfg.md) page and `defaults.yaml`\n",
            "  GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/cfg/default.yaml).\n",
            "\n",
            "!!! warning \"Warning\"\n",
            "\n",
            "    Arguments must be passed as `arg=val` pairs, split by an equals `=` sign and delimited by spaces ` ` between pairs. Do not use `--` argument prefixes or commas `,` beteen arguments.\n",
            "\n",
            "    - `yolo predict model=yolov8n.pt imgsz=640 conf=0.25` &nbsp; ✅\n",
            "    - `yolo predict model yolov8n.pt imgsz 640 conf 0.25` &nbsp; ❌\n",
            "    - `yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25` &nbsp; ❌\n",
            "\n",
            "## Train\n",
            "\n",
            "Train YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. For a full list of available arguments see\n",
            "the [Configuration](cfg.md) page.\n",
            "\n",
            "!!! example \"Example\"\n",
            "\n",
            "    === \"Train\"\n",
            "        \n",
            "        Start training YOLOv8n on COCO128 for 100 epochs at image-size 640.\n",
            "        ```bash\n",
            "        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n",
            "        ```\n",
            "\n",
            "    === \"Resume\"\n",
            "\n",
            "        Resume an interrupted training.\n",
            "        ```bash\n",
            "        yolo detect train resume model=last.pt\n",
            "        ```\n",
            "\n",
            "## Val\n",
            "\n",
            "Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the `model` retains it's\n",
            "training `data` and arguments as model attributes.\n",
            "\n",
            "!!! example \"Example\"\n",
            "\n",
            "    === \"Official\"\n",
            "\n",
            "        Validate an official YOLOv8n model.\n",
            "        ```bash\n",
            "        yolo detect val model=yolov8n.pt\n",
            "        ```\n",
            "\n",
            "    === \"Custom\"\n",
            "\n",
            "        Validate a custom-trained model.\n",
            "        ```bash\n",
            "        yolo detect val model=path/to/best.pt\n",
            "        ```\n",
            "\n",
            "## Predict\n",
            "\n",
            "Use a trained YOLOv8n model to run predictions on images.\n",
            "\n",
            "!!! example \"Example\"\n",
            "\n",
            "    === \"Official\"\n",
            "\n",
            "        Predict with an official YOLOv8n model.\n",
            "        ```bash\n",
            "        yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'\n",
            "        ```\n",
            "\n",
            "    === \"Custom\"\n",
            "\n",
            "        Predict with a custom model.\n",
            "        ```bash\n",
            "        yolo detect predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'\n",
            "        ```\n",
            "\n",
            "## Export\n",
            "\n",
            "Export a YOLOv8n model to a different format like ONNX, CoreML, etc.\n",
            "\n",
            "!!! example \"Example\"\n",
            "\n",
            "    === \"Official\"\n",
            "\n",
            "        Export an official YOLOv8n model to ONNX format.\n",
            "        ```bash\n",
            "        yolo export model=yolov8n.pt format=onnx\n",
            "        ```\n",
            "\n",
            "    === \"Custom\"\n",
            "\n",
            "        Export a custom-trained model to ONNX format.\n",
            "        ```bash\n",
            "        yolo export model=path/to/best.pt format=onnx\n",
            "        ```\n",
            "\n",
            "Available YOLOv8 export formats are in the table below. You can export to any format using the `format` argument,\n",
            "i.e. `format='onnx'` or `format='engine'`.\n",
            "\n",
            "| Format                                                             | `format` Argument | Model                     | Metadata |\n",
            "|--------------------------------------------------------------------|-------------------|---------------------------|----------|\n",
            "| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | ✅        |\n",
            "| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | ✅        |\n",
            "| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | ✅        |\n",
            "| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | ✅        |\n",
            "| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | ✅        |\n",
            "| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | ✅        |\n",
            "| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | ✅        |\n",
            "| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | ❌        |\n",
            "| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | ✅        |\n",
            "| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ✅        |\n",
            "| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | ✅        |\n",
            "| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | ✅        |\n",
            "\n",
            "---\n",
            "\n",
            "## Overriding default arguments\n",
            "\n",
            "Default arguments can be overridden by simply passing them as arguments in the CLI in `arg=value` pairs.\n",
            "\n",
            "!!! tip \"\"\n",
            "\n",
            "    === \"Train\"\n",
            "        Train a detection model for `10 epochs` with `learning_rate` of `0.01`\n",
            "        ```bash\n",
            "        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n",
            "        ```\n",
            "\n",
            "    === \"Predict\"\n",
            "        Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
            "        ```bash\n",
            "        yolo segment predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n",
            "        ```\n",
            "\n",
            "    === \"Val\"\n",
            "        Validate a pretrained detection model at batch-size 1 and image size 640:\n",
            "        ```bash\n",
            "        yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n",
            "        ```\n",
            "\n",
            "---\n",
            "\n",
            "## Overriding default config file\n",
            "\n",
            "You can override the `default.yaml` config file entirely by passing a new file with the `cfg` arguments,\n",
            "i.e. `cfg=custom.yaml`.\n",
            "\n",
            "To do this first create a copy of `default.yaml` in your current working dir with the `yolo copy-cfg` command.\n",
            "\n",
            "This will create `default_copy.yaml`, which you can then pass as `cfg=default_copy.yaml` along with any additional args,\n",
            "like `imgsz=320` in this example:\n",
            "\n",
            "!!! example \"\"\n",
            "\n",
            "    === \"CLI\"\n",
            "        ```bash\n",
            "        yolo copy-cfg\n",
            "        yolo cfg=default_copy.yaml imgsz=320\n",
            "        ```\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "Both the Ultralytics YOLO command-line and python interfaces are simply a high-level abstraction on the base engine\n",
            "executors. Let's take a look at the Trainer engine.\n",
            "\n",
            "## BaseTrainer\n",
            "\n",
            "BaseTrainer contains the generic boilerplate training routine. It can be customized for any task based over overriding\n",
            "the required functions or operations as long the as correct formats are followed. For example, you can support your own\n",
            "custom model and dataloader by just overriding these functions:\n",
            "\n",
            "* `get_model(cfg, weights)` - The function that builds the model to be trained\n",
            "* `get_dataloder()` - The function that builds the dataloader\n",
            "  More details and source code can be found in [`BaseTrainer` Reference](../reference/yolo/engine/trainer.md)\n",
            "\n",
            "## DetectionTrainer\n",
            "\n",
            "Here's how you can use the YOLOv8 `DetectionTrainer` and customize it.\n",
            "\n",
            "```python\n",
            "from ultralytics.yolo.v8.detect import DetectionTrainer\n",
            "\n",
            "trainer = DetectionTrainer(overrides={...})\n",
            "trainer.train()\n",
            "trained_model = trainer.best  # get best model\n",
            "```\n",
            "\n",
            "### Customizing the DetectionTrainer\n",
            "\n",
            "Let's customize the trainer **to train a custom detection model** that is not supported directly. You can do this by\n",
            "simply overloading the existing the `get_model` functionality:\n",
            "\n",
            "```python\n",
            "from ultralytics.yolo.v8.detect import DetectionTrainer\n",
            "\n",
            "\n",
            "class CustomTrainer(DetectionTrainer):\n",
            "    def get_model(self, cfg, weights):\n",
            "        ...\n",
            "\n",
            "\n",
            "trainer = CustomTrainer(overrides={...})\n",
            "trainer.train()\n",
            "```\n",
            "\n",
            "You now realize that you need to customize the trainer further to:\n",
            "\n",
            "* Customize the `loss function`.\n",
            "* Add `callback` that uploads model to your Google Drive after every 10 `epochs`\n",
            "  Here's how you can do it:\n",
            "\n",
            "```python\n",
            "from ultralytics.yolo.v8.detect import DetectionTrainer\n",
            "\n",
            "\n",
            "class CustomTrainer(DetectionTrainer):\n",
            "    def get_model(self, cfg, weights):\n",
            "        ...\n",
            "\n",
            "    def criterion(self, preds, batch):\n",
            "        # get ground truth\n",
            "        imgs = batch[\"imgs\"]\n",
            "        bboxes = batch[\"bboxes\"]\n",
            "        ...\n",
            "        return loss, loss_items  # see Reference-> Trainer for details on the expected format\n",
            "\n",
            "\n",
            "# callback to upload model weights\n",
            "def log_model(trainer):\n",
            "    last_weight_path = trainer.last\n",
            "    ...\n",
            "\n",
            "\n",
            "trainer = CustomTrainer(overrides={...})\n",
            "trainer.add_callback(\"on_train_epoch_end\", log_model)  # Adds to existing callback\n",
            "trainer.train()\n",
            "```\n",
            "\n",
            "To know more about Callback triggering events and entry point, checkout our [Callbacks Guide](callbacks.md)\n",
            "\n",
            "## Other engine components\n",
            "\n",
            "There are other components that can be customized similarly like `Validators` and `Predictors`\n",
            "See Reference section for more information on these.\n",
            "\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Hyperparameter Tuning with Ray Tune and YOLOv8\n",
            "\n",
            "Hyperparameter tuning (or hyperparameter optimization) is the process of determining the right combination of hyperparameters that maximizes model performance. It works by running multiple trials in a single training process, evaluating the performance of each trial, and selecting the best hyperparameter values based on the evaluation results.\n",
            "\n",
            "## Ultralytics YOLOv8 and Ray Tune Integration\n",
            "\n",
            "[Ultralytics](https://ultralytics.com) YOLOv8 integrates hyperparameter tuning with Ray Tune, allowing you to easily optimize your YOLOv8 model's hyperparameters. By using Ray Tune, you can leverage advanced search algorithms, parallelism, and early stopping to speed up the tuning process and achieve better model performance.\n",
            "\n",
            "###  Ray Tune\n",
            "\n",
            "<div align=\"center\">\n",
            "<a href=\"https://docs.ray.io/en/latest/tune/index.html\" target=\"_blank\">\n",
            "<img width=\"480\" src=\"https://docs.ray.io/en/latest/_images/tune_overview.png\"></a>\n",
            "</div>\n",
            "\n",
            "[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) is a powerful and flexible hyperparameter tuning library for machine learning models. It provides an efficient way to optimize hyperparameters by supporting various search algorithms, parallelism, and early stopping strategies. Ray Tune's flexible architecture enables seamless integration with popular machine learning frameworks, including Ultralytics YOLOv8.\n",
            "\n",
            "### Weights & Biases\n",
            "\n",
            "YOLOv8 also supports optional integration with [Weights & Biases](https://wandb.ai/site) (wandb) for tracking the tuning progress.\n",
            "\n",
            "## Installation\n",
            "\n",
            "To install the required packages, run:\n",
            "\n",
            "!!! tip \"Installation\"\n",
            "\n",
            "    ```bash\n",
            "    pip install -U ultralytics \"ray[tune]\"  # install and/or update\n",
            "    pip install wandb  # optional\n",
            "    ```\n",
            "\n",
            "## Usage\n",
            "\n",
            "!!! example \"Usage\"\n",
            "\n",
            "    ```python\n",
            "    from ultralytics import YOLO\n",
            "\n",
            "    model = YOLO(\"yolov8n.pt\")\n",
            "    results = model.tune(data=\"coco128.yaml\")\n",
            "    ```\n",
            "\n",
            "## `tune()` Method Parameters\n",
            "\n",
            "The `tune()` method in YOLOv8 provides an easy-to-use interface for hyperparameter tuning with Ray Tune. It accepts several arguments that allow you to customize the tuning process. Below is a detailed explanation of each parameter:\n",
            "\n",
            "| Parameter       | Type           | Description                                                                                                                                                                                                                                                                                                                   | Default Value |\n",
            "|-----------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
            "| `data`          | str            | The dataset configuration file (in YAML format) to run the tuner on. This file should specify the training and validation data paths, as well as other dataset-specific settings.                                                                                                                                             |               |\n",
            "| `space`         | dict, optional | A dictionary defining the hyperparameter search space for Ray Tune. Each key corresponds to a hyperparameter name, and the value specifies the range of values to explore during tuning. If not provided, YOLOv8 uses a default search space with various hyperparameters.                                                    |               |\n",
            "| `grace_period`  | int, optional  | The grace period in epochs for the [ASHA scheduler](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-asha) in Ray Tune. The scheduler will not terminate any trial before this number of epochs, allowing the model to have some minimum training before making a decision on early stopping. | 10            |\n",
            "| `gpu_per_trial` | int, optional  | The number of GPUs to allocate per trial during tuning. This helps manage GPU usage, particularly in multi-GPU environments. If not provided, the tuner will use all available GPUs.                                                                                                                                          | None          |\n",
            "| `max_samples`   | int, optional  | The maximum number of trials to run during tuning. This parameter helps control the total number of hyperparameter combinations tested, ensuring the tuning process does not run indefinitely.                                                                                                                                | 10            |\n",
            "| `train_args`    | dict, optional | A dictionary of additional arguments to pass to the `train()` method during tuning. These arguments can include settings like the number of training epochs, batch size, and other training-specific configurations.                                                                                                          | {}            |\n",
            "\n",
            "By customizing these parameters, you can fine-tune the hyperparameter optimization process to suit your specific needs and available computational resources.\n",
            "\n",
            "## Default Search Space Description\n",
            "\n",
            "The following table lists the default search space parameters for hyperparameter tuning in YOLOv8 with Ray Tune. Each parameter has a specific value range defined by `tune.uniform()`.\n",
            "\n",
            "| Parameter       | Value Range                | Description                              |\n",
            "|-----------------|----------------------------|------------------------------------------|\n",
            "| lr0             | `tune.uniform(1e-5, 1e-1)` | Initial learning rate                    |\n",
            "| lrf             | `tune.uniform(0.01, 1.0)`  | Final learning rate factor               |\n",
            "| momentum        | `tune.uniform(0.6, 0.98)`  | Momentum                                 |\n",
            "| weight_decay    | `tune.uniform(0.0, 0.001)` | Weight decay                             |\n",
            "| warmup_epochs   | `tune.uniform(0.0, 5.0)`   | Warmup epochs                            |\n",
            "| warmup_momentum | `tune.uniform(0.0, 0.95)`  | Warmup momentum                          |\n",
            "| box             | `tune.uniform(0.02, 0.2)`  | Box loss weight                          |\n",
            "| cls             | `tune.uniform(0.2, 4.0)`   | Class loss weight                        |\n",
            "| hsv_h           | `tune.uniform(0.0, 0.1)`   | Hue augmentation range                   |\n",
            "| hsv_s           | `tune.uniform(0.0, 0.9)`   | Saturation augmentation range            |\n",
            "| hsv_v           | `tune.uniform(0.0, 0.9)`   | Value (brightness) augmentation range    |\n",
            "| degrees         | `tune.uniform(0.0, 45.0)`  | Rotation augmentation range (degrees)    |\n",
            "| translate       | `tune.uniform(0.0, 0.9)`   | Translation augmentation range           |\n",
            "| scale           | `tune.uniform(0.0, 0.9)`   | Scaling augmentation range               |\n",
            "| shear           | `tune.uniform(0.0, 10.0)`  | Shear augmentation range (degrees)       |\n",
            "| perspective     | `tune.uniform(0.0, 0.001)` | Perspective augmentation range           |\n",
            "| flipud          | `tune.uniform(0.0, 1.0)`   | Vertical flip augmentation probability   |\n",
            "| fliplr          | `tune.uniform(0.0, 1.0)`   | Horizontal flip augmentation probability |\n",
            "| mosaic          | `tune.uniform(0.0, 1.0)`   | Mosaic augmentation probability          |\n",
            "| mixup           | `tune.uniform(0.0, 1.0)`   | Mixup augmentation probability           |\n",
            "| copy_paste      | `tune.uniform(0.0, 1.0)`   | Copy-paste augmentation probability      |\n",
            "\n",
            "\n",
            "## Custom Search Space Example\n",
            "\n",
            "In this example, we demonstrate how to use a custom search space for hyperparameter tuning with Ray Tune and YOLOv8. By providing a custom search space, you can focus the tuning process on specific hyperparameters of interest.\n",
            "\n",
            "!!! example \"Usage\"\n",
            "\n",
            "    ```python\n",
            "    from ultralytics import YOLO\n",
            "    from ray import tune\n",
            "    \n",
            "    model = YOLO(\"yolov8n.pt\")\n",
            "    result = model.tune(\n",
            "        data=\"coco128.yaml\",\n",
            "        space={\"lr0\": tune.uniform(1e-5, 1e-1)},\n",
            "        train_args={\"epochs\": 50}\n",
            "    )\n",
            "    ```\n",
            "\n",
            "In the code snippet above, we create a YOLO model with the \"yolov8n.pt\" pretrained weights. Then, we call the `tune()` method, specifying the dataset configuration with \"coco128.yaml\". We provide a custom search space for the initial learning rate `lr0` using a dictionary with the key \"lr0\" and the value `tune.uniform(1e-5, 1e-1)`. Finally, we pass additional training arguments, such as the number of epochs, using the `train_args` parameter.\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Python Usage\n",
            "\n",
            "Welcome to the YOLOv8 Python Usage documentation! This guide is designed to help you seamlessly integrate YOLOv8 into\n",
            "your Python projects for object detection, segmentation, and classification. Here, you'll learn how to load and use\n",
            "pretrained models, train new models, and perform predictions on images. The easy-to-use Python interface is a valuable\n",
            "resource for anyone looking to incorporate YOLOv8 into their Python projects, allowing you to quickly implement advanced\n",
            "object detection capabilities. Let's get started!\n",
            "\n",
            "For example, users can load a model, train it, evaluate its performance on a validation set, and even export it to ONNX\n",
            "format with just a few lines of code.\n",
            "\n",
            "!!! example \"Python\"\n",
            "\n",
            "    ```python\n",
            "    from ultralytics import YOLO\n",
            "    \n",
            "    # Create a new YOLO model from scratch\n",
            "    model = YOLO('yolov8n.yaml')\n",
            "    \n",
            "    # Load a pretrained YOLO model (recommended for training)\n",
            "    model = YOLO('yolov8n.pt')\n",
            "    \n",
            "    # Train the model using the 'coco128.yaml' dataset for 3 epochs\n",
            "    results = model.train(data='coco128.yaml', epochs=3)\n",
            "    \n",
            "    # Evaluate the model's performance on the validation set\n",
            "    results = model.val()\n",
            "    \n",
            "    # Perform object detection on an image using the model\n",
            "    results = model('https://ultralytics.com/images/bus.jpg')\n",
            "    \n",
            "    # Export the model to ONNX format\n",
            "    success = model.export(format='onnx')\n",
            "    ```\n",
            "\n",
            "## [Train](../modes/train.md)\n",
            "\n",
            "Train mode is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the\n",
            "specified dataset and hyperparameters. The training process involves optimizing the model's parameters so that it can\n",
            "accurately predict the classes and locations of objects in an image.\n",
            "\n",
            "!!! example \"Train\"\n",
            "\n",
            "    === \"From pretrained(recommended)\"\n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "\n",
            "        model = YOLO('yolov8n.pt') # pass any model type\n",
            "        model.train(epochs=5)\n",
            "        ```\n",
            "\n",
            "    === \"From scratch\"\n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "\n",
            "        model = YOLO('yolov8n.yaml')\n",
            "        model.train(data='coco128.yaml', epochs=5)\n",
            "        ```\n",
            "\n",
            "    === \"Resume\"\n",
            "        ```python\n",
            "        model = YOLO(\"last.pt\")\n",
            "        model.train(resume=True)\n",
            "        ```\n",
            "\n",
            "[Train Examples](../modes/train.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Val](../modes/val.md)\n",
            "\n",
            "Val mode is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a\n",
            "validation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters\n",
            "of the model to improve its performance.\n",
            "\n",
            "!!! example \"Val\"\n",
            "\n",
            "    === \"Val after training\"\n",
            "        ```python\n",
            "          from ultralytics import YOLO\n",
            "\n",
            "          model = YOLO('yolov8n.yaml')\n",
            "          model.train(data='coco128.yaml', epochs=5)\n",
            "          model.val()  # It'll automatically evaluate the data you trained.\n",
            "        ```\n",
            "\n",
            "    === \"Val independently\"\n",
            "        ```python\n",
            "          from ultralytics import YOLO\n",
            "\n",
            "          model = YOLO(\"model.pt\")\n",
            "          # It'll use the data yaml file in model.pt if you don't set data.\n",
            "          model.val()\n",
            "          # or you can set the data you want to val\n",
            "          model.val(data='coco128.yaml')\n",
            "        ```\n",
            "\n",
            "[Val Examples](../modes/val.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Predict](../modes/predict.md)\n",
            "\n",
            "Predict mode is used for making predictions using a trained YOLOv8 model on new images or videos. In this mode, the\n",
            "model is loaded from a checkpoint file, and the user can provide images or videos to perform inference. The model\n",
            "predicts the classes and locations of objects in the input images or videos.\n",
            "\n",
            "!!! example \"Predict\"\n",
            "\n",
            "    === \"From source\"\n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "        from PIL import Image\n",
            "        import cv2\n",
            "\n",
            "        model = YOLO(\"model.pt\")\n",
            "        # accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\n",
            "        results = model.predict(source=\"0\")\n",
            "        results = model.predict(source=\"folder\", show=True) # Display preds. Accepts all YOLO predict arguments\n",
            "\n",
            "        # from PIL\n",
            "        im1 = Image.open(\"bus.jpg\")\n",
            "        results = model.predict(source=im1, save=True)  # save plotted images\n",
            "\n",
            "        # from ndarray\n",
            "        im2 = cv2.imread(\"bus.jpg\")\n",
            "        results = model.predict(source=im2, save=True, save_txt=True)  # save predictions as labels\n",
            "\n",
            "        # from list of PIL/ndarray\n",
            "        results = model.predict(source=[im1, im2])\n",
            "        ```\n",
            "\n",
            "    === \"Results usage\"\n",
            "        ```python\n",
            "        # results would be a list of Results object including all the predictions by default\n",
            "        # but be careful as it could occupy a lot memory when there're many images, \n",
            "        # especially the task is segmentation.\n",
            "        # 1. return as a list\n",
            "        results = model.predict(source=\"folder\")\n",
            "\n",
            "        # results would be a generator which is more friendly to memory by setting stream=True\n",
            "        # 2. return as a generator\n",
            "        results = model.predict(source=0, stream=True)\n",
            "\n",
            "        for result in results:\n",
            "            # Detection\n",
            "            result.boxes.xyxy   # box with xyxy format, (N, 4)\n",
            "            result.boxes.xywh   # box with xywh format, (N, 4)\n",
            "            result.boxes.xyxyn  # box with xyxy format but normalized, (N, 4)\n",
            "            result.boxes.xywhn  # box with xywh format but normalized, (N, 4)\n",
            "            result.boxes.conf   # confidence score, (N, 1)\n",
            "            result.boxes.cls    # cls, (N, 1)\n",
            "\n",
            "            # Segmentation\n",
            "            result.masks.data      # masks, (N, H, W)\n",
            "            result.masks.xy        # x,y segments (pixels), List[segment] * N\n",
            "            result.masks.xyn       # x,y segments (normalized), List[segment] * N\n",
            "\n",
            "            # Classification\n",
            "            result.probs     # cls prob, (num_class, )\n",
            "\n",
            "        # Each result is composed of torch.Tensor by default, \n",
            "        # in which you can easily use following functionality:\n",
            "        result = result.cuda()\n",
            "        result = result.cpu()\n",
            "        result = result.to(\"cpu\")\n",
            "        result = result.numpy()\n",
            "        ```\n",
            "\n",
            "[Predict Examples](../modes/predict.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Export](../modes/export.md)\n",
            "\n",
            "Export mode is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the model is\n",
            "converted to a format that can be used by other software applications or hardware devices. This mode is useful when\n",
            "deploying the model to production environments.\n",
            "\n",
            "!!! example \"Export\"\n",
            "\n",
            "    === \"Export to ONNX\"\n",
            "\n",
            "        Export an official YOLOv8n model to ONNX with dynamic batch-size and image-size.\n",
            "        ```python\n",
            "          from ultralytics import YOLO\n",
            "\n",
            "          model = YOLO('yolov8n.pt')\n",
            "          model.export(format='onnx', dynamic=True)\n",
            "        ```\n",
            "\n",
            "    === \"Export to TensorRT\"\n",
            "\n",
            "        Export an official YOLOv8n model to TensorRT on `device=0` for acceleration on CUDA devices.\n",
            "        ```python\n",
            "          from ultralytics import YOLO\n",
            "\n",
            "          model = YOLO('yolov8n.pt')\n",
            "          model.export(format='onnx', device=0)\n",
            "        ```\n",
            "\n",
            "[Export Examples](../modes/export.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Track](../modes/track.md)\n",
            "\n",
            "Track mode is used for tracking objects in real-time using a YOLOv8 model. In this mode, the model is loaded from a\n",
            "checkpoint file, and the user can provide a live video stream to perform real-time object tracking. This mode is useful\n",
            "for applications such as surveillance systems or self-driving cars.\n",
            "\n",
            "!!! example \"Track\"\n",
            "\n",
            "    === \"Python\"\n",
            "    \n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "        \n",
            "        # Load a model\n",
            "        model = YOLO('yolov8n.pt')  # load an official detection model\n",
            "        model = YOLO('yolov8n-seg.pt')  # load an official segmentation model\n",
            "        model = YOLO('path/to/best.pt')  # load a custom model\n",
            "        \n",
            "        # Track with the model\n",
            "        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True) \n",
            "        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True, tracker=\"bytetrack.yaml\") \n",
            "        ```\n",
            "\n",
            "[Track Examples](../modes/track.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Benchmark](../modes/benchmark.md)\n",
            "\n",
            "Benchmark mode is used to profile the speed and accuracy of various export formats for YOLOv8. The benchmarks provide\n",
            "information on the size of the exported format, its `mAP50-95` metrics (for object detection and segmentation)\n",
            "or `accuracy_top5` metrics (for classification), and the inference time in milliseconds per image across various export\n",
            "formats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for\n",
            "their specific use case based on their requirements for speed and accuracy.\n",
            "\n",
            "!!! example \"Benchmark\"\n",
            "\n",
            "    === \"Python\"\n",
            "    \n",
            "        Benchmark an official YOLOv8n model across all export formats.\n",
            "        ```python\n",
            "        from ultralytics.yolo.utils.benchmarks import benchmark\n",
            "        \n",
            "        # Benchmark\n",
            "        benchmark(model='yolov8n.pt', imgsz=640, half=False, device=0)\n",
            "        ```\n",
            "\n",
            "[Benchmark Examples](../modes/benchmark.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Using Trainers\n",
            "\n",
            "`YOLO` model class is a high-level wrapper on the Trainer classes. Each YOLO task has its own trainer that inherits\n",
            "from `BaseTrainer`.\n",
            "\n",
            "!!! tip \"Detection Trainer Example\"\n",
            "\n",
            "        ```python\n",
            "        from ultralytics.yolo import v8 import DetectionTrainer, DetectionValidator, DetectionPredictor\n",
            "\n",
            "        # trainer\n",
            "        trainer = DetectionTrainer(overrides={})\n",
            "        trainer.train()\n",
            "        trained_model = trainer.best\n",
            "\n",
            "        # Validator\n",
            "        val = DetectionValidator(args=...)\n",
            "        val(model=trained_model)\n",
            "\n",
            "        # predictor\n",
            "        pred = DetectionPredictor(overrides={})\n",
            "        pred(source=SOURCE, model=trained_model)\n",
            "\n",
            "        # resume from last weight\n",
            "        overrides[\"resume\"] = trainer.last\n",
            "        trainer = detect.DetectionTrainer(overrides=overrides)\n",
            "        ```\n",
            "\n",
            "You can easily customize Trainers to support custom tasks or explore R&D ideas.\n",
            "Learn more about Customizing `Trainers`, `Validators` and `Predictors` to suit your project needs in the Customization\n",
            "Section.\n",
            "\n",
            "[Customization tutorials](engine.md){ .md-button .md-button--primary}\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Models\n",
            "\n",
            "Ultralytics supports many models and architectures with more to come in the future. Want to add your model architecture? [Here's](../help/contributing.md) how you can contribute.\n",
            "\n",
            "In this documentation, we provide information on four major models:\n",
            "\n",
            "1. [YOLOv3](./yolov3.md): The third iteration of the YOLO model family, known for its efficient real-time object detection capabilities.\n",
            "2. [YOLOv5](./yolov5.md): An improved version of the YOLO architecture, offering better performance and speed tradeoffs compared to previous versions.\n",
            "3. [YOLOv8](./yolov8.md): The latest version of the YOLO family, featuring enhanced capabilities such as instance segmentation, pose/keypoints estimation, and classification.\n",
            "4. [Segment Anything Model (SAM)](./sam.md): Meta's Segment Anything Model (SAM).\n",
            "\n",
            "You can use these models directly in the Command Line Interface (CLI) or in a Python environment. Below are examples of how to use the models with CLI and Python:\n",
            "\n",
            "## CLI Example\n",
            "\n",
            "```bash\n",
            "yolo task=detect mode=train model=yolov8n.yaml data=coco128.yaml epochs=100\n",
            "```\n",
            "\n",
            "## Python Example\n",
            "\n",
            "```python\n",
            "from ultralytics import YOLO\n",
            "\n",
            "model = YOLO(\"model.yaml\")  # build a YOLOv8n model from scratch\n",
            "# YOLO(\"model.pt\")  use pre-trained model if available\n",
            "model.info()  # display model information\n",
            "model.train(data=\"coco128.yaml\", epochs=100)  # train the model\n",
            "```\n",
            "\n",
            "For more details on each model, their supported tasks, modes, and performance, please visit their respective documentation pages linked above.\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Vision Transformers\n",
            "\n",
            "Vit models currently support Python environment:\n",
            "\n",
            "```python\n",
            "from ultralytics.vit import SAM\n",
            "\n",
            "# from ultralytics.vit import MODEL_TYPe\n",
            "\n",
            "model = SAM(\"sam_b.pt\")\n",
            "model.info()  # display model information\n",
            "model.predict(...)  # train the model\n",
            "```\n",
            "\n",
            "# Segment Anything\n",
            "\n",
            "## About\n",
            "\n",
            "## Supported Tasks\n",
            "\n",
            "| Model Type | Pre-trained Weights | Tasks Supported       |\n",
            "|------------|---------------------|-----------------------|\n",
            "| sam base   | `sam_b.pt`          | Instance Segmentation |\n",
            "| sam large  | `sam_l.pt`          | Instance Segmentation |\n",
            "\n",
            "## Supported Modes\n",
            "\n",
            "| Mode       | Supported          |\n",
            "|------------|--------------------|\n",
            "| Inference  | :heavy_check_mark: |\n",
            "| Validation | :x:                |\n",
            "| Training   | :x:                |\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# 🚧Page Under Construction ⚒\n",
            "\n",
            "This page is currently under construction!️👷Please check back later for updates. 😃🔜\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# YOLOv5u\n",
            "\n",
            "## About\n",
            "\n",
            "Anchor-free YOLOv5 models with improved accuracy-speed tradeoff.\n",
            "\n",
            "## Supported Tasks\n",
            "\n",
            "| Model Type | Pre-trained Weights                                                                                                         | Task      |\n",
            "|------------|-----------------------------------------------------------------------------------------------------------------------------|-----------|\n",
            "| YOLOv5u    | `yolov5nu`, `yolov5su`, `yolov5mu`, `yolov5lu`, `yolov5xu`, `yolov5n6u`, `yolov5s6u`, `yolov5m6u`, `yolov5l6u`, `yolov5x6u` | Detection |\n",
            "\n",
            "## Supported Modes\n",
            "\n",
            "| Mode       | Supported          |\n",
            "|------------|--------------------|\n",
            "| Inference  | :heavy_check_mark: |\n",
            "| Validation | :heavy_check_mark: |\n",
            "| Training   | :heavy_check_mark: |\n",
            "\n",
            "??? Performance\n",
            "\n",
            "    === \"Detection\"\n",
            "\n",
            "        | Model                                                                                    | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "        | ---------------------------------------------------------------------------------------- | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n",
            "        | [YOLOv5nu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5nu.pt)   | 640                   | 34.3                 | 73.6                           | 1.06                                | 2.6                | 7.7               |\n",
            "        | [YOLOv5su](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5su.pt)   | 640                   | 43.0                 | 120.7                          | 1.27                                | 9.1                | 24.0              |\n",
            "        | [YOLOv5mu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5mu.pt)   | 640                   | 49.0                 | 233.9                          | 1.86                                | 25.1               | 64.2              |\n",
            "        | [YOLOv5lu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5lu.pt)   | 640                   | 52.2                 | 408.4                          | 2.50                                | 53.2               | 135.0             |\n",
            "        | [YOLOv5xu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5xu.pt)   | 640                   | 53.2                 | 763.2                          | 3.81                                | 97.2               | 246.4             |\n",
            "        |                                                                                          |                       |                      |                                |                                     |                    |                   |\n",
            "        | [YOLOv5n6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5n6u.pt) | 1280                  | 42.1                 | -                              | -                                   | 4.3                | 7.8               |\n",
            "        | [YOLOv5s6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5s6u.pt) | 1280                  | 48.6                 | -                              | -                                   | 15.3               | 24.6              |\n",
            "        | [YOLOv5m6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5m6u.pt) | 1280                  | 53.6                 | -                              | -                                   | 41.2               | 65.7              |\n",
            "        | [YOLOv5l6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5l6u.pt) | 1280                  | 55.7                 | -                              | -                                   | 86.1               | 137.4             |\n",
            "        | [YOLOv5x6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5x6u.pt) | 1280                  | 56.8                 | -                              | -                                   | 155.4              | 250.7             |\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# YOLOv8\n",
            "\n",
            "## About\n",
            "\n",
            "## Supported Tasks\n",
            "\n",
            "| Model Type  | Pre-trained Weights                                                                                              | Task                  |\n",
            "|-------------|------------------------------------------------------------------------------------------------------------------|-----------------------|\n",
            "| YOLOv8      | `yolov8n.pt`, `yolov8s.pt`, `yolov8m.pt`, `yolov8l.pt`, `yolov8x.pt`                                             | Detection             |\n",
            "| YOLOv8-seg  | `yolov8n-seg.pt`, `yolov8s-seg.pt`, `yolov8m-seg.pt`, `yolov8l-seg.pt`, `yolov8x-seg.pt`                         | Instance Segmentation |\n",
            "| YOLOv8-pose | `yolov8n-pose.pt`, `yolov8s-pose.pt`, `yolov8m-pose.pt`, `yolov8l-pose.pt`, `yolov8x-pose.pt` ,`yolov8x-pose-p6` | Pose/Keypoints        |\n",
            "| YOLOv8-cls  | `yolov8n-cls.pt`, `yolov8s-cls.pt`, `yolov8m-cls.pt`, `yolov8l-cls.pt`, `yolov8x-cls.pt`                         | Classification        |\n",
            "\n",
            "## Supported Modes\n",
            "\n",
            "| Mode       | Supported          |\n",
            "|------------|--------------------|\n",
            "| Inference  | :heavy_check_mark: |\n",
            "| Validation | :heavy_check_mark: |\n",
            "| Training   | :heavy_check_mark: |\n",
            "\n",
            "??? Performance\n",
            "\n",
            "    === \"Detection\"\n",
            "\n",
            "        | Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "        | ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n",
            "        | [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640                   | 37.3                 | 80.4                           | 0.99                                | 3.2                | 8.7               |\n",
            "        | [YOLOv8s](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt) | 640                   | 44.9                 | 128.4                          | 1.20                                | 11.2               | 28.6              |\n",
            "        | [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640                   | 50.2                 | 234.7                          | 1.83                                | 25.9               | 78.9              |\n",
            "        | [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640                   | 52.9                 | 375.2                          | 2.39                                | 43.7               | 165.2             |\n",
            "        | [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640                   | 53.9                 | 479.1                          | 3.53                                | 68.2               | 257.8             |\n",
            "\n",
            "    === \"Segmentation\"\n",
            "\n",
            "        | Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "        | -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n",
            "        | [YOLOv8n-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt) | 640                   | 36.7                 | 30.5                  | 96.1                           | 1.21                                | 3.4                | 12.6              |\n",
            "        | [YOLOv8s-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-seg.pt) | 640                   | 44.6                 | 36.8                  | 155.7                          | 1.47                                | 11.8               | 42.6              |\n",
            "        | [YOLOv8m-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-seg.pt) | 640                   | 49.9                 | 40.8                  | 317.0                          | 2.18                                | 27.3               | 110.2             |\n",
            "        | [YOLOv8l-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-seg.pt) | 640                   | 52.3                 | 42.6                  | 572.4                          | 2.79                                | 46.0               | 220.5             |\n",
            "        | [YOLOv8x-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt) | 640                   | 53.4                 | 43.4                  | 712.1                          | 4.02                                | 71.8               | 344.1             |\n",
            "\n",
            "    === \"Classification\"\n",
            "\n",
            "        | Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 640 |\n",
            "        | -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\n",
            "        | [YOLOv8n-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-cls.pt) | 224                   | 66.6             | 87.0             | 12.9                           | 0.31                                | 2.7                | 4.3                      |\n",
            "        | [YOLOv8s-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-cls.pt) | 224                   | 72.3             | 91.1             | 23.4                           | 0.35                                | 6.4                | 13.5                     |\n",
            "        | [YOLOv8m-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-cls.pt) | 224                   | 76.4             | 93.2             | 85.4                           | 0.62                                | 17.0               | 42.7                     |\n",
            "        | [YOLOv8l-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-cls.pt) | 224                   | 78.0             | 94.1             | 163.0                          | 0.87                                | 37.5               | 99.7                     |\n",
            "        | [YOLOv8x-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt) | 224                   | 78.4             | 94.3             | 232.0                          | 1.01                                | 57.4               | 154.8                    |\n",
            "\n",
            "    === \"Pose\"\n",
            "\n",
            "        | Model                                                                                                | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "        | ---------------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n",
            "        | [YOLOv8n-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt)       | 640                   | 50.4                  | 80.1               | 131.8                          | 1.18                                | 3.3                | 9.2               |\n",
            "        | [YOLOv8s-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt)       | 640                   | 60.0                  | 86.2               | 233.2                          | 1.42                                | 11.6               | 30.2              |\n",
            "        | [YOLOv8m-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-pose.pt)       | 640                   | 65.0                  | 88.8               | 456.3                          | 2.00                                | 26.4               | 81.0              |\n",
            "        | [YOLOv8l-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-pose.pt)       | 640                   | 67.6                  | 90.0               | 784.5                          | 2.59                                | 44.4               | 168.6             |\n",
            "        | [YOLOv8x-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose.pt)       | 640                   | 69.2                  | 90.2               | 1607.1                         | 3.73                                | 69.4               | 263.2             |\n",
            "        | [YOLOv8x-pose-p6](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose-p6.pt) | 1280                  | 71.6                  | 91.2               | 4088.7                         | 10.04                               | 99.1               | 1066.4            |\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in docids:\n",
        "  print(data[i].page_content)"
      ],
      "metadata": {
        "id": "QL4oCHFPEHfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a100a06-ee1d-41c6-d111-790e342ef788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "## Callbacks\n",
            "\n",
            "Ultralytics framework supports callbacks as entry points in strategic stages of train, val, export, and predict modes.\n",
            "Each callback accepts a `Trainer`, `Validator`, or `Predictor` object depending on the operation type. All properties of\n",
            "these objects can be found in Reference section of the docs.\n",
            "\n",
            "## Examples\n",
            "\n",
            "### Returning additional information with Prediction\n",
            "\n",
            "In this example, we want to return the original frame with each result object. Here's how we can do that\n",
            "\n",
            "```python\n",
            "def on_predict_batch_end(predictor):\n",
            "    # Retrieve the batch data\n",
            "    _, im0s, _, _ = predictor.batch\n",
            "    \n",
            "    # Ensure that im0s is a list\n",
            "    im0s = im0s if isinstance(im0s, list) else [im0s]\n",
            "    \n",
            "    # Combine the prediction results with the corresponding frames\n",
            "    predictor.results = zip(predictor.results, im0s)\n",
            "\n",
            "# Create a YOLO model instance\n",
            "model = YOLO(f'yolov8n.pt')\n",
            "\n",
            "# Add the custom callback to the model\n",
            "model.add_callback(\"on_predict_batch_end\", on_predict_batch_end)\n",
            "\n",
            "# Iterate through the results and frames\n",
            "for (result, frame) in model.track/predict():\n",
            "    pass\n",
            "```\n",
            "\n",
            "## All callbacks\n",
            "\n",
            "Here are all supported callbacks. See callbacks [source code](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/utils/callbacks/base.py) for additional details.\n",
            "\n",
            "\n",
            "### Trainer Callbacks\n",
            "\n",
            "| Callback                    | Description                                             |\n",
            "|-----------------------------|---------------------------------------------------------|\n",
            "| `on_pretrain_routine_start` | Triggered at the beginning of pre-training routine      |\n",
            "| `on_pretrain_routine_end`   | Triggered at the end of pre-training routine            |\n",
            "| `on_train_start`            | Triggered when the training starts                      |\n",
            "| `on_train_epoch_start`      | Triggered at the start of each training epoch           |\n",
            "| `on_train_batch_start`      | Triggered at the start of each training batch           |\n",
            "| `optimizer_step`            | Triggered during the optimizer step                     |\n",
            "| `on_before_zero_grad`       | Triggered before gradients are zeroed                   |\n",
            "| `on_train_batch_end`        | Triggered at the end of each training batch             |\n",
            "| `on_train_epoch_end`        | Triggered at the end of each training epoch             |\n",
            "| `on_fit_epoch_end`          | Triggered at the end of each fit epoch                  |\n",
            "| `on_model_save`             | Triggered when the model is saved                       |\n",
            "| `on_train_end`              | Triggered when the training process ends                |\n",
            "| `on_params_update`          | Triggered when model parameters are updated             |\n",
            "| `teardown`                  | Triggered when the training process is being cleaned up |\n",
            "\n",
            "\n",
            "### Validator Callbacks\n",
            "\n",
            "| Callback             | Description                                     |\n",
            "|----------------------|-------------------------------------------------|\n",
            "| `on_val_start`       | Triggered when the validation starts            |\n",
            "| `on_val_batch_start` | Triggered at the start of each validation batch |\n",
            "| `on_val_batch_end`   | Triggered at the end of each validation batch   |\n",
            "| `on_val_end`         | Triggered when the validation ends              |\n",
            "\n",
            "\n",
            "### Predictor Callbacks\n",
            "\n",
            "| Callback                     | Description                                       |\n",
            "|------------------------------|---------------------------------------------------|\n",
            "| `on_predict_start`           | Triggered when the prediction process starts      |\n",
            "| `on_predict_batch_start`     | Triggered at the start of each prediction batch   |\n",
            "| `on_predict_postprocess_end` | Triggered at the end of prediction postprocessing |\n",
            "| `on_predict_batch_end`       | Triggered at the end of each prediction batch     |\n",
            "| `on_predict_end`             | Triggered when the prediction process ends        |\n",
            "\n",
            "### Exporter Callbacks\n",
            "\n",
            "| Callback          | Description                              |\n",
            "|-------------------|------------------------------------------|\n",
            "| `on_export_start` | Triggered when the export process starts |\n",
            "| `on_export_end`   | Triggered when the export process ends   |\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "YOLO settings and hyperparameters play a critical role in the model's performance, speed, and accuracy. These settings\n",
            "and hyperparameters can affect the model's behavior at various stages of the model development process, including\n",
            "training, validation, and prediction.\n",
            "\n",
            "YOLOv8 'yolo' CLI commands use the following syntax:\n",
            "\n",
            "!!! example \"\"\n",
            "\n",
            "    === \"CLI\"\n",
            "    \n",
            "        ```bash\n",
            "        yolo TASK MODE ARGS\n",
            "        ```\n",
            "\n",
            "    === \"Python\"\n",
            "    \n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "        \n",
            "        # Load a YOLOv8 model from a pre-trained weights file\n",
            "        model = YOLO('yolov8n.pt')\n",
            "         \n",
            "        # Run MODE mode using the custom arguments ARGS (guess TASK)\n",
            "        model.MODE(ARGS)\n",
            "        ```\n",
            "\n",
            "Where:\n",
            "\n",
            "- `TASK` (optional) is one of `[detect, segment, classify, pose]`. If it is not passed explicitly YOLOv8 will try to\n",
            "  guess\n",
            "  the `TASK` from the model type.\n",
            "- `MODE` (required) is one of `[train, val, predict, export, track, benchmark]`\n",
            "- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults.\n",
            "  For a full list of available `ARGS` see the [Configuration](cfg.md) page and `defaults.yaml`\n",
            "  GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/cfg/default.yaml).\n",
            "\n",
            "#### Tasks\n",
            "\n",
            "YOLO models can be used for a variety of tasks, including detection, segmentation, classification and pose. These tasks\n",
            "differ in the type of output they produce and the specific problem they are designed to solve.\n",
            "\n",
            "**Detect**: For identifying and localizing objects or regions of interest in an image or video.  \n",
            "**Segment**: For dividing an image or video into regions or pixels that correspond to different objects or classes.  \n",
            "**Classify**: For predicting the class label of an input image.  \n",
            "**Pose**: For identifying objects and estimating their keypoints in an image or video.\n",
            "\n",
            "| Key    | Value      | Description                                     |\n",
            "|--------|------------|-------------------------------------------------|\n",
            "| `task` | `'detect'` | YOLO task, i.e. detect, segment, classify, pose |\n",
            "\n",
            "[Tasks Guide](../tasks/index.md){ .md-button .md-button--primary}\n",
            "\n",
            "#### Modes\n",
            "\n",
            "YOLO models can be used in different modes depending on the specific problem you are trying to solve. These modes\n",
            "include:\n",
            "\n",
            "**Train**: For training a YOLOv8 model on a custom dataset.  \n",
            "**Val**: For validating a YOLOv8 model after it has been trained.  \n",
            "**Predict**: For making predictions using a trained YOLOv8 model on new images or videos.  \n",
            "**Export**: For exporting a YOLOv8 model to a format that can be used for deployment.  \n",
            "**Track**: For tracking objects in real-time using a YOLOv8 model.  \n",
            "**Benchmark**: For benchmarking YOLOv8 exports (ONNX, TensorRT, etc.) speed and accuracy.\n",
            "\n",
            "| Key    | Value     | Description                                                   |\n",
            "|--------|-----------|---------------------------------------------------------------|\n",
            "| `mode` | `'train'` | YOLO mode, i.e. train, val, predict, export, track, benchmark |\n",
            "\n",
            "[Modes Guide](../modes/index.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Train\n",
            "\n",
            "The training settings for YOLO models encompass various hyperparameters and configurations used during the training process. These settings influence the model's performance, speed, and accuracy. Key training settings include batch size, learning rate, momentum, and weight decay. Additionally, the choice of optimizer, loss function, and training dataset composition can impact the training process. Careful tuning and experimentation with these settings are crucial for optimizing performance.\n",
            "\n",
            "| Key               | Value    | Description                                                                 |\n",
            "|-------------------|----------|-----------------------------------------------------------------------------|\n",
            "| `model`           | `None`   | path to model file, i.e. yolov8n.pt, yolov8n.yaml                           |\n",
            "| `data`            | `None`   | path to data file, i.e. coco128.yaml                                        |\n",
            "| `epochs`          | `100`    | number of epochs to train for                                               |\n",
            "| `patience`        | `50`     | epochs to wait for no observable improvement for early stopping of training |\n",
            "| `batch`           | `16`     | number of images per batch (-1 for AutoBatch)                               |\n",
            "| `imgsz`           | `640`    | size of input images as integer or w,h                                      |\n",
            "| `save`            | `True`   | save train checkpoints and predict results                                  |\n",
            "| `save_period`     | `-1`     | Save checkpoint every x epochs (disabled if < 1)                            |\n",
            "| `cache`           | `False`  | True/ram, disk or False. Use cache for data loading                         |\n",
            "| `device`          | `None`   | device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu        |\n",
            "| `workers`         | `8`      | number of worker threads for data loading (per RANK if DDP)                 |\n",
            "| `project`         | `None`   | project name                                                                |\n",
            "| `name`            | `None`   | experiment name                                                             |\n",
            "| `exist_ok`        | `False`  | whether to overwrite existing experiment                                    |\n",
            "| `pretrained`      | `False`  | whether to use a pretrained model                                           |\n",
            "| `optimizer`       | `'SGD'`  | optimizer to use, choices=['SGD', 'Adam', 'AdamW', 'RMSProp']               |\n",
            "| `verbose`         | `False`  | whether to print verbose output                                             |\n",
            "| `seed`            | `0`      | random seed for reproducibility                                             |\n",
            "| `deterministic`   | `True`   | whether to enable deterministic mode                                        |\n",
            "| `single_cls`      | `False`  | train multi-class data as single-class                                      |\n",
            "| `rect`            | `False`  | rectangular training with each batch collated for minimum padding           |\n",
            "| `cos_lr`          | `False`  | use cosine learning rate scheduler                                          |\n",
            "| `close_mosaic`    | `0`      | (int) disable mosaic augmentation for final epochs                          |\n",
            "| `resume`          | `False`  | resume training from last checkpoint                                        |\n",
            "| `amp`             | `True`   | Automatic Mixed Precision (AMP) training, choices=[True, False]             |\n",
            "| `lr0`             | `0.01`   | initial learning rate (i.e. SGD=1E-2, Adam=1E-3)                            |\n",
            "| `lrf`             | `0.01`   | final learning rate (lr0 * lrf)                                             |\n",
            "| `momentum`        | `0.937`  | SGD momentum/Adam beta1                                                     |\n",
            "| `weight_decay`    | `0.0005` | optimizer weight decay 5e-4                                                 |\n",
            "| `warmup_epochs`   | `3.0`    | warmup epochs (fractions ok)                                                |\n",
            "| `warmup_momentum` | `0.8`    | warmup initial momentum                                                     |\n",
            "| `warmup_bias_lr`  | `0.1`    | warmup initial bias lr                                                      |\n",
            "| `box`             | `7.5`    | box loss gain                                                               |\n",
            "| `cls`             | `0.5`    | cls loss gain (scale with pixels)                                           |\n",
            "| `dfl`             | `1.5`    | dfl loss gain                                                               |\n",
            "| `pose`            | `12.0`   | pose loss gain (pose-only)                                                  |\n",
            "| `kobj`            | `2.0`    | keypoint obj loss gain (pose-only)                                          |\n",
            "| `label_smoothing` | `0.0`    | label smoothing (fraction)                                                  |\n",
            "| `nbs`             | `64`     | nominal batch size                                                          |\n",
            "| `overlap_mask`    | `True`   | masks should overlap during training (segment train only)                   |\n",
            "| `mask_ratio`      | `4`      | mask downsample ratio (segment train only)                                  |\n",
            "| `dropout`         | `0.0`    | use dropout regularization (classify train only)                            |\n",
            "| `val`             | `True`   | validate/test during training                                               |\n",
            "\n",
            "[Train Guide](../modes/train.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Predict\n",
            "\n",
            "The prediction settings for YOLO models encompass a range of hyperparameters and configurations that influence the model's performance, speed, and accuracy during inference on new data. Careful tuning and experimentation with these settings are essential to achieve optimal performance for a specific task. Key settings include the confidence threshold, Non-Maximum Suppression (NMS) threshold, and the number of classes considered. Additional factors affecting the prediction process are input data size and format, the presence of supplementary features such as masks or multiple labels per box, and the particular task the model is employed for.\n",
            "\n",
            "| Key              | Value                  | Description                                              |\n",
            "|------------------|------------------------|----------------------------------------------------------|\n",
            "| `source`         | `'ultralytics/assets'` | source directory for images or videos                    |\n",
            "| `conf`           | `0.25`                 | object confidence threshold for detection                |\n",
            "| `iou`            | `0.7`                  | intersection over union (IoU) threshold for NMS          |\n",
            "| `half`           | `False`                | use half precision (FP16)                                |\n",
            "| `device`         | `None`                 | device to run on, i.e. cuda device=0/1/2/3 or device=cpu |\n",
            "| `show`           | `False`                | show results if possible                                 |\n",
            "| `save`           | `False`                | save images with results                                 |\n",
            "| `save_txt`       | `False`                | save results as .txt file                                |\n",
            "| `save_conf`      | `False`                | save results with confidence scores                      |\n",
            "| `save_crop`      | `False`                | save cropped images with results                         |\n",
            "| `show_labels`    | `True`                 | show object labels in plots                              |\n",
            "| `show_conf`      | `True`                 | show object confidence scores in plots                   |\n",
            "| `max_det`        | `300`                  | maximum number of detections per image                   |\n",
            "| `vid_stride`     | `False`                | video frame-rate stride                                  |\n",
            "| `line_thickness` | `3`                    | bounding box thickness (pixels)                          |\n",
            "| `visualize`      | `False`                | visualize model features                                 |\n",
            "| `augment`        | `False`                | apply image augmentation to prediction sources           |\n",
            "| `agnostic_nms`   | `False`                | class-agnostic NMS                                       |\n",
            "| `retina_masks`   | `False`                | use high-resolution segmentation masks                   |\n",
            "| `classes`        | `None`                 | filter results by class, i.e. class=0, or class=[0,2,3]  |\n",
            "| `boxes`          | `True`                 | Show boxes in segmentation predictions                   |\n",
            "\n",
            "[Predict Guide](../modes/predict.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Val\n",
            "\n",
            "The val (validation) settings for YOLO models involve various hyperparameters and configurations used to evaluate the model's performance on a validation dataset. These settings influence the model's performance, speed, and accuracy. Common YOLO validation settings include batch size, validation frequency during training, and performance evaluation metrics. Other factors affecting the validation process include the validation dataset's size and composition, as well as the specific task the model is employed for. Careful tuning and experimentation with these settings are crucial to ensure optimal performance on the validation dataset and detect and prevent overfitting.\n",
            "\n",
            "| Key           | Value   | Description                                                        |\n",
            "|---------------|---------|--------------------------------------------------------------------|\n",
            "| `save_json`   | `False` | save results to JSON file                                          |\n",
            "| `save_hybrid` | `False` | save hybrid version of labels (labels + additional predictions)    |\n",
            "| `conf`        | `0.001` | object confidence threshold for detection                          |\n",
            "| `iou`         | `0.6`   | intersection over union (IoU) threshold for NMS                    |\n",
            "| `max_det`     | `300`   | maximum number of detections per image                             |\n",
            "| `half`        | `True`  | use half precision (FP16)                                          |\n",
            "| `device`      | `None`  | device to run on, i.e. cuda device=0/1/2/3 or device=cpu           |\n",
            "| `dnn`         | `False` | use OpenCV DNN for ONNX inference                                  |\n",
            "| `plots`       | `False` | show plots during training                                         |\n",
            "| `rect`        | `False` | rectangular val with each batch collated for minimum padding       |\n",
            "| `split`       | `val`   | dataset split to use for validation, i.e. 'val', 'test' or 'train' |\n",
            "\n",
            "[Val Guide](../modes/val.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Export\n",
            "\n",
            "Export settings for YOLO models encompass configurations and options related to saving or exporting the model for use in different environments or platforms. These settings can impact the model's performance, size, and compatibility with various systems. Key export settings include the exported model file format (e.g., ONNX, TensorFlow SavedModel), the target device (e.g., CPU, GPU), and additional features such as masks or multiple labels per box. The export process may also be affected by the model's specific task and the requirements or constraints of the destination environment or platform. It is crucial to thoughtfully configure these settings to ensure the exported model is optimized for the intended use case and functions effectively in the target environment.\n",
            "\n",
            "| Key         | Value           | Description                                          |\n",
            "|-------------|-----------------|------------------------------------------------------|\n",
            "| `format`    | `'torchscript'` | format to export to                                  |\n",
            "| `imgsz`     | `640`           | image size as scalar or (h, w) list, i.e. (640, 480) |\n",
            "| `keras`     | `False`         | use Keras for TF SavedModel export                   |\n",
            "| `optimize`  | `False`         | TorchScript: optimize for mobile                     |\n",
            "| `half`      | `False`         | FP16 quantization                                    |\n",
            "| `int8`      | `False`         | INT8 quantization                                    |\n",
            "| `dynamic`   | `False`         | ONNX/TF/TensorRT: dynamic axes                       |\n",
            "| `simplify`  | `False`         | ONNX: simplify model                                 |\n",
            "| `opset`     | `None`          | ONNX: opset version (optional, defaults to latest)   |\n",
            "| `workspace` | `4`             | TensorRT: workspace size (GB)                        |\n",
            "| `nms`       | `False`         | CoreML: add NMS                                      |\n",
            "\n",
            "[Export Guide](../modes/export.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Augmentation\n",
            "\n",
            "Augmentation settings for YOLO models refer to the various transformations and modifications\n",
            "applied to the training data to increase the diversity and size of the dataset. These settings can affect the model's\n",
            "performance, speed, and accuracy. Some common YOLO augmentation settings include the type and intensity of the\n",
            "transformations applied (e.g. random flips, rotations, cropping, color changes), the probability with which each\n",
            "transformation is applied, and the presence of additional features such as masks or multiple labels per box. Other\n",
            "factors that may affect the augmentation process include the size and composition of the original dataset and the\n",
            "specific task the model is being used for. It is important to carefully tune and experiment with these settings to\n",
            "ensure that the augmented dataset is diverse and representative enough to train a high-performing model.\n",
            "\n",
            "| Key           | Value | Description                                     |\n",
            "|---------------|-------|-------------------------------------------------|\n",
            "| `hsv_h`       | 0.015 | image HSV-Hue augmentation (fraction)           |\n",
            "| `hsv_s`       | 0.7   | image HSV-Saturation augmentation (fraction)    |\n",
            "| `hsv_v`       | 0.4   | image HSV-Value augmentation (fraction)         |\n",
            "| `degrees`     | 0.0   | image rotation (+/- deg)                        |\n",
            "| `translate`   | 0.1   | image translation (+/- fraction)                |\n",
            "| `scale`       | 0.5   | image scale (+/- gain)                          |\n",
            "| `shear`       | 0.0   | image shear (+/- deg)                           |\n",
            "| `perspective` | 0.0   | image perspective (+/- fraction), range 0-0.001 |\n",
            "| `flipud`      | 0.0   | image flip up-down (probability)                |\n",
            "| `fliplr`      | 0.5   | image flip left-right (probability)             |\n",
            "| `mosaic`      | 1.0   | image mosaic (probability)                      |\n",
            "| `mixup`       | 0.0   | image mixup (probability)                       |\n",
            "| `copy_paste`  | 0.0   | segment copy-paste (probability)                |\n",
            "\n",
            "## Logging, checkpoints, plotting and file management\n",
            "\n",
            "Logging, checkpoints, plotting, and file management are important considerations when training a YOLO model.\n",
            "\n",
            "- Logging: It is often helpful to log various metrics and statistics during training to track the model's progress and\n",
            "  diagnose any issues that may arise. This can be done using a logging library such as TensorBoard or by writing log\n",
            "  messages to a file.\n",
            "- Checkpoints: It is a good practice to save checkpoints of the model at regular intervals during training. This allows\n",
            "  you to resume training from a previous point if the training process is interrupted or if you want to experiment with\n",
            "  different training configurations.\n",
            "- Plotting: Visualizing the model's performance and training progress can be helpful for understanding how the model is\n",
            "  behaving and identifying potential issues. This can be done using a plotting library such as matplotlib or by\n",
            "  generating plots using a logging library such as TensorBoard.\n",
            "- File management: Managing the various files generated during the training process, such as model checkpoints, log\n",
            "  files, and plots, can be challenging. It is important to have a clear and organized file structure to keep track of\n",
            "  these files and make it easy to access and analyze them as needed.\n",
            "\n",
            "Effective logging, checkpointing, plotting, and file management can help you keep track of the model's progress and make\n",
            "it easier to debug and optimize the training process.\n",
            "\n",
            "| Key        | Value    | Description                                                                                    |\n",
            "|------------|----------|------------------------------------------------------------------------------------------------|\n",
            "| `project`  | `'runs'` | project name                                                                                   |\n",
            "| `name`     | `'exp'`  | experiment name. `exp` gets automatically incremented if not specified, i.e, `exp`, `exp2` ... |\n",
            "| `exist_ok` | `False`  | whether to overwrite existing experiment                                                       |\n",
            "| `plots`    | `False`  | save plots during train/val                                                                    |\n",
            "| `save`     | `False`  | save train checkpoints and predict results                                                     |\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Command Line Interface Usage\n",
            "\n",
            "The YOLO command line interface (CLI) allows for simple single-line commands without the need for a Python environment.\n",
            "CLI requires no customization or Python code. You can simply run all tasks from the terminal with the `yolo` command.\n",
            "\n",
            "!!! example\n",
            "\n",
            "    === \"Syntax\"\n",
            "\n",
            "        Ultralytics `yolo` commands use the following syntax:\n",
            "        ```bash\n",
            "        yolo TASK MODE ARGS\n",
            "\n",
            "        Where   TASK (optional) is one of [detect, segment, classify]\n",
            "                MODE (required) is one of [train, val, predict, export, track]\n",
            "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
            "        ```\n",
            "        See all ARGS in the full [Configuration Guide](./cfg.md) or with `yolo cfg`\n",
            "\n",
            "    === \"Train\"\n",
            "\n",
            "        Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
            "        ```bash\n",
            "        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n",
            "        ```\n",
            "\n",
            "    === \"Predict\"\n",
            "\n",
            "        Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
            "        ```bash\n",
            "        yolo predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n",
            "        ```\n",
            "\n",
            "    === \"Val\"\n",
            "\n",
            "        Val a pretrained detection model at batch-size 1 and image size 640:\n",
            "        ```bash\n",
            "        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n",
            "        ```\n",
            "\n",
            "    === \"Export\"\n",
            "\n",
            "        Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
            "        ```bash\n",
            "        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n",
            "        ```\n",
            "\n",
            "    === \"Special\"\n",
            "\n",
            "        Run special commands to see version, view settings, run checks and more:\n",
            "        ```bash\n",
            "        yolo help\n",
            "        yolo checks\n",
            "        yolo version\n",
            "        yolo settings\n",
            "        yolo copy-cfg\n",
            "        yolo cfg\n",
            "        ```\n",
            "\n",
            "Where:\n",
            "\n",
            "- `TASK` (optional) is one of `[detect, segment, classify]`. If it is not passed explicitly YOLOv8 will try to guess\n",
            "  the `TASK` from the model type.\n",
            "- `MODE` (required) is one of `[train, val, predict, export, track]`\n",
            "- `ARGS` (optional) are any number of custom `arg=value` pairs like `imgsz=320` that override defaults.\n",
            "  For a full list of available `ARGS` see the [Configuration](cfg.md) page and `defaults.yaml`\n",
            "  GitHub [source](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/cfg/default.yaml).\n",
            "\n",
            "!!! warning \"Warning\"\n",
            "\n",
            "    Arguments must be passed as `arg=val` pairs, split by an equals `=` sign and delimited by spaces ` ` between pairs. Do not use `--` argument prefixes or commas `,` beteen arguments.\n",
            "\n",
            "    - `yolo predict model=yolov8n.pt imgsz=640 conf=0.25` &nbsp; ✅\n",
            "    - `yolo predict model yolov8n.pt imgsz 640 conf 0.25` &nbsp; ❌\n",
            "    - `yolo predict --model yolov8n.pt --imgsz 640 --conf 0.25` &nbsp; ❌\n",
            "\n",
            "## Train\n",
            "\n",
            "Train YOLOv8n on the COCO128 dataset for 100 epochs at image size 640. For a full list of available arguments see\n",
            "the [Configuration](cfg.md) page.\n",
            "\n",
            "!!! example \"Example\"\n",
            "\n",
            "    === \"Train\"\n",
            "        \n",
            "        Start training YOLOv8n on COCO128 for 100 epochs at image-size 640.\n",
            "        ```bash\n",
            "        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=100 imgsz=640\n",
            "        ```\n",
            "\n",
            "    === \"Resume\"\n",
            "\n",
            "        Resume an interrupted training.\n",
            "        ```bash\n",
            "        yolo detect train resume model=last.pt\n",
            "        ```\n",
            "\n",
            "## Val\n",
            "\n",
            "Validate trained YOLOv8n model accuracy on the COCO128 dataset. No argument need to passed as the `model` retains it's\n",
            "training `data` and arguments as model attributes.\n",
            "\n",
            "!!! example \"Example\"\n",
            "\n",
            "    === \"Official\"\n",
            "\n",
            "        Validate an official YOLOv8n model.\n",
            "        ```bash\n",
            "        yolo detect val model=yolov8n.pt\n",
            "        ```\n",
            "\n",
            "    === \"Custom\"\n",
            "\n",
            "        Validate a custom-trained model.\n",
            "        ```bash\n",
            "        yolo detect val model=path/to/best.pt\n",
            "        ```\n",
            "\n",
            "## Predict\n",
            "\n",
            "Use a trained YOLOv8n model to run predictions on images.\n",
            "\n",
            "!!! example \"Example\"\n",
            "\n",
            "    === \"Official\"\n",
            "\n",
            "        Predict with an official YOLOv8n model.\n",
            "        ```bash\n",
            "        yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg'\n",
            "        ```\n",
            "\n",
            "    === \"Custom\"\n",
            "\n",
            "        Predict with a custom model.\n",
            "        ```bash\n",
            "        yolo detect predict model=path/to/best.pt source='https://ultralytics.com/images/bus.jpg'\n",
            "        ```\n",
            "\n",
            "## Export\n",
            "\n",
            "Export a YOLOv8n model to a different format like ONNX, CoreML, etc.\n",
            "\n",
            "!!! example \"Example\"\n",
            "\n",
            "    === \"Official\"\n",
            "\n",
            "        Export an official YOLOv8n model to ONNX format.\n",
            "        ```bash\n",
            "        yolo export model=yolov8n.pt format=onnx\n",
            "        ```\n",
            "\n",
            "    === \"Custom\"\n",
            "\n",
            "        Export a custom-trained model to ONNX format.\n",
            "        ```bash\n",
            "        yolo export model=path/to/best.pt format=onnx\n",
            "        ```\n",
            "\n",
            "Available YOLOv8 export formats are in the table below. You can export to any format using the `format` argument,\n",
            "i.e. `format='onnx'` or `format='engine'`.\n",
            "\n",
            "| Format                                                             | `format` Argument | Model                     | Metadata |\n",
            "|--------------------------------------------------------------------|-------------------|---------------------------|----------|\n",
            "| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | ✅        |\n",
            "| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | ✅        |\n",
            "| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | ✅        |\n",
            "| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | ✅        |\n",
            "| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | ✅        |\n",
            "| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | ✅        |\n",
            "| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | ✅        |\n",
            "| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | ❌        |\n",
            "| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | ✅        |\n",
            "| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ✅        |\n",
            "| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | ✅        |\n",
            "| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | ✅        |\n",
            "\n",
            "---\n",
            "\n",
            "## Overriding default arguments\n",
            "\n",
            "Default arguments can be overridden by simply passing them as arguments in the CLI in `arg=value` pairs.\n",
            "\n",
            "!!! tip \"\"\n",
            "\n",
            "    === \"Train\"\n",
            "        Train a detection model for `10 epochs` with `learning_rate` of `0.01`\n",
            "        ```bash\n",
            "        yolo detect train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n",
            "        ```\n",
            "\n",
            "    === \"Predict\"\n",
            "        Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
            "        ```bash\n",
            "        yolo segment predict model=yolov8n-seg.pt source='https://youtu.be/Zgi9g1ksQHc' imgsz=320\n",
            "        ```\n",
            "\n",
            "    === \"Val\"\n",
            "        Validate a pretrained detection model at batch-size 1 and image size 640:\n",
            "        ```bash\n",
            "        yolo detect val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n",
            "        ```\n",
            "\n",
            "---\n",
            "\n",
            "## Overriding default config file\n",
            "\n",
            "You can override the `default.yaml` config file entirely by passing a new file with the `cfg` arguments,\n",
            "i.e. `cfg=custom.yaml`.\n",
            "\n",
            "To do this first create a copy of `default.yaml` in your current working dir with the `yolo copy-cfg` command.\n",
            "\n",
            "This will create `default_copy.yaml`, which you can then pass as `cfg=default_copy.yaml` along with any additional args,\n",
            "like `imgsz=320` in this example:\n",
            "\n",
            "!!! example \"\"\n",
            "\n",
            "    === \"CLI\"\n",
            "        ```bash\n",
            "        yolo copy-cfg\n",
            "        yolo cfg=default_copy.yaml imgsz=320\n",
            "        ```\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "Both the Ultralytics YOLO command-line and python interfaces are simply a high-level abstraction on the base engine\n",
            "executors. Let's take a look at the Trainer engine.\n",
            "\n",
            "## BaseTrainer\n",
            "\n",
            "BaseTrainer contains the generic boilerplate training routine. It can be customized for any task based over overriding\n",
            "the required functions or operations as long the as correct formats are followed. For example, you can support your own\n",
            "custom model and dataloader by just overriding these functions:\n",
            "\n",
            "* `get_model(cfg, weights)` - The function that builds the model to be trained\n",
            "* `get_dataloder()` - The function that builds the dataloader\n",
            "  More details and source code can be found in [`BaseTrainer` Reference](../reference/yolo/engine/trainer.md)\n",
            "\n",
            "## DetectionTrainer\n",
            "\n",
            "Here's how you can use the YOLOv8 `DetectionTrainer` and customize it.\n",
            "\n",
            "```python\n",
            "from ultralytics.yolo.v8.detect import DetectionTrainer\n",
            "\n",
            "trainer = DetectionTrainer(overrides={...})\n",
            "trainer.train()\n",
            "trained_model = trainer.best  # get best model\n",
            "```\n",
            "\n",
            "### Customizing the DetectionTrainer\n",
            "\n",
            "Let's customize the trainer **to train a custom detection model** that is not supported directly. You can do this by\n",
            "simply overloading the existing the `get_model` functionality:\n",
            "\n",
            "```python\n",
            "from ultralytics.yolo.v8.detect import DetectionTrainer\n",
            "\n",
            "\n",
            "class CustomTrainer(DetectionTrainer):\n",
            "    def get_model(self, cfg, weights):\n",
            "        ...\n",
            "\n",
            "\n",
            "trainer = CustomTrainer(overrides={...})\n",
            "trainer.train()\n",
            "```\n",
            "\n",
            "You now realize that you need to customize the trainer further to:\n",
            "\n",
            "* Customize the `loss function`.\n",
            "* Add `callback` that uploads model to your Google Drive after every 10 `epochs`\n",
            "  Here's how you can do it:\n",
            "\n",
            "```python\n",
            "from ultralytics.yolo.v8.detect import DetectionTrainer\n",
            "\n",
            "\n",
            "class CustomTrainer(DetectionTrainer):\n",
            "    def get_model(self, cfg, weights):\n",
            "        ...\n",
            "\n",
            "    def criterion(self, preds, batch):\n",
            "        # get ground truth\n",
            "        imgs = batch[\"imgs\"]\n",
            "        bboxes = batch[\"bboxes\"]\n",
            "        ...\n",
            "        return loss, loss_items  # see Reference-> Trainer for details on the expected format\n",
            "\n",
            "\n",
            "# callback to upload model weights\n",
            "def log_model(trainer):\n",
            "    last_weight_path = trainer.last\n",
            "    ...\n",
            "\n",
            "\n",
            "trainer = CustomTrainer(overrides={...})\n",
            "trainer.add_callback(\"on_train_epoch_end\", log_model)  # Adds to existing callback\n",
            "trainer.train()\n",
            "```\n",
            "\n",
            "To know more about Callback triggering events and entry point, checkout our [Callbacks Guide](callbacks.md)\n",
            "\n",
            "## Other engine components\n",
            "\n",
            "There are other components that can be customized similarly like `Validators` and `Predictors`\n",
            "See Reference section for more information on these.\n",
            "\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Hyperparameter Tuning with Ray Tune and YOLOv8\n",
            "\n",
            "Hyperparameter tuning (or hyperparameter optimization) is the process of determining the right combination of hyperparameters that maximizes model performance. It works by running multiple trials in a single training process, evaluating the performance of each trial, and selecting the best hyperparameter values based on the evaluation results.\n",
            "\n",
            "## Ultralytics YOLOv8 and Ray Tune Integration\n",
            "\n",
            "[Ultralytics](https://ultralytics.com) YOLOv8 integrates hyperparameter tuning with Ray Tune, allowing you to easily optimize your YOLOv8 model's hyperparameters. By using Ray Tune, you can leverage advanced search algorithms, parallelism, and early stopping to speed up the tuning process and achieve better model performance.\n",
            "\n",
            "###  Ray Tune\n",
            "\n",
            "<div align=\"center\">\n",
            "<a href=\"https://docs.ray.io/en/latest/tune/index.html\" target=\"_blank\">\n",
            "<img width=\"480\" src=\"https://docs.ray.io/en/latest/_images/tune_overview.png\"></a>\n",
            "</div>\n",
            "\n",
            "[Ray Tune](https://docs.ray.io/en/latest/tune/index.html) is a powerful and flexible hyperparameter tuning library for machine learning models. It provides an efficient way to optimize hyperparameters by supporting various search algorithms, parallelism, and early stopping strategies. Ray Tune's flexible architecture enables seamless integration with popular machine learning frameworks, including Ultralytics YOLOv8.\n",
            "\n",
            "### Weights & Biases\n",
            "\n",
            "YOLOv8 also supports optional integration with [Weights & Biases](https://wandb.ai/site) (wandb) for tracking the tuning progress.\n",
            "\n",
            "## Installation\n",
            "\n",
            "To install the required packages, run:\n",
            "\n",
            "!!! tip \"Installation\"\n",
            "\n",
            "    ```bash\n",
            "    pip install -U ultralytics \"ray[tune]\"  # install and/or update\n",
            "    pip install wandb  # optional\n",
            "    ```\n",
            "\n",
            "## Usage\n",
            "\n",
            "!!! example \"Usage\"\n",
            "\n",
            "    ```python\n",
            "    from ultralytics import YOLO\n",
            "\n",
            "    model = YOLO(\"yolov8n.pt\")\n",
            "    results = model.tune(data=\"coco128.yaml\")\n",
            "    ```\n",
            "\n",
            "## `tune()` Method Parameters\n",
            "\n",
            "The `tune()` method in YOLOv8 provides an easy-to-use interface for hyperparameter tuning with Ray Tune. It accepts several arguments that allow you to customize the tuning process. Below is a detailed explanation of each parameter:\n",
            "\n",
            "| Parameter       | Type           | Description                                                                                                                                                                                                                                                                                                                   | Default Value |\n",
            "|-----------------|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|\n",
            "| `data`          | str            | The dataset configuration file (in YAML format) to run the tuner on. This file should specify the training and validation data paths, as well as other dataset-specific settings.                                                                                                                                             |               |\n",
            "| `space`         | dict, optional | A dictionary defining the hyperparameter search space for Ray Tune. Each key corresponds to a hyperparameter name, and the value specifies the range of values to explore during tuning. If not provided, YOLOv8 uses a default search space with various hyperparameters.                                                    |               |\n",
            "| `grace_period`  | int, optional  | The grace period in epochs for the [ASHA scheduler](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-asha) in Ray Tune. The scheduler will not terminate any trial before this number of epochs, allowing the model to have some minimum training before making a decision on early stopping. | 10            |\n",
            "| `gpu_per_trial` | int, optional  | The number of GPUs to allocate per trial during tuning. This helps manage GPU usage, particularly in multi-GPU environments. If not provided, the tuner will use all available GPUs.                                                                                                                                          | None          |\n",
            "| `max_samples`   | int, optional  | The maximum number of trials to run during tuning. This parameter helps control the total number of hyperparameter combinations tested, ensuring the tuning process does not run indefinitely.                                                                                                                                | 10            |\n",
            "| `train_args`    | dict, optional | A dictionary of additional arguments to pass to the `train()` method during tuning. These arguments can include settings like the number of training epochs, batch size, and other training-specific configurations.                                                                                                          | {}            |\n",
            "\n",
            "By customizing these parameters, you can fine-tune the hyperparameter optimization process to suit your specific needs and available computational resources.\n",
            "\n",
            "## Default Search Space Description\n",
            "\n",
            "The following table lists the default search space parameters for hyperparameter tuning in YOLOv8 with Ray Tune. Each parameter has a specific value range defined by `tune.uniform()`.\n",
            "\n",
            "| Parameter       | Value Range                | Description                              |\n",
            "|-----------------|----------------------------|------------------------------------------|\n",
            "| lr0             | `tune.uniform(1e-5, 1e-1)` | Initial learning rate                    |\n",
            "| lrf             | `tune.uniform(0.01, 1.0)`  | Final learning rate factor               |\n",
            "| momentum        | `tune.uniform(0.6, 0.98)`  | Momentum                                 |\n",
            "| weight_decay    | `tune.uniform(0.0, 0.001)` | Weight decay                             |\n",
            "| warmup_epochs   | `tune.uniform(0.0, 5.0)`   | Warmup epochs                            |\n",
            "| warmup_momentum | `tune.uniform(0.0, 0.95)`  | Warmup momentum                          |\n",
            "| box             | `tune.uniform(0.02, 0.2)`  | Box loss weight                          |\n",
            "| cls             | `tune.uniform(0.2, 4.0)`   | Class loss weight                        |\n",
            "| hsv_h           | `tune.uniform(0.0, 0.1)`   | Hue augmentation range                   |\n",
            "| hsv_s           | `tune.uniform(0.0, 0.9)`   | Saturation augmentation range            |\n",
            "| hsv_v           | `tune.uniform(0.0, 0.9)`   | Value (brightness) augmentation range    |\n",
            "| degrees         | `tune.uniform(0.0, 45.0)`  | Rotation augmentation range (degrees)    |\n",
            "| translate       | `tune.uniform(0.0, 0.9)`   | Translation augmentation range           |\n",
            "| scale           | `tune.uniform(0.0, 0.9)`   | Scaling augmentation range               |\n",
            "| shear           | `tune.uniform(0.0, 10.0)`  | Shear augmentation range (degrees)       |\n",
            "| perspective     | `tune.uniform(0.0, 0.001)` | Perspective augmentation range           |\n",
            "| flipud          | `tune.uniform(0.0, 1.0)`   | Vertical flip augmentation probability   |\n",
            "| fliplr          | `tune.uniform(0.0, 1.0)`   | Horizontal flip augmentation probability |\n",
            "| mosaic          | `tune.uniform(0.0, 1.0)`   | Mosaic augmentation probability          |\n",
            "| mixup           | `tune.uniform(0.0, 1.0)`   | Mixup augmentation probability           |\n",
            "| copy_paste      | `tune.uniform(0.0, 1.0)`   | Copy-paste augmentation probability      |\n",
            "\n",
            "\n",
            "## Custom Search Space Example\n",
            "\n",
            "In this example, we demonstrate how to use a custom search space for hyperparameter tuning with Ray Tune and YOLOv8. By providing a custom search space, you can focus the tuning process on specific hyperparameters of interest.\n",
            "\n",
            "!!! example \"Usage\"\n",
            "\n",
            "    ```python\n",
            "    from ultralytics import YOLO\n",
            "    from ray import tune\n",
            "    \n",
            "    model = YOLO(\"yolov8n.pt\")\n",
            "    result = model.tune(\n",
            "        data=\"coco128.yaml\",\n",
            "        space={\"lr0\": tune.uniform(1e-5, 1e-1)},\n",
            "        train_args={\"epochs\": 50}\n",
            "    )\n",
            "    ```\n",
            "\n",
            "In the code snippet above, we create a YOLO model with the \"yolov8n.pt\" pretrained weights. Then, we call the `tune()` method, specifying the dataset configuration with \"coco128.yaml\". We provide a custom search space for the initial learning rate `lr0` using a dictionary with the key \"lr0\" and the value `tune.uniform(1e-5, 1e-1)`. Finally, we pass additional training arguments, such as the number of epochs, using the `train_args` parameter.\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Python Usage\n",
            "\n",
            "Welcome to the YOLOv8 Python Usage documentation! This guide is designed to help you seamlessly integrate YOLOv8 into\n",
            "your Python projects for object detection, segmentation, and classification. Here, you'll learn how to load and use\n",
            "pretrained models, train new models, and perform predictions on images. The easy-to-use Python interface is a valuable\n",
            "resource for anyone looking to incorporate YOLOv8 into their Python projects, allowing you to quickly implement advanced\n",
            "object detection capabilities. Let's get started!\n",
            "\n",
            "For example, users can load a model, train it, evaluate its performance on a validation set, and even export it to ONNX\n",
            "format with just a few lines of code.\n",
            "\n",
            "!!! example \"Python\"\n",
            "\n",
            "    ```python\n",
            "    from ultralytics import YOLO\n",
            "    \n",
            "    # Create a new YOLO model from scratch\n",
            "    model = YOLO('yolov8n.yaml')\n",
            "    \n",
            "    # Load a pretrained YOLO model (recommended for training)\n",
            "    model = YOLO('yolov8n.pt')\n",
            "    \n",
            "    # Train the model using the 'coco128.yaml' dataset for 3 epochs\n",
            "    results = model.train(data='coco128.yaml', epochs=3)\n",
            "    \n",
            "    # Evaluate the model's performance on the validation set\n",
            "    results = model.val()\n",
            "    \n",
            "    # Perform object detection on an image using the model\n",
            "    results = model('https://ultralytics.com/images/bus.jpg')\n",
            "    \n",
            "    # Export the model to ONNX format\n",
            "    success = model.export(format='onnx')\n",
            "    ```\n",
            "\n",
            "## [Train](../modes/train.md)\n",
            "\n",
            "Train mode is used for training a YOLOv8 model on a custom dataset. In this mode, the model is trained using the\n",
            "specified dataset and hyperparameters. The training process involves optimizing the model's parameters so that it can\n",
            "accurately predict the classes and locations of objects in an image.\n",
            "\n",
            "!!! example \"Train\"\n",
            "\n",
            "    === \"From pretrained(recommended)\"\n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "\n",
            "        model = YOLO('yolov8n.pt') # pass any model type\n",
            "        model.train(epochs=5)\n",
            "        ```\n",
            "\n",
            "    === \"From scratch\"\n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "\n",
            "        model = YOLO('yolov8n.yaml')\n",
            "        model.train(data='coco128.yaml', epochs=5)\n",
            "        ```\n",
            "\n",
            "    === \"Resume\"\n",
            "        ```python\n",
            "        model = YOLO(\"last.pt\")\n",
            "        model.train(resume=True)\n",
            "        ```\n",
            "\n",
            "[Train Examples](../modes/train.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Val](../modes/val.md)\n",
            "\n",
            "Val mode is used for validating a YOLOv8 model after it has been trained. In this mode, the model is evaluated on a\n",
            "validation set to measure its accuracy and generalization performance. This mode can be used to tune the hyperparameters\n",
            "of the model to improve its performance.\n",
            "\n",
            "!!! example \"Val\"\n",
            "\n",
            "    === \"Val after training\"\n",
            "        ```python\n",
            "          from ultralytics import YOLO\n",
            "\n",
            "          model = YOLO('yolov8n.yaml')\n",
            "          model.train(data='coco128.yaml', epochs=5)\n",
            "          model.val()  # It'll automatically evaluate the data you trained.\n",
            "        ```\n",
            "\n",
            "    === \"Val independently\"\n",
            "        ```python\n",
            "          from ultralytics import YOLO\n",
            "\n",
            "          model = YOLO(\"model.pt\")\n",
            "          # It'll use the data yaml file in model.pt if you don't set data.\n",
            "          model.val()\n",
            "          # or you can set the data you want to val\n",
            "          model.val(data='coco128.yaml')\n",
            "        ```\n",
            "\n",
            "[Val Examples](../modes/val.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Predict](../modes/predict.md)\n",
            "\n",
            "Predict mode is used for making predictions using a trained YOLOv8 model on new images or videos. In this mode, the\n",
            "model is loaded from a checkpoint file, and the user can provide images or videos to perform inference. The model\n",
            "predicts the classes and locations of objects in the input images or videos.\n",
            "\n",
            "!!! example \"Predict\"\n",
            "\n",
            "    === \"From source\"\n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "        from PIL import Image\n",
            "        import cv2\n",
            "\n",
            "        model = YOLO(\"model.pt\")\n",
            "        # accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\n",
            "        results = model.predict(source=\"0\")\n",
            "        results = model.predict(source=\"folder\", show=True) # Display preds. Accepts all YOLO predict arguments\n",
            "\n",
            "        # from PIL\n",
            "        im1 = Image.open(\"bus.jpg\")\n",
            "        results = model.predict(source=im1, save=True)  # save plotted images\n",
            "\n",
            "        # from ndarray\n",
            "        im2 = cv2.imread(\"bus.jpg\")\n",
            "        results = model.predict(source=im2, save=True, save_txt=True)  # save predictions as labels\n",
            "\n",
            "        # from list of PIL/ndarray\n",
            "        results = model.predict(source=[im1, im2])\n",
            "        ```\n",
            "\n",
            "    === \"Results usage\"\n",
            "        ```python\n",
            "        # results would be a list of Results object including all the predictions by default\n",
            "        # but be careful as it could occupy a lot memory when there're many images, \n",
            "        # especially the task is segmentation.\n",
            "        # 1. return as a list\n",
            "        results = model.predict(source=\"folder\")\n",
            "\n",
            "        # results would be a generator which is more friendly to memory by setting stream=True\n",
            "        # 2. return as a generator\n",
            "        results = model.predict(source=0, stream=True)\n",
            "\n",
            "        for result in results:\n",
            "            # Detection\n",
            "            result.boxes.xyxy   # box with xyxy format, (N, 4)\n",
            "            result.boxes.xywh   # box with xywh format, (N, 4)\n",
            "            result.boxes.xyxyn  # box with xyxy format but normalized, (N, 4)\n",
            "            result.boxes.xywhn  # box with xywh format but normalized, (N, 4)\n",
            "            result.boxes.conf   # confidence score, (N, 1)\n",
            "            result.boxes.cls    # cls, (N, 1)\n",
            "\n",
            "            # Segmentation\n",
            "            result.masks.data      # masks, (N, H, W)\n",
            "            result.masks.xy        # x,y segments (pixels), List[segment] * N\n",
            "            result.masks.xyn       # x,y segments (normalized), List[segment] * N\n",
            "\n",
            "            # Classification\n",
            "            result.probs     # cls prob, (num_class, )\n",
            "\n",
            "        # Each result is composed of torch.Tensor by default, \n",
            "        # in which you can easily use following functionality:\n",
            "        result = result.cuda()\n",
            "        result = result.cpu()\n",
            "        result = result.to(\"cpu\")\n",
            "        result = result.numpy()\n",
            "        ```\n",
            "\n",
            "[Predict Examples](../modes/predict.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Export](../modes/export.md)\n",
            "\n",
            "Export mode is used for exporting a YOLOv8 model to a format that can be used for deployment. In this mode, the model is\n",
            "converted to a format that can be used by other software applications or hardware devices. This mode is useful when\n",
            "deploying the model to production environments.\n",
            "\n",
            "!!! example \"Export\"\n",
            "\n",
            "    === \"Export to ONNX\"\n",
            "\n",
            "        Export an official YOLOv8n model to ONNX with dynamic batch-size and image-size.\n",
            "        ```python\n",
            "          from ultralytics import YOLO\n",
            "\n",
            "          model = YOLO('yolov8n.pt')\n",
            "          model.export(format='onnx', dynamic=True)\n",
            "        ```\n",
            "\n",
            "    === \"Export to TensorRT\"\n",
            "\n",
            "        Export an official YOLOv8n model to TensorRT on `device=0` for acceleration on CUDA devices.\n",
            "        ```python\n",
            "          from ultralytics import YOLO\n",
            "\n",
            "          model = YOLO('yolov8n.pt')\n",
            "          model.export(format='onnx', device=0)\n",
            "        ```\n",
            "\n",
            "[Export Examples](../modes/export.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Track](../modes/track.md)\n",
            "\n",
            "Track mode is used for tracking objects in real-time using a YOLOv8 model. In this mode, the model is loaded from a\n",
            "checkpoint file, and the user can provide a live video stream to perform real-time object tracking. This mode is useful\n",
            "for applications such as surveillance systems or self-driving cars.\n",
            "\n",
            "!!! example \"Track\"\n",
            "\n",
            "    === \"Python\"\n",
            "    \n",
            "        ```python\n",
            "        from ultralytics import YOLO\n",
            "        \n",
            "        # Load a model\n",
            "        model = YOLO('yolov8n.pt')  # load an official detection model\n",
            "        model = YOLO('yolov8n-seg.pt')  # load an official segmentation model\n",
            "        model = YOLO('path/to/best.pt')  # load a custom model\n",
            "        \n",
            "        # Track with the model\n",
            "        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True) \n",
            "        results = model.track(source=\"https://youtu.be/Zgi9g1ksQHc\", show=True, tracker=\"bytetrack.yaml\") \n",
            "        ```\n",
            "\n",
            "[Track Examples](../modes/track.md){ .md-button .md-button--primary}\n",
            "\n",
            "## [Benchmark](../modes/benchmark.md)\n",
            "\n",
            "Benchmark mode is used to profile the speed and accuracy of various export formats for YOLOv8. The benchmarks provide\n",
            "information on the size of the exported format, its `mAP50-95` metrics (for object detection and segmentation)\n",
            "or `accuracy_top5` metrics (for classification), and the inference time in milliseconds per image across various export\n",
            "formats like ONNX, OpenVINO, TensorRT and others. This information can help users choose the optimal export format for\n",
            "their specific use case based on their requirements for speed and accuracy.\n",
            "\n",
            "!!! example \"Benchmark\"\n",
            "\n",
            "    === \"Python\"\n",
            "    \n",
            "        Benchmark an official YOLOv8n model across all export formats.\n",
            "        ```python\n",
            "        from ultralytics.yolo.utils.benchmarks import benchmark\n",
            "        \n",
            "        # Benchmark\n",
            "        benchmark(model='yolov8n.pt', imgsz=640, half=False, device=0)\n",
            "        ```\n",
            "\n",
            "[Benchmark Examples](../modes/benchmark.md){ .md-button .md-button--primary}\n",
            "\n",
            "## Using Trainers\n",
            "\n",
            "`YOLO` model class is a high-level wrapper on the Trainer classes. Each YOLO task has its own trainer that inherits\n",
            "from `BaseTrainer`.\n",
            "\n",
            "!!! tip \"Detection Trainer Example\"\n",
            "\n",
            "        ```python\n",
            "        from ultralytics.yolo import v8 import DetectionTrainer, DetectionValidator, DetectionPredictor\n",
            "\n",
            "        # trainer\n",
            "        trainer = DetectionTrainer(overrides={})\n",
            "        trainer.train()\n",
            "        trained_model = trainer.best\n",
            "\n",
            "        # Validator\n",
            "        val = DetectionValidator(args=...)\n",
            "        val(model=trained_model)\n",
            "\n",
            "        # predictor\n",
            "        pred = DetectionPredictor(overrides={})\n",
            "        pred(source=SOURCE, model=trained_model)\n",
            "\n",
            "        # resume from last weight\n",
            "        overrides[\"resume\"] = trainer.last\n",
            "        trainer = detect.DetectionTrainer(overrides=overrides)\n",
            "        ```\n",
            "\n",
            "You can easily customize Trainers to support custom tasks or explore R&D ideas.\n",
            "Learn more about Customizing `Trainers`, `Validators` and `Predictors` to suit your project needs in the Customization\n",
            "Section.\n",
            "\n",
            "[Customization tutorials](engine.md){ .md-button .md-button--primary}\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Models\n",
            "\n",
            "Ultralytics supports many models and architectures with more to come in the future. Want to add your model architecture? [Here's](../help/contributing.md) how you can contribute.\n",
            "\n",
            "In this documentation, we provide information on four major models:\n",
            "\n",
            "1. [YOLOv3](./yolov3.md): The third iteration of the YOLO model family, known for its efficient real-time object detection capabilities.\n",
            "2. [YOLOv5](./yolov5.md): An improved version of the YOLO architecture, offering better performance and speed tradeoffs compared to previous versions.\n",
            "3. [YOLOv8](./yolov8.md): The latest version of the YOLO family, featuring enhanced capabilities such as instance segmentation, pose/keypoints estimation, and classification.\n",
            "4. [Segment Anything Model (SAM)](./sam.md): Meta's Segment Anything Model (SAM).\n",
            "\n",
            "You can use these models directly in the Command Line Interface (CLI) or in a Python environment. Below are examples of how to use the models with CLI and Python:\n",
            "\n",
            "## CLI Example\n",
            "\n",
            "```bash\n",
            "yolo task=detect mode=train model=yolov8n.yaml data=coco128.yaml epochs=100\n",
            "```\n",
            "\n",
            "## Python Example\n",
            "\n",
            "```python\n",
            "from ultralytics import YOLO\n",
            "\n",
            "model = YOLO(\"model.yaml\")  # build a YOLOv8n model from scratch\n",
            "# YOLO(\"model.pt\")  use pre-trained model if available\n",
            "model.info()  # display model information\n",
            "model.train(data=\"coco128.yaml\", epochs=100)  # train the model\n",
            "```\n",
            "\n",
            "For more details on each model, their supported tasks, modes, and performance, please visit their respective documentation pages linked above.\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# Vision Transformers\n",
            "\n",
            "Vit models currently support Python environment:\n",
            "\n",
            "```python\n",
            "from ultralytics.vit import SAM\n",
            "\n",
            "# from ultralytics.vit import MODEL_TYPe\n",
            "\n",
            "model = SAM(\"sam_b.pt\")\n",
            "model.info()  # display model information\n",
            "model.predict(...)  # train the model\n",
            "```\n",
            "\n",
            "# Segment Anything\n",
            "\n",
            "## About\n",
            "\n",
            "## Supported Tasks\n",
            "\n",
            "| Model Type | Pre-trained Weights | Tasks Supported       |\n",
            "|------------|---------------------|-----------------------|\n",
            "| sam base   | `sam_b.pt`          | Instance Segmentation |\n",
            "| sam large  | `sam_l.pt`          | Instance Segmentation |\n",
            "\n",
            "## Supported Modes\n",
            "\n",
            "| Mode       | Supported          |\n",
            "|------------|--------------------|\n",
            "| Inference  | :heavy_check_mark: |\n",
            "| Validation | :x:                |\n",
            "| Training   | :x:                |\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# 🚧Page Under Construction ⚒\n",
            "\n",
            "This page is currently under construction!️👷Please check back later for updates. 😃🔜\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# YOLOv5u\n",
            "\n",
            "## About\n",
            "\n",
            "Anchor-free YOLOv5 models with improved accuracy-speed tradeoff.\n",
            "\n",
            "## Supported Tasks\n",
            "\n",
            "| Model Type | Pre-trained Weights                                                                                                         | Task      |\n",
            "|------------|-----------------------------------------------------------------------------------------------------------------------------|-----------|\n",
            "| YOLOv5u    | `yolov5nu`, `yolov5su`, `yolov5mu`, `yolov5lu`, `yolov5xu`, `yolov5n6u`, `yolov5s6u`, `yolov5m6u`, `yolov5l6u`, `yolov5x6u` | Detection |\n",
            "\n",
            "## Supported Modes\n",
            "\n",
            "| Mode       | Supported          |\n",
            "|------------|--------------------|\n",
            "| Inference  | :heavy_check_mark: |\n",
            "| Validation | :heavy_check_mark: |\n",
            "| Training   | :heavy_check_mark: |\n",
            "\n",
            "??? Performance\n",
            "\n",
            "    === \"Detection\"\n",
            "\n",
            "        | Model                                                                                    | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "        | ---------------------------------------------------------------------------------------- | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n",
            "        | [YOLOv5nu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5nu.pt)   | 640                   | 34.3                 | 73.6                           | 1.06                                | 2.6                | 7.7               |\n",
            "        | [YOLOv5su](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5su.pt)   | 640                   | 43.0                 | 120.7                          | 1.27                                | 9.1                | 24.0              |\n",
            "        | [YOLOv5mu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5mu.pt)   | 640                   | 49.0                 | 233.9                          | 1.86                                | 25.1               | 64.2              |\n",
            "        | [YOLOv5lu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5lu.pt)   | 640                   | 52.2                 | 408.4                          | 2.50                                | 53.2               | 135.0             |\n",
            "        | [YOLOv5xu](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5xu.pt)   | 640                   | 53.2                 | 763.2                          | 3.81                                | 97.2               | 246.4             |\n",
            "        |                                                                                          |                       |                      |                                |                                     |                    |                   |\n",
            "        | [YOLOv5n6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5n6u.pt) | 1280                  | 42.1                 | -                              | -                                   | 4.3                | 7.8               |\n",
            "        | [YOLOv5s6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5s6u.pt) | 1280                  | 48.6                 | -                              | -                                   | 15.3               | 24.6              |\n",
            "        | [YOLOv5m6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5m6u.pt) | 1280                  | 53.6                 | -                              | -                                   | 41.2               | 65.7              |\n",
            "        | [YOLOv5l6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5l6u.pt) | 1280                  | 55.7                 | -                              | -                                   | 86.1               | 137.4             |\n",
            "        | [YOLOv5x6u](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov5x6u.pt) | 1280                  | 56.8                 | -                              | -                                   | 155.4              | 250.7             |\n",
            "\n",
            "---\n",
            "comments: true\n",
            "---\n",
            "\n",
            "# YOLOv8\n",
            "\n",
            "## About\n",
            "\n",
            "## Supported Tasks\n",
            "\n",
            "| Model Type  | Pre-trained Weights                                                                                              | Task                  |\n",
            "|-------------|------------------------------------------------------------------------------------------------------------------|-----------------------|\n",
            "| YOLOv8      | `yolov8n.pt`, `yolov8s.pt`, `yolov8m.pt`, `yolov8l.pt`, `yolov8x.pt`                                             | Detection             |\n",
            "| YOLOv8-seg  | `yolov8n-seg.pt`, `yolov8s-seg.pt`, `yolov8m-seg.pt`, `yolov8l-seg.pt`, `yolov8x-seg.pt`                         | Instance Segmentation |\n",
            "| YOLOv8-pose | `yolov8n-pose.pt`, `yolov8s-pose.pt`, `yolov8m-pose.pt`, `yolov8l-pose.pt`, `yolov8x-pose.pt` ,`yolov8x-pose-p6` | Pose/Keypoints        |\n",
            "| YOLOv8-cls  | `yolov8n-cls.pt`, `yolov8s-cls.pt`, `yolov8m-cls.pt`, `yolov8l-cls.pt`, `yolov8x-cls.pt`                         | Classification        |\n",
            "\n",
            "## Supported Modes\n",
            "\n",
            "| Mode       | Supported          |\n",
            "|------------|--------------------|\n",
            "| Inference  | :heavy_check_mark: |\n",
            "| Validation | :heavy_check_mark: |\n",
            "| Training   | :heavy_check_mark: |\n",
            "\n",
            "??? Performance\n",
            "\n",
            "    === \"Detection\"\n",
            "\n",
            "        | Model                                                                                | size<br><sup>(pixels) | mAP<sup>val<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "        | ------------------------------------------------------------------------------------ | --------------------- | -------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n",
            "        | [YOLOv8n](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt) | 640                   | 37.3                 | 80.4                           | 0.99                                | 3.2                | 8.7               |\n",
            "        | [YOLOv8s](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt) | 640                   | 44.9                 | 128.4                          | 1.20                                | 11.2               | 28.6              |\n",
            "        | [YOLOv8m](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt) | 640                   | 50.2                 | 234.7                          | 1.83                                | 25.9               | 78.9              |\n",
            "        | [YOLOv8l](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt) | 640                   | 52.9                 | 375.2                          | 2.39                                | 43.7               | 165.2             |\n",
            "        | [YOLOv8x](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt) | 640                   | 53.9                 | 479.1                          | 3.53                                | 68.2               | 257.8             |\n",
            "\n",
            "    === \"Segmentation\"\n",
            "\n",
            "        | Model                                                                                        | size<br><sup>(pixels) | mAP<sup>box<br>50-95 | mAP<sup>mask<br>50-95 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "        | -------------------------------------------------------------------------------------------- | --------------------- | -------------------- | --------------------- | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n",
            "        | [YOLOv8n-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt) | 640                   | 36.7                 | 30.5                  | 96.1                           | 1.21                                | 3.4                | 12.6              |\n",
            "        | [YOLOv8s-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-seg.pt) | 640                   | 44.6                 | 36.8                  | 155.7                          | 1.47                                | 11.8               | 42.6              |\n",
            "        | [YOLOv8m-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-seg.pt) | 640                   | 49.9                 | 40.8                  | 317.0                          | 2.18                                | 27.3               | 110.2             |\n",
            "        | [YOLOv8l-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-seg.pt) | 640                   | 52.3                 | 42.6                  | 572.4                          | 2.79                                | 46.0               | 220.5             |\n",
            "        | [YOLOv8x-seg](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt) | 640                   | 53.4                 | 43.4                  | 712.1                          | 4.02                                | 71.8               | 344.1             |\n",
            "\n",
            "    === \"Classification\"\n",
            "\n",
            "        | Model                                                                                        | size<br><sup>(pixels) | acc<br><sup>top1 | acc<br><sup>top5 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) at 640 |\n",
            "        | -------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | ------------------------------ | ----------------------------------- | ------------------ | ------------------------ |\n",
            "        | [YOLOv8n-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-cls.pt) | 224                   | 66.6             | 87.0             | 12.9                           | 0.31                                | 2.7                | 4.3                      |\n",
            "        | [YOLOv8s-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-cls.pt) | 224                   | 72.3             | 91.1             | 23.4                           | 0.35                                | 6.4                | 13.5                     |\n",
            "        | [YOLOv8m-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-cls.pt) | 224                   | 76.4             | 93.2             | 85.4                           | 0.62                                | 17.0               | 42.7                     |\n",
            "        | [YOLOv8l-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-cls.pt) | 224                   | 78.0             | 94.1             | 163.0                          | 0.87                                | 37.5               | 99.7                     |\n",
            "        | [YOLOv8x-cls](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-cls.pt) | 224                   | 78.4             | 94.3             | 232.0                          | 1.01                                | 57.4               | 154.8                    |\n",
            "\n",
            "    === \"Pose\"\n",
            "\n",
            "        | Model                                                                                                | size<br><sup>(pixels) | mAP<sup>pose<br>50-95 | mAP<sup>pose<br>50 | Speed<br><sup>CPU ONNX<br>(ms) | Speed<br><sup>A100 TensorRT<br>(ms) | params<br><sup>(M) | FLOPs<br><sup>(B) |\n",
            "        | ---------------------------------------------------------------------------------------------------- | --------------------- | --------------------- | ------------------ | ------------------------------ | ----------------------------------- | ------------------ | ----------------- |\n",
            "        | [YOLOv8n-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt)       | 640                   | 50.4                  | 80.1               | 131.8                          | 1.18                                | 3.3                | 9.2               |\n",
            "        | [YOLOv8s-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt)       | 640                   | 60.0                  | 86.2               | 233.2                          | 1.42                                | 11.6               | 30.2              |\n",
            "        | [YOLOv8m-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-pose.pt)       | 640                   | 65.0                  | 88.8               | 456.3                          | 2.00                                | 26.4               | 81.0              |\n",
            "        | [YOLOv8l-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-pose.pt)       | 640                   | 67.6                  | 90.0               | 784.5                          | 2.59                                | 44.4               | 168.6             |\n",
            "        | [YOLOv8x-pose](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose.pt)       | 640                   | 69.2                  | 90.2               | 1607.1                         | 3.73                                | 69.4               | 263.2             |\n",
            "        | [YOLOv8x-pose-p6](https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose-p6.pt) | 1280                  | 71.6                  | 91.2               | 4088.7                         | 10.04                               | 99.1               | 1066.4            |\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings"
      ],
      "metadata": {
        "id": "UUYNdvdHLT47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOqjMXZSLvjf",
        "outputId": "fafbfc3f-ffbd-4323-ec56-35f0935a44d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ],
      "metadata": {
        "id": "lehQTgwWIwmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "YhkOef3hEGMo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "68819bb2cf3b4fd39fe09940fe75049c",
            "5321d9008d0643f191264361fc221732",
            "417525a8d49d478098292890b572bb42",
            "3b31c24f3a2c44638037d3335c52c55d",
            "931b5a6d6f864ac5a9390ff9984f5f6b",
            "a8920436632045979d5533d80dafc009",
            "e14f2615a73947439527eaf1c4bf3eb2",
            "4e4756e0938a4c5690df9b041b00f972",
            "088014ae5f204b57be402f04410b3f54",
            "1c2cf8dcae5e42dd9293e2a4a23c542e",
            "a3ba6c3d04b6427da8e1b1e8440d573a",
            "20fbcc4f779449cabf711858f49055e4",
            "d9b04025bfb64bf0b898fda83ef72db9",
            "ed3a34c862244d07a51ae6fb6e381035",
            "b80e3a82d6064b99811f68c68c979ca4",
            "e694d8db36fa4f3a882f19c5381f35ab",
            "4a6614264b0945f8973e028a5c15522a",
            "60a804f034d24fd0898d6b7eaa7154f0",
            "c36c786174fd4d4482abbbc1f5f6aef3",
            "c2638a8754e9468d898af955eda61e7e",
            "36cb7897e78449808203a7c064d031ec",
            "d131ff7eb321446c9071d0e9743cda80",
            "9af6b649646a4e1e87458f411b0d48cb",
            "94123671246a4579bceaee1e8656bc64",
            "849cdc09b236402bb37ddac54b713d5f",
            "7c2ba4e3e9b4453b9adeaade146f2fbf",
            "066b4ac6681e4b318e6004b81f3fe883",
            "ee4a6f90bf8e4f2fa7682b3083601429",
            "2b0e04ccc68a422e9e0c2b87cd05ed2d",
            "55b3365ed5fd40ed973e84ead7f54b75",
            "e902b54cc49f462db88d4a1327123014",
            "9a9b6cfec69d4de58e9290f6543448ce",
            "2ee55ced628a46619342a66d3df00d40",
            "008a533621ff4a0ba50e7c14a6231ee5",
            "0cf63a0ea8aa4b59815a56a1e98cf13f",
            "6b2b0c55f02e44bbbe5a100752d8767f",
            "cb2ed650c4bf4da5aa83bc9c1d98eaa4",
            "d28d6aa863e04dcdb7e6a17413e895a2",
            "55a2d15518c840f9bf64b66a0b2821a3",
            "8c37d831e92f4a3dba2c9ee479160c78",
            "faf504e3e75546a7a10bf2f4cb20eb82",
            "f05060afab83439b859ef54e53f4b541",
            "38997c1f17d54dc59c7b4439e2bb52c7",
            "2ab725bb471d452d9d0df49174b45090",
            "3152049236b14468a11bb7379b68f944",
            "7a0d38b387804d8e8727e7e72ed77717",
            "580d30538e724e06826d3532c8534855",
            "9bc79ca0a92c47298616876a610f2e1c",
            "e6247340bb8648c9a5205bf7da6e3a95",
            "89cdb56084984bb3a673d949f6d1e178",
            "16e0e1b261d5427492f8028203bea6a3",
            "b676a9e6ca3f41709a01f5643b6ac545",
            "5b9b5e51c73947d0ad03dadb37a48819",
            "b9cbd5ad1dff4d8e8cfc7e299c1b56cc",
            "7b02ec15c3d4438d8ec768a90489ff0d",
            "487a475f99a64da49cb7179a3dfbc2cb",
            "a638bcdabcfa4b24ab19c7deca6055ae",
            "bafe8939e7954915b369f2eea6e76d9a",
            "37f57707981e45f3a75009efb5f012c3",
            "2488cf63cd7642ff9323a93f0c69d8f0",
            "43c6ca333e754244868376777a6154f4",
            "42056904575a4482b696840e07c6c435",
            "52dd684ad8064860810021c7bb50b55c",
            "3b4f2b7fd3314d5fbd3a1744eb2eeb96",
            "fb3d758b5e804c8c98715ed65a28c175",
            "b7d875ecaeef46aebafea05ea94970db",
            "1080e7e815a24886ac2945eb372449ff",
            "2f5a48704c7a457387bd5f8a37641b33",
            "3730ac6e81e345f8854f300e7d0c1ea7",
            "eb7cff5271bc492bb896f4779ef7c058",
            "dd289b200a7948cba9daef3d266923a4",
            "d7f3887922ae46d0a514468ec4060381",
            "2fe9ffb0d1da410fb3dfd2bfe8f9e47d",
            "8f62893df22e4c19beab22a77ae51d14",
            "a858169961cc40f0bbb1e2c58e199037",
            "0a12a085d7814b7b90ecdbec2232e18c",
            "a562a5537d2248e09a10bcce39930f80",
            "967e017d869645b5affed4cc74460ceb",
            "354a64b17c2d4a7cb80cb2980e007479",
            "2ba4d6089ffa44cfb4f0bac3871a3628",
            "dc666829a6b04fc0a504eaa655ecad7c",
            "a61fe0a293354855b82e73e3b8bfe952",
            "32a87a7315f649ba8a6094c610bac013",
            "27ff1577e2e14c9d9302a4219d545ca4",
            "ab6306f366744a4e857c8cc55e828dd9",
            "aa39b6b2ba454cb4ad08be0b7ceac1b2",
            "48c0b544ab3546469d6e6e3f19994168",
            "2f7df748843241ce8dbada8a7c6a1570",
            "ed1e6cbbcd2b4ab598f3a3a1f77a7643",
            "3e7d51ec78cb40fbafecb58d3870ec71",
            "689175bc3c024d519a86c96ebc69bb11",
            "cd2fd71b7abd4086bfccbcf55a1076ea",
            "23144a86a08e4fe7aeaa91e2ed1bf175",
            "48b5289953c747db97e8ddeda89bd270",
            "9e3560df021145e0bd72d5ccbdedc775",
            "13cd1fd1e6374fac898a2419442d07ea",
            "257d2e5eb35e4afe83295fc5f8e27b5d",
            "914c90a8fe5849099219ded0ab7825a8",
            "e4e40910ddd14b748a7a976a235cd283",
            "e97350e624524cf98d367a4b03f6292f",
            "98974814a6df4b7f90736fcce563c664",
            "9be064946b8f4db1897b3c2f7258c72d",
            "68ce432feec34fc286c0971b817001c7",
            "e097aa9ed1794fc7ba87da18bf2f8720",
            "896d108f42a54ef49d2f8e78ec1c4976",
            "37484a3608dd49a9965db2f70eb0c6f5",
            "aa8c1e75c5904eedb533d83c639dead8",
            "4c34fb7a18274f878f08acbbad67e9b7",
            "1537d07935c3413197ca949e73871f14",
            "3b64b30838a94051890d1853b8da8ec7",
            "865b5bc1258343da870770a3e1f9289f",
            "d4714a012fa141edba79830bb023cb31",
            "4ac42c43abc5494b86da2c4d21806e37",
            "0d1f767b1eb8494fb30f9488858e1295",
            "dbc2cdecaffa4d56aa62024a1b17ef80",
            "74cf7e5c392d49b58d3b16a00a5bd1ba",
            "63de72d757be4631b580ae5a28d4c596",
            "d8eb176a023047239ae927a2b2027e4c",
            "3241ddf2f7154aafa42b3aca4503e98b",
            "162f7ccb140b4bccb4713838b04c2bd5",
            "1c4eef4b7faa4697838189678f4808f0",
            "abbe315e3e5c49c6b88abf2f54d8a5d4",
            "83ff2924d3c94b268215a7bf6e9f8f75",
            "ddb2010532d348d49c771419e1d7bec9",
            "b00ffe5c6a604e53ac80c71d8e98fe05",
            "aa25821fc38347c4959123c60cf64c51",
            "bd633254e20d4814afb75f49e33d97ad",
            "e671b2008db04ad3b4b45d43993d2931",
            "0a532f80b1a4426dba63da8aa80ce382",
            "b3e129f2c45140ae834b849cd00aadab",
            "173605a30c154e74afc46dfba9a8d2cc",
            "93522d7f1b5647b3a2d4d25e23718b4a",
            "ff4689a1837e4e179789779d02f98a6f",
            "eeabe8e3d3f64edf8d6e8ca62419c4a0",
            "5b68a00d2fd74a0b84c6141ba1a459cb",
            "75f99cdf6fb84389bf0f7daed5f4c09a",
            "1a3e93300d734111adc10e8dc1220404",
            "93fe4d78ec38472991713fadfffac79a",
            "5acea4729dda4bd1970e15486585277d",
            "348acd9bff494e46a34451a23c2ec301",
            "968cbff9100b4db58eb57f5719776aa4",
            "72a6d325967042c7925d858350d07b58",
            "4dffc4428f8f47589a9f6300072585d9",
            "6766f692045149a3b4522380339fb2ce",
            "fa6a680c2566475f92acfefef4021e96",
            "f98050587d7a490ca7d8b25791ba93c2",
            "36a16b719f084547b93a553a61b00b8f",
            "1b89d9022abb420996d0d3500b4c2dac",
            "55d2ea44547c4e8d99bf5ddd442c2fc6",
            "030933d49b4942b895ac7098fa00ab92",
            "f5f0834d95f44e9a9518d420a7ca2e1b",
            "a89fff1cc33d4483b5dd2a131c4e97d7",
            "3ed61366d2154340bce904d3a1a0c70d",
            "1933752b75474525ab6d8d692468bdd7"
          ]
        },
        "outputId": "82d46dea-a1a2-4e3a-9f17-867a393bb5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68819bb2cf3b4fd39fe09940fe75049c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20fbcc4f779449cabf711858f49055e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9af6b649646a4e1e87458f411b0d48cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)0bca8e1d/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "008a533621ff4a0ba50e7c14a6231ee5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3152049236b14468a11bb7379b68f944"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e1d/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "487a475f99a64da49cb7179a3dfbc2cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1080e7e815a24886ac2945eb372449ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "967e017d869645b5affed4cc74460ceb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed1e6cbbcd2b4ab598f3a3a1f77a7643"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)a8e1d/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e97350e624524cf98d367a4b03f6292f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "865b5bc1258343da870770a3e1f9289f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)8e1d/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abbe315e3e5c49c6b88abf2f54d8a5d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)b20bca8e1d/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff4689a1837e4e179789779d02f98a6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)bca8e1d/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6766f692045149a3b4522380339fb2ce"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text=\"Hello, lets get started with the embeddings\"\n",
        "# embedded_query=embeddings.embed_query(text)\n",
        "# print(len(embedded_query))\n",
        "# embedded_query"
      ],
      "metadata": {
        "id": "Ap7YXeMjLt0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunks first, then embeddings\n"
      ],
      "metadata": {
        "id": "tI9Ibfw7MQnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Recursive Character splitter, works much better than standard char splitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 2000, # this defines the size of the chunk\n",
        "    chunk_overlap  = 0, # this defines the overlap between chunks, this can lead to higher amounts of documents, but gives a better context\n",
        ")"
      ],
      "metadata": {
        "id": "ijuWKsBNMHs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = text_splitter.split_documents(data)\n",
        "print(texts[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEXrWo7vNDCD",
        "outputId": "a3eda90b-a4ad-4efc-e218-2c5b3be38321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## Contributing to YOLOv8 🚀\n",
            "\n",
            "We love your input! We want to make contributing to YOLOv8 as easy and transparent as possible, whether it's:\n",
            "\n",
            "- Reporting a bug\n",
            "- Discussing the current state of the code\n",
            "- Submitting a fix\n",
            "- Proposing a new feature\n",
            "- Becoming a maintainer\n",
            "\n",
            "YOLOv8 works so well due to our combined community effort, and for every small improvement you contribute you will be\n",
            "helping push the frontiers of what's possible in AI 😃!\n",
            "\n",
            "## Submitting a Pull Request (PR) 🛠️\n",
            "\n",
            "Submitting a PR is easy! This example shows how to submit a PR for updating `requirements.txt` in 4 steps:\n",
            "\n",
            "### 1. Select File to Update\n",
            "\n",
            "Select `requirements.txt` to update by clicking on it in GitHub.\n",
            "\n",
            "<p align=\"center\"><img width=\"800\" alt=\"PR_step1\" src=\"https://user-images.githubusercontent.com/26833433/122260847-08be2600-ced4-11eb-828b-8287ace4136c.png\"></p>\n",
            "\n",
            "### 2. Click 'Edit this file'\n",
            "\n",
            "Button is in top-right corner.\n",
            "\n",
            "<p align=\"center\"><img width=\"800\" alt=\"PR_step2\" src=\"https://user-images.githubusercontent.com/26833433/122260844-06f46280-ced4-11eb-9eec-b8a24be519ca.png\"></p>\n",
            "\n",
            "### 3. Make Changes\n",
            "\n",
            "Change `matplotlib` version from `3.2.2` to `3.3`.\n",
            "\n",
            "<p align=\"center\"><img width=\"800\" alt=\"PR_step3\" src=\"https://user-images.githubusercontent.com/26833433/122260853-0a87e980-ced4-11eb-9fd2-3650fb6e0842.png\"></p>\n",
            "\n",
            "### 4. Preview Changes and Submit PR\n",
            "\n",
            "Click on the **Preview changes** tab to verify your updates. At the bottom of the screen select 'Create a **new branch**\n",
            "for this commit', assign your branch a descriptive name such as `fix/matplotlib_version` and click the green **Propose\n",
            "changes** button. All done, your PR is now submitted to YOLOv8 for review and approval 😃!\n",
            "\n",
            "<p align=\"center\"><img width=\"800\" alt=\"PR_step4\" src=\"https://user-images.githubusercontent.com/26833433/122260856-0b208000-ced4-11eb-8e8e-77b6151cbcc3.png\"></p>\n",
            "\n",
            "### PR recommendations\n",
            "\n",
            "To allow your work to be integrated as seamlessly as possible, we advise you to:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-rH5eB8Nj0l",
        "outputId": "d09ad265-d94c-46b1-fd78-5daca5fd763d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1952"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33d6pCAeNVJ7",
        "outputId": "49a96270-7f0e-4e12-8679-7b1964631575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- ✅ Verify your PR is **up-to-date** with `ultralytics/ultralytics` `main` branch. If your PR is behind you can update\n",
            "  your code by clicking the 'Update branch' button or by running `git pull` and `git merge main` locally.\n",
            "\n",
            "<p align=\"center\"><img width=\"751\" alt=\"Screenshot 2022-08-29 at 22 47 15\" src=\"https://user-images.githubusercontent.com/26833433/187295893-50ed9f44-b2c9-4138-a614-de69bd1753d7.png\"></p>\n",
            "\n",
            "- ✅ Verify all YOLOv8 Continuous Integration (CI) **checks are passing**.\n",
            "\n",
            "<p align=\"center\"><img width=\"751\" alt=\"Screenshot 2022-08-29 at 22 47 03\" src=\"https://user-images.githubusercontent.com/26833433/187296922-545c5498-f64a-4d8c-8300-5fa764360da6.png\"></p>\n",
            "\n",
            "- ✅ Reduce changes to the absolute **minimum** required for your bug fix or feature addition. _\"It is not daily increase\n",
            "  but daily decrease, hack away the unessential. The closer to the source, the less wastage there is.\"_  — Bruce Lee\n",
            "\n",
            "### Docstrings\n",
            "\n",
            "Not all functions or classes require docstrings but when they do, we\n",
            "follow [google-style docstrings format](https://google.github.io/styleguide/pyguide.html#38-comments-and-docstrings).\n",
            "Here is an example:\n",
            "\n",
            "```python\n",
            "\"\"\"\n",
            "   What the function does. Performs NMS on given detection predictions.\n",
            "\n",
            "    Args:\n",
            "        arg1: The description of the 1st argument\n",
            "        arg2: The description of the 2nd argument\n",
            "\n",
            "    Returns:\n",
            "        What the function returns. Empty if nothing is returned.\n",
            "\n",
            "    Raises:\n",
            "        Exception Class: When and why this exception can be raised by the function.\n",
            "\"\"\"\n",
            "```\n",
            "\n",
            "## Submitting a Bug Report 🐛\n",
            "\n",
            "If you spot a problem with YOLOv8 please submit a Bug Report!\n",
            "\n",
            "For us to start investigating a possible problem we need to be able to reproduce it ourselves first. We've created a few\n",
            "short guidelines below to help users provide what we need in order to get started.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[1].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZW81q_rNoMB",
        "outputId": "ba2163c2-308c-4a0b-ad9d-39332f1eb6d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Create a YOLO model instance\n",
            "model = YOLO(f'yolov8n.pt')\n",
            "\n",
            "# Add the custom callback to the model\n",
            "model.add_callback(\"on_predict_batch_end\", on_predict_batch_end)\n",
            "\n",
            "# Iterate through the results and frames\n",
            "for (result, frame) in model.track/predict():\n",
            "    pass\n",
            "```\n",
            "\n",
            "## All callbacks\n",
            "\n",
            "Here are all supported callbacks. See callbacks [source code](https://github.com/ultralytics/ultralytics/blob/main/ultralytics/yolo/utils/callbacks/base.py) for additional details.\n",
            "\n",
            "\n",
            "### Trainer Callbacks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## as it can be seen from the above example,\n",
        "## the last line is duplicated in the two documents and this helps in having more context\n",
        "## THIS is especially useful when the context switches up quite regulary\n",
        "\n",
        "# one more important thing is using a database\n",
        "## there are multiple variants, from FAISS, Chroma and also Pinecone\n"
      ],
      "metadata": {
        "id": "foz6N5IFNyQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu\n",
        "!pip install -q chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0yiI1dQQFWz",
        "outputId": "54d8d925-e895-48e4-c164-900e2228f432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m921.9/921.9 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.1/414.1 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## make sure to ensure that you are using GPU, to save significant time\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.vectorstores import FAISS\n",
        "db=Chroma.from_documents(texts,embeddings)\n",
        "db2=FAISS.from_documents(texts,embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kd1K_ElOT2h",
        "outputId": "cac4626a-fd4f-4388-a0a6-186d6d8e1cc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB without persistence: data will be transient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## use similrity search to retrieve related documents based on query\n",
        "embedding_vector = embeddings.embed_query(\"How do i predict and plot them\")\n",
        "docs=db.similarity_search_by_vector(embedding_vector,k=10)"
      ],
      "metadata": {
        "id": "4Xk5nkbpbyJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs2=db2.similarity_search(\"How do i predict and plot them\",k=10)"
      ],
      "metadata": {
        "id": "g2_VUd9QWVZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfe_ESJmRIsu",
        "outputId": "902206b2-cbce-431a-f94f-0170940b817b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-V1-szyWdM6",
        "outputId": "6bb9fd9e-2377-428d-8fcb-5af25a891dfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x226FscmRMP_",
        "outputId": "dd7bb21d-d8ee-4633-acae-3f29a92d8e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`probs` attribute of `Results` class is a `Tensor` containing class probabilities of a classification operation.\n",
            "\n",
            "!!! example \"Probs\"\n",
            "\n",
            "    ```python\n",
            "    results = model(inputs)\n",
            "    results[0].probs  # cls prob, (num_class, )\n",
            "    ```\n",
            "\n",
            "Class reference documentation for `Results` module and its components can be found [here](../reference/yolo/engine/results.md)\n",
            "\n",
            "## Plotting results\n",
            "\n",
            "You can use `plot()` function of `Result` object to plot results on in image object. It plots all components(boxes,\n",
            "masks, classification logits, etc.) found in the results object\n",
            "\n",
            "!!! example \"Plotting\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs2[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZQz58vMRWG6",
        "outputId": "81e0418d-279d-4c83-f0b0-a9836f3825ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Colors\n",
            "---\n",
            ":::ultralytics.yolo.utils.plotting.Colors\n",
            "<br><br>\n",
            "\n",
            "# Annotator\n",
            "---\n",
            ":::ultralytics.yolo.utils.plotting.Annotator\n",
            "<br><br>\n",
            "\n",
            "# plot_labels\n",
            "---\n",
            ":::ultralytics.yolo.utils.plotting.plot_labels\n",
            "<br><br>\n",
            "\n",
            "# save_one_box\n",
            "---\n",
            ":::ultralytics.yolo.utils.plotting.save_one_box\n",
            "<br><br>\n",
            "\n",
            "# plot_images\n",
            "---\n",
            ":::ultralytics.yolo.utils.plotting.plot_images\n",
            "<br><br>\n",
            "\n",
            "# plot_results\n",
            "---\n",
            ":::ultralytics.yolo.utils.plotting.plot_results\n",
            "<br><br>\n",
            "\n",
            "# output_to_target\n",
            "---\n",
            ":::ultralytics.yolo.utils.plotting.output_to_target\n",
            "<br><br>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxNGLQphWhI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0x_zfAeRPmt",
        "outputId": "aa909cca-6381-4ebf-f2b4-2a0183e332e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`probs` attribute of `Results` class is a `Tensor` containing class probabilities of a classification operation.\n",
            "\n",
            "!!! example \"Probs\"\n",
            "\n",
            "    ```python\n",
            "    results = model(inputs)\n",
            "    results[0].probs  # cls prob, (num_class, )\n",
            "    ```\n",
            "\n",
            "Class reference documentation for `Results` module and its components can be found [here](../reference/yolo/engine/results.md)\n",
            "\n",
            "## Plotting results\n",
            "\n",
            "You can use `plot()` function of `Result` object to plot results on in image object. It plots all components(boxes,\n",
            "masks, classification logits, etc.) found in the results object\n",
            "\n",
            "!!! example \"Plotting\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(docs[1].metadata['file_path'])"
      ],
      "metadata": {
        "id": "8UipT-OgRTYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_references(docs):\n",
        "  for doc in docs:\n",
        "      print(doc.metadata['file_path'])"
      ],
      "metadata": {
        "id": "MBFfY7z9RcmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_all_references(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W3vIVYlRit2",
        "outputId": "2f277977-48cd-4876-a01a-5f5b580b8081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docs/modes/predict.md\n",
            "docs/usage/cfg.md\n",
            "docs/inference_api.md\n",
            "docs/usage/python.md\n",
            "docs/yolov5/tutorials/test_time_augmentation.md\n",
            "docs/modes/train.md\n",
            "docs/usage/cfg.md\n",
            "docs/yolov5/tutorials/model_ensembling.md\n",
            "docs/modes/predict.md\n",
            "README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_all_references(docs2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qM7uOstWnjp",
        "outputId": "9010dda5-b9ea-4165-c79a-a0b4edbaf30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docs/reference/yolo/utils/plotting.md\n",
            "docs/modes/predict.md\n",
            "docs/usage/cfg.md\n",
            "docs/inference_api.md\n",
            "docs/reference/yolo/v8/classify/predict.md\n",
            "docs/reference/yolo/utils/callbacks/comet.md\n",
            "docs/usage/python.md\n",
            "docs/yolov5/tutorials/test_time_augmentation.md\n",
            "docs/modes/train.md\n",
            "docs/usage/cfg.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHhXczurWiBO",
        "outputId": "eda378ae-36cf-41ff-85cf-980771f1dd2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/71.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"openai api key\""
      ],
      "metadata": {
        "id": "B7B7ZAm1V-vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "FMtnSKSXWpc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-52aNwcB-3wH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
      ],
      "metadata": {
        "id": "idh7IB-DwL-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(model_name=\"gpt-3.5-turbo\"), retriever=db.as_retriever())"
      ],
      "metadata": {
        "id": "yZS48OozXZ9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JbJ26nM1zbUj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"how do i predict and plot\"\n",
        "result = qa({\"question\": query})"
      ],
      "metadata": {
        "id": "gzUXTlJ6XbkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0-9SniKXg4-",
        "outputId": "f40738c4-66c9-4383-e390-734adacdd23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To predict and plot with YOLO, you can use the `predict()` method of the `YOLO` class to get the results, and then use the `plot()` method of the `Results` class to plot the predictions on the image. Here's an example:\n",
            "\n",
            "```python\n",
            "from yolov5 import YOLO\n",
            "from PIL import Image\n",
            "\n",
            "# Load model\n",
            "model = YOLO(\"model.pt\")\n",
            "\n",
            "# Load image\n",
            "image = Image.open(\"image.jpg\")\n",
            "\n",
            "# Predict on image\n",
            "results = model.predict(source=image)\n",
            "\n",
            "# Plot predictions on image\n",
            "results.plot()\n",
            "```\n",
            "\n",
            "This will display the image with the predicted objects overlaid on top of it. You can also save the plotted image by specifying a filename in the `plot()` method:\n",
            "\n",
            "```python\n",
            "results.plot(filename=\"image_pred.jpg\")\n",
            "```\n",
            "\n",
            "This will save the plotted image as `image_pred.jpg`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history=[]\n",
        "query = \"write a small sample application to predict and plot on an image\"\n",
        "result = qa({\"question\": query,\"chat_history\": chat_history})"
      ],
      "metadata": {
        "id": "T7rG8xM7cM7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tbcqd4Kuchtz",
        "outputId": "a77faeb2-dbf6-47ec-ac2a-4b7663aae983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a sample application that loads an image, runs it through YOLOv8 for object detection, and then plots the bounding boxes on the original image:\n",
            "\n",
            "```python\n",
            "import cv2\n",
            "from yolov8 import YOLO\n",
            "\n",
            "# Load the image\n",
            "img = cv2.imread(\"path/to/image.jpg\")\n",
            "\n",
            "# Initialize the model\n",
            "model = YOLO(\"path/to/model.pt\")\n",
            "\n",
            "# Run inference on the image\n",
            "results = model.predict(img)\n",
            "\n",
            "# Plot the bounding boxes on the original image\n",
            "res_plotted = results[0].plot(img=img)\n",
            "\n",
            "# Show the result\n",
            "cv2.imshow(\"result\", res_plotted)\n",
            "cv2.waitKey()\n",
            "```\n",
            "\n",
            "This script assumes that you have already installed the necessary packages (opencv-python and yolov8) and that you have a trained YOLOv8 model (specified by `\"path/to/model.pt\"`) that is compatible with the input image.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "query = \"how to import yolov8\"\n",
        "result = qa({\"question\": query})"
      ],
      "metadata": {
        "id": "1GZvk8Wfcmpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlZhKvUjkQeR",
        "outputId": "3280264e-3e6f-4e94-c195-8a46b09b3d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You can import YOLOv8 in Python using the `ultralytics` library. Here is an example of how to import and use YOLOv8 in Python:\n",
            "\n",
            "```python\n",
            "from ultralytics import YOLO\n",
            "\n",
            "# Load a YOLOv8 model from a pre-trained weights file\n",
            "model = YOLO('yolov8n.pt')\n",
            "\n",
            "# Display model information\n",
            "model.info()\n",
            "\n",
            "# Make predictions on an image\n",
            "results = model('image.jpg')\n",
            "\n",
            "# Display the results\n",
            "results.show()\n",
            "```\n",
            "\n",
            "This code loads a YOLOv8 model from a pre-trained weights file, displays the model information, makes predictions on an image, and displays the results. You can also use YOLOv8 to train a custom model and export it to different formats like ONNX and TensorRT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dAdnCnS2lifA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-4QpdpAllJn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}